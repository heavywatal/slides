<!DOCTYPE html>
<html>
<head prefix="og: http://ogp.me/ns#">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
<title>7. Statistical modeling 1: probability distribution, likelihood â€” Hands-on Introduction to R 2023</title>
<meta name="author" content="Watal M. Iwasaki">
<meta property="og:title" content="7. Statistical modeling 1: probability distribution, likelihood â€” Hands-on Introduction to R 2023">
<meta property="og:type" content="article">
<meta property="og:url" content="https://heavywatal.github.io/slides/english2023r/7-distribution.html">
<meta property="og:image" content="https://avatars.githubusercontent.com/heavywatal">
<meta property="og:description" content="">
<meta property="og:site_name" content="Slide decks â€” Heavy Watal">
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@heavywatal">
<meta name="twitter:creator" content="@heavywatal">
<link rel="stylesheet" href="/lib/katex/katex.min.css">
<script defer src="/lib/katex/katex.min.js"></script>
<script defer src="/lib/katex/contrib/auto-render.min.js"></script>
<script>
document.addEventListener("DOMContentLoaded", function() {
  renderMathInElement(document.body, {
    delimiters: [
      {left: "\\[", right: "\\]", display: true},
      {left: "$", right: "$", display: false}
    ]
  });
});
</script>
<style>
.katex {
  font-size: 1.12em;
}

.katex-display > .katex {
  text-align: left;
  padding-left: 2rem;
}
</style>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-V60H2JH0G6"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-V60H2JH0G6', { 'anonymize_ip': false });
}
</script>
<link rel="stylesheet" href="/slides/lib/reveal.js/reveal.css">
<script defer src="/slides/lib/reveal.js/reveal.js"></script>
<script defer src="/slides/lib/reveal.js/plugin/notes/notes.js"></script>
<script defer src="/slides/lib/reveal.js/plugin/search/search.js"></script>
<script defer src="/slides/lib/reveal-initialize.js"></script>
<script>
window.addEventListener('DOMContentLoaded', function() {
  Reveal.configure({width: 1440, height: 1080});
  document.querySelector('html').style.fontSize = '240%';
})
</script>
<script defer src="/slides/lib/reload-img-onclick.js"></script>
<link rel="stylesheet" href="/slides/css/style-reveal.css">
<link rel="stylesheet" href="style.css">
</head>
<body>
<div class="reveal">
<div class="slides">
<section>
<h1 id="hands-on-introduction-to-r-2023"><a href=".">Hands-on Introduction to R 2023</a></h1>
<div class="author">
å²©åµœ èˆª (Watal M. Iwasaki, PhD)
</div>
<div class="affiliation">
Graduate School of Life Sciences, Tohoku University
</div>
<ol>
<li><a href="1-introduction.html">Introduction: what is data analysis and R basics</a>
<li><a href="2-visualization.html">Data visualization and reporting</a>
<li><a href="3-structure1.html">Data transformation 1: extract, summarize</a>
<li><a href="4-structure2.html">Data transformation 2: join, pivot</a>
<li><a href="5-content.html">Data cleansing and conversion: numbers, text</a>
<li><a href="6-input.html">Data input and interpretation</a>
<li class="current-deck"><a href="7-distribution.html">Statistical modeling 1: probability distribution, likelihood</a>
<li><a href="8-glm.html">Statistical modeling 2: linear regression</a>
</ol>
<div class="footnote">
2023-12-05 Tohoku University<br>
<a href="https://heavywatal.github.io/slides/english2023r/">https://heavywatal.github.io/slides/english2023r/</a>
</div>

</section>
<section>
<h2 id="purposes-of-this-hands-on-lectures">Purposes of this hands-on lectures</h2>
<h3 id="-delevery-biological-research-involves-data-and-modelsdel">âœ… <del>Every biological research involves data and models</del></h3>
<h3 id="-delyou-want-to-do-reproducible-analysisdel">âœ… <del>You want to do reproducible analysis</del></h3>
<h3 id="-dellearn-how-to-do-it-and-how-to-learn-moredel">âœ… <del>Learn how to do it and how to learn more</del></h3>
<h3 id="-glance-at-the-basics-of-data-analysis">â¬œ Glance at the basics of data analysis</h3>
<hr>
<p>You don&rsquo;t have to remember every command.<br>
Just repeat forgetting and searching.
</section>
<section>
<h2 id="what-do-you-want-to-do-with-data">What do you want to do with data?</h2>
<ul>
<li>to <strong>understand</strong> phenomena</li>
<li>to <strong>predict</strong> future</li>
<li>to <strong>classify</strong> objects</li>
<li>to <strong>control</strong> behavior</li>
<li>to <strong>generate</strong> something new</li>
</ul>
<p>Is analysis necessary for that?<br>
Why not just raw data?
</section>
<section>
<h2 id="look-back-day-1">Look back day 1</h2>
<iframe width="600" height="450" src="./1-introduction.html#/4"></iframe>
<iframe width="600" height="450" src="./1-introduction.html#/5"></iframe>
<iframe width="600" height="450" src="./1-introduction.html#/6"></iframe>
<iframe width="600" height="450" src="./1-introduction.html#/7"></iframe>
</section>
<section>
<h2 id="mathematical-models-in-data-science">Mathematical models in data science</h2>
<p>Mathematical expression of assumptions to simulate data generation<br>
e.g., the larger the more expensive: $\text{price} = A \times \text{carat} + B + \epsilon$</p>
<p><img src="./figure/lm-diamonds-1.png" alt="plot of chunk lm-diamonds"></p>
<dl>
<dt>Regression</dt>
<dd>express y as a function of x.</dd>
</dl>

</section>
<section>
<h2 id="extending-linear-regression">Extending linear regression</h2>
<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="280" alt="ãƒ‡ãƒ¼ã‚¿è§£æã®ãŸã‚ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€ ä¹…ä¿æ‹“å¼¥ 2012">
</a>
</figure>
<p><strong>Linear Model (LM)</strong> ğŸ‘ˆ #7 today</p>
<p><span style="color: #888888;">Â  Â  â†“ probability distribution</span></p>
<p><strong>Generalized Linear Model (GLM)</strong> <a href="8-glm.html">&mdash; #8 next time</a></p>
<p><span style="color: #888888;">Â  Â  â†“ individual difference, random effect</span></p>
<p><strong>Generalized Linear Mixed Model (GLMM)</strong></p>
<p><span style="color: #888888;">Â  Â  â†“ flexible modelling</span></p>
<p><strong>Hierarchical Bayesian Model (HBM)</strong></p>
<p><cite><a href="https://amzn.to/33suMIZ">ãƒ‡ãƒ¼ã‚¿è§£æã®ãŸã‚ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€</a> ä¹…ä¿æ‹“å¼¥ 2012 ã‚ˆã‚Šæ”¹å¤‰</cite></p>

</section>
<section>
<h2 id="two-parts-to-a-regression-model">Two parts to a regression model</h2>
<ol>
<li>
<p>Define a <strong>family of models</strong>: express generic pattern</p>
<ul>
<li>straight line: $y = a_1 + a_2 x$</li>
<li>log curve: $\log(y) = a_1 + a_2 x$</li>
<li>quadratic curve: $y = a_1 + a_2 x^2$</li>
</ul>
</li>
<li>
<p>Generate a <strong>fitted model</strong>: adjust parameters to get closer to the data</p>
<ul>
<li>$y = 3x + 7$</li>
<li>$y = 9x^2$</li>
</ul>
</li>
</ol>
<cite>
<a href="https://r4ds.had.co.nz/model-basics.html" class="url">https://r4ds.had.co.nz/model-basics.html</a>
</cite>
</section>
<section>
<h2 id="can-see-a-strong-pattern-the-taller-the-heavier">Can see a strong pattern: the taller the heavier</h2>
<p>The relationship looks linear, $y = a x + b$.<br>
Â </p>
<p><img src="./figure/weight-height-1.png" alt="plot of chunk weight-height"></p>

</section>
<section>
<h2 id="can-see-a-strong-pattern-the-taller-the-heavier">Can see a strong pattern: the taller the heavier</h2>
<p>The relationship looks linear, $y = a x + b$.<br>
OK, let&rsquo;s try random slope <em>a</em> and intersect <em>b</em>:</p>
<p><img src="./figure/weight-lines-1.png" alt="plot of chunk weight-lines"></p>
<p>Need to find a good slope and intersect.</p>

</section>
<section>
<h2 id="ordinary-least-square-ols">Ordinary Least Square (OLS)</h2>
<p>minimizes the <strong style="color: #E69F00">residual</strong> sum of squares (RSS)
from <span style="color: #3366ff">the regression line</span>.</p>
<p><img src="./figure/weight-residual-1.png" alt="plot of chunk weight-residual"></p>

</section>
<section>
<h2 id="searching-for-models-to-minimize-rss">Searching for models to minimize RSS</h2>
<p>Try random values, and pick the best ones.<br>
May need to generate much more to find good one.</p>
<p><img src="./figure/weight-goodlines-1.png" alt="plot of chunk weight-goodlines"></p>

</section>
<section>
<h2 id="searching-for-models-to-minimize-rss">Searching for models to minimize RSS</h2>
<p><strong>Grid search</strong>: generate an evenly spaced grid of points.<br>
Slightly more efficient than random search?</p>
<p><img src="./figure/weight-grid-1.png" alt="plot of chunk weight-grid"></p>
<p>There are many other <strong>optimization</strong> techniques although not covered here.</p>

</section>
<section>
<h2 id="r-can-find-the-optimum-in-an-instant">R can find the optimum in an instant</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">par_init</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="n">intercept</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">slope</span> <span class="o">=</span> <span class="m">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span> <span class="o">=</span> <span class="nf">optim</span><span class="p">(</span><span class="n">par_init</span><span class="p">,</span> <span class="n">fn</span> <span class="o">=</span> <span class="n">rss_weight</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">df_weight</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span><span class="o">$</span><span class="n">par</span>
</span></span></code></pre></div><pre tabindex="0"><code>intercept     slope 
-69.68394  78.53490 
</code></pre><p><img src="./figure/weight-lm-1.png" alt="plot of chunk weight-lm"></p>
<p>The code above is for general optimization.<br>
For simple linear regression, an easier way is as follows&hellip;</p>

</section>
<section>
<h2 id="lm-function-to-fit-linear-models"><code>lm()</code> function to fit linear models</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">fit</span> <span class="o">=</span> <span class="nf">lm</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">mpg</span><span class="p">,</span> <span class="n">formula</span> <span class="o">=</span> <span class="n">hwy</span> <span class="o">~</span> <span class="n">displ</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">broom</span><span class="o">::</span><span class="nf">tidy</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span>
</span></span></code></pre></div><pre tabindex="0"><code>         term  estimate std.error statistic       p.value
1 (Intercept) 35.697651 0.7203676  49.55477 2.123519e-125
2       displ -3.530589 0.1945137 -18.15085  2.038974e-46
</code></pre><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">mpg_added</span> <span class="o">=</span> <span class="n">modelr</span><span class="o">::</span><span class="nf">add_predictions</span><span class="p">(</span><span class="n">mpg</span><span class="p">,</span> <span class="n">fit</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&#34;response&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">ggplot</span><span class="p">(</span><span class="n">mpg_added</span><span class="p">)</span> <span class="o">+</span> <span class="nf">aes</span><span class="p">(</span><span class="n">displ</span><span class="p">,</span> <span class="n">hwy</span><span class="p">)</span> <span class="o">+</span> <span class="nf">geom_point</span><span class="p">()</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">geom_line</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">y</span> <span class="o">=</span> <span class="n">pred</span><span class="p">),</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">&#34;#3366ff&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p><img src="./figure/lm-mpg-1.png" alt="plot of chunk lm-mpg"></p>
<p>ğŸ”° Try <code>lm()</code> with <code>diamonds</code> and <code>iris</code>.</p>

</section>
<section>
<h2 id="straight-lm-does-not-fit-all">Straight LM does not fit all</h2>
<p><img src="./figure/lm-bad-1.png" alt="plot of chunk lm-bad"></p>
<ul>
<li>Prediction goes below zero whereas all the observations are <strong>positive</strong>.</li>
<li>Y values are <strong>integer</strong>. Their <strong>dispersion</strong> is larger when X is larger.</li>
</ul>

</section>
<section>
<h2 id="straight-lm-does-not-fit-all">Straight LM does not fit all</h2>
<p><img src="./figure/glm-better-1.png" alt="plot of chunk glm-better"></p>
<ul>
<li>Prediction goes below zero whereas all the observations are <strong>positive</strong>.</li>
<li>Y values are <strong>integer</strong>. Their <strong>dispersion</strong> is larger when X is larger.</li>
<li>Let&rsquo;s learn statistical modelling for better fitting to the data.</li>
</ul>

</section>
<section>
<h2 id="extending-linear-regression">Extending linear regression</h2>
<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="280" alt="ãƒ‡ãƒ¼ã‚¿è§£æã®ãŸã‚ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€ ä¹…ä¿æ‹“å¼¥ 2012">
</a>
</figure>
<p><strong>Linear Model (LM)</strong> ğŸ‘ˆ #7 today</p>
<p><span style="color: #888888;">Â  Â  â†“ <span class="fragment highlight-blue custom bold">probability distribution</span></span></p>
<p><strong>Generalized Linear Model (GLM)</strong> <a href="8-glm.html">&mdash; #8 next time</a></p>
<p><span style="color: #888888;">Â  Â  â†“ individual difference, random effect</span></p>
<p><strong>Generalized Linear Mixed Model (GLMM)</strong></p>
<p><span style="color: #888888;">Â  Â  â†“ flexible modelling</span></p>
<p><strong>Hierarchical Bayesian Model (HBM)</strong></p>
<p><cite><a href="https://amzn.to/33suMIZ">ãƒ‡ãƒ¼ã‚¿è§£æã®ãŸã‚ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€</a> ä¹…ä¿æ‹“å¼¥ 2012 ã‚ˆã‚Šæ”¹å¤‰</cite></p>

</section>
<section>
<h2 id="probability-distribution">Probability distribution</h2>
<p>The relationship between phenomena and their frequencies.</p>
<dl>
<dt>empirical distribution</dt>
<dd>created by collecting samples.<br></dd>
<dd>e.g., rolling a dice 12 times, heights of 1000 students:</dd>
</dl>
<p><img src="./figure/distribution-1.png" alt="plot of chunk distribution"></p>
<dl>
<dt>theoretical distribution</dt>
<dd>described with math equation and a few parameters.</dd>
</dl>

</section>
<section>
<h2 id="random-variable-x-follows-probability-distribution-f">Random variable $X$ follows probability distribution $f$</h2>
<p>$X \sim f(\theta)$</p>
<p>e.g.,<br>
The number of heads in tossing 3 fair coins $X$ <strong>follows binomial distribution</strong>.<br>
$X \sim \text{Binomial}(n = 3, p = 0.5)$</p>
<div class="column-container">
  <div class="column" style="flex-shrink: 2.0;">
<p><img src="./figure/dbinom-1.png" alt="plot of chunk dbinom"></p>
  </div>
  <div class="column" style="padding-top: 10px;">
\[\begin{split}
\text{Prob}(X = k) &= \binom n k p^k (1 - p)^{n - k} \\
k &\in \{0, 1, 2, \ldots, n\}
\end{split}\]
  </div>
</div>
<p>Let&rsquo;s experiment.</p>

</section>
<section>
<h2 id="record-repeated-trials">Record repeated trials</h2>
<p>The number of heads observed in tossing 3 fair coins: $X$</p>
<p>trial 1: <strong>H</strong> T <strong>H</strong> â†’ $X = 2$<br>
trial 2: T T T â†’ $X = 0$<br>
trial 3: <strong>H</strong> T T â†’ $X = 1$, subsequently, $2, 1, 3, 0, 2, \ldots$</p>
<p><img src="./figure/rbinom-1.png" alt="plot of chunk rbinom"></p>
<div style="text-align: right;">
It approaches asymptotically to <b>binomial distribution</b>.<br>
0 and 3 is rare. 1 and 2 are three times more likely.
</div>
</section>
<section>
<h2 id="you-can-generate-similar-values-without-tossing-coins">You can generate similar values without tossing coins</h2>
<ul>
<li>The number of heads $X$ observed in tossing 3 fair coins.</li>
<li>Random samples $X$ from the binomial distribution with $n = 3, p = 0.5$.</li>
</ul>
<div class="column-container">
  <div class="column" style="flex-shrink: 2.0;">
<img src="figure/dbinom-1.png" alt="plot of chunk dbinom">
  </div>
  <div class="column" style="padding-top: 10px;">
$X \sim \text{Binomial}(n = 3, p = 0.5)$
<p>Â Â  â†“ sample</p>
<p>{2, 0, 1, 2, 1, 3, 0, 2, &hellip;}</p>
  </div>
</div>
<p>These are so similar that we can say<br>
&ldquo;The number of heads in <em>n</em> tosses follows binary distribution.&rdquo;</p>
<p>Conversely, we can understand it like<br>
&ldquo;Random variable of binomial distribution is the number of successes in <em>n</em> trials.&rdquo;</p>

</section>
<section>
<h2 id="a-kind-of-statistical-modelling">A kind of statistical modelling</h2>
<p>Tossing 3 fair coins repeatedly {2, 0, 1, 2, 1, 3, 0, 2, &hellip;}</p>
<p>Â Â  â†“ describe with only two parameters. information reduction.</p>
<p>Can reproduce with binary distribution with $n = 3, p = 0.5$</p>
<figure>
<img src="../tokiomarine2021/math-model.drawio.svg" width="900"><br>
<figcaption><cite>ã€Œãƒ‡ãƒ¼ã‚¿åˆ†æã®ãŸã‚ã®æ•°ç†ãƒ¢ãƒ‡ãƒ«å…¥é–€ã€æ±Ÿå´è²´è£• 2020 ã‚ˆã‚Šæ”¹å¤‰</cite></figcaption>
</figure>
<p>Any other probability distributions related to real phenomena like this?</p>

</section>
<section>
<h2 id="major-probability-distributions-and-related-phenomena">Major probability distributions and related phenomena</h2>
<dl>
<dt>Discrete uniform distribution</dt>
<dd>ã‚³ã‚¤ãƒ³ã®è¡¨è£ã€ã‚µã‚¤ã‚³ãƒ­ã®å‡ºç›®1â€“6</dd>
<dt>Negative binomial distribution (Geometric distribution if n = 1)</dt>
<dd>æˆåŠŸç‡pã®è©¦è¡ŒãŒåˆã‚ã¦æˆåŠŸã™ã‚‹ã¾ã§ã®å¤±æ•—å›æ•°</dd>
<dt>Binomial distribution</dt>
<dd>æˆåŠŸç‡pã€è©¦è¡Œå›æ•°nã®ã†ã¡ã®æˆåŠŸå›æ•°</dd>
<dt>Poisson distribution</dt>
<dd>å˜ä½æ™‚é–“ã‚ãŸã‚Šå¹³å‡$\lambda$å›èµ·ã“ã‚‹äº‹è±¡ã®ç™ºç”Ÿå›æ•°</dd>
<dt>Gamma distribution (Exponential distribution if k = 1)</dt>
<dd>ãƒã‚¢ã‚½ãƒ³éç¨‹ã§kå›èµ·ã“ã‚‹ã¾ã§ã®å¾…ã¡æ™‚é–“</dd>
<dt>Normal/Gaussian distribution</dt>
<dd>ç¢ºç‡å¤‰æ•°ã®å’Œã€å¹³å‡å€¤</dd>
</dl>

</section>
<section>
<h2 id="discrete-uniform-distribution">Discrete uniform distribution</h2>
<p>åŒã˜ç¢ºç‡ã§èµ·ã“ã‚‹né€šã‚Šã®äº‹è±¡ã®ã†ã¡XãŒèµ·ã“ã‚‹ç¢ºç‡</p>
<p>e.g., ã‚³ã‚¤ãƒ³ã®è¡¨è£ã€ã‚µã‚¤ã‚³ãƒ­ã®å‡ºç›®1â€“6</p>
<p><img src="./figure/dunif-1.png" alt="plot of chunk dunif"></p>
<p>ğŸ”° ä¸€æ§˜åˆ†å¸ƒã«ãªã‚Šãã†ãªä¾‹ã‚’è€ƒãˆã¦ã¿ã‚ˆã†</p>

</section>
<section>
<h2 id="geometric-textgeomp">Geometric $~\text{Geom}(p)$</h2>
<p>æˆåŠŸç‡pã®è©¦è¡ŒãŒåˆã‚ã¦æˆåŠŸã™ã‚‹ã¾ã§ã®å¤±æ•—å›æ•°</p>
<p>e.g., ã‚³ã‚¤ãƒ³ãƒˆã‚¹ã§è¡¨ãŒå‡ºã‚‹ã¾ã§ã«ä½•å›è£ãŒå‡ºã‚‹ã‹</p>
<p><img src="./figure/geometric-1.png" alt="plot of chunk geometric"></p>
<p>\[
\text{Prob}(X = k \mid p) = p (1 - p)^k
\]</p>
<p>ã€Œåˆã‚ã¦æˆåŠŸã™ã‚‹ã¾ã§ã®è©¦è¡Œå›æ•°ã€ã¨ã™ã‚‹å®šç¾©ã‚‚ã‚ã‚‹ã€‚</p>
<p>ğŸ”° å¹¾ä½•åˆ†å¸ƒã«ãªã‚Šãã†ãªä¾‹ã‚’è€ƒãˆã¦ã¿ã‚ˆã†</p>

</section>
<section>
<h2 id="è² ã®äºŒé …åˆ†å¸ƒ-textnbn-p">è² ã®äºŒé …åˆ†å¸ƒ $~\text{NB}(n, p)$</h2>
<p>æˆåŠŸç‡pã®è©¦è¡ŒãŒnå›æˆåŠŸã™ã‚‹ã¾ã§ã®å¤±æ•—å›æ•°Xã€‚
n = 1 ã®ã¨ãå¹¾ä½•åˆ†å¸ƒã¨ä¸€è‡´ã€‚</p>
<p><img src="./figure/nbinom-1.png" alt="plot of chunk nbinom"></p>
<p>\[
\text{Prob}(X = k \mid n,~p) = \binom {n + k - 1} k p^n (1 - p)^k
\]</p>
<p>å¤±æ•—å›æ•°ã§ã¯ãªãè©¦è¡Œå›æ•°ã‚’å¤‰æ•°ã¨ã™ã‚‹å®šç¾©ã‚‚ã‚ã‚‹ã€‚</p>
<p>ğŸ”° è² ã®äºŒé …åˆ†å¸ƒã«ãªã‚Šãã†ãªä¾‹ã‚’è€ƒãˆã¦ã¿ã‚ˆã†</p>
<!--
å¹³å‡$\lambda$ãŒã‚¬ãƒ³ãƒåˆ†å¸ƒã§ã°ã‚‰ã¤ã„ãŸãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã€ã¨ã‚‚è§£é‡ˆã§ãã‚‹ã€‚<br>
($k \to \infty$ã§ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã¨ä¸€è‡´)
-->

</section>
<section>
<h2 id="äºŒé …åˆ†å¸ƒ-textbinomialnp">äºŒé …åˆ†å¸ƒ $~\text{Binomial}(n,~p)$</h2>
<p>ç¢ºç‡$p$ã§å½“ãŸã‚‹ã‚¯ã‚¸ã‚’$n$å›å¼•ã„ã¦Xå›å½“ãŸã‚‹ç¢ºç‡ã€‚å¹³å‡ã¯$np$ã€‚</p>
<p><img src="./figure/dbinom-n-1.png" alt="plot of chunk dbinom-n"></p>
<p>\[
\text{Prob}(X = k \mid n,~p) = \binom n k p^k (1 - p)^{n - k}
\]</p>
<p>ğŸ”° äºŒé …åˆ†å¸ƒã«ãªã‚Šãã†ãªä¾‹ã‚’è€ƒãˆã¦ã¿ã‚ˆã†</p>

</section>
<section>
<h2 id="ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ-textpoissonlambda">ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ $~\text{Poisson}(\lambda)$</h2>
<p>å¹³å‡$\lambda$ã§å˜ä½æ™‚é–“(ç©ºé–“)ã‚ãŸã‚Šã«ç™ºç”Ÿã™ã‚‹äº‹è±¡ã®å›æ•°ã€‚</p>
<p>e.g., 1æ™‚é–“ã‚ãŸã‚Šã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡ä»¶æ•°ã€ãƒ¡ãƒƒã‚·ãƒ¥åŒºç”»å†…ã®ç”Ÿç‰©å€‹ä½“æ•°</p>
<p><img src="./figure/dpoisson-1.png" alt="plot of chunk dpoisson"></p>
<p>\[
\text{Prob}(X = k \mid \lambda) = \frac {\lambda^k e^{-\lambda}} {k!}
\]</p>
<p>äºŒé …åˆ†å¸ƒã®æ¥µé™ $(\lambda = np;~n \to \infty;~p \to 0)$ã€‚<br>
ã‚ã£ãŸã«èµ·ããªã„ã“ã¨ã‚’ä½•å›ã‚‚è©¦è¡Œã™ã‚‹ã‚ˆã†ãªæ„Ÿã˜ã€‚</p>

</section>
<section>
<h2 id="æŒ‡æ•°åˆ†å¸ƒ-textexplambda">æŒ‡æ•°åˆ†å¸ƒ $~\text{Exp}(\lambda)$</h2>
<p>ãƒã‚¢ã‚½ãƒ³éç¨‹ã®äº‹è±¡ã®ç™ºç”Ÿé–“éš”ã€‚å¹³å‡ã¯ $1 / \lambda$ ã€‚</p>
<p>e.g., ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å—ä¿¡é–“éš”ã€é“è·¯æ²¿ã„ã«è½ã¡ã¦ã‚‹æ‰‹è¢‹ã®é–“éš”</p>
<p><img src="./figure/dexp-1.png" alt="plot of chunk dexp"></p>
<p>\[
\text{Prob}(x \mid \lambda) = \lambda e^{-\lambda x}
\]</p>
<p>å¹¾ä½•åˆ†å¸ƒã®é€£ç¶šå€¤ç‰ˆã€‚</p>
<p>ğŸ”° ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒãƒ»æŒ‡æ•°åˆ†å¸ƒã«ãªã‚Šãã†ãªä¾‹ã‚’è€ƒãˆã¦ã¿ã‚ˆã†</p>

</section>
<section>
<h2 id="ã‚¬ãƒ³ãƒåˆ†å¸ƒ-textgammaklambda">ã‚¬ãƒ³ãƒåˆ†å¸ƒ $~\text{Gamma}(k,~\lambda)$</h2>
<p>ãƒã‚¢ã‚½ãƒ³éç¨‹ã®äº‹è±¡kå›ç™ºç”Ÿã¾ã§ã®å¾…ã¡æ™‚é–“</p>
<p>e.g., ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’2ã¤å—ä¿¡ã™ã‚‹ã¾ã§ã®å¾…ã¡æ™‚é–“</p>
<p><img src="./figure/dgamma-1.png" alt="plot of chunk dgamma"></p>
<p>\[
\text{Prob}(x \mid k,~\lambda) = \frac {\lambda^k x^{k - 1} e^{-\lambda x}} {\Gamma(k)}
\]</p>
<p>æŒ‡æ•°åˆ†å¸ƒã‚’kã®ã¶ã‚“å³ã«è†¨ã‚‰ã¾ã›ãŸæ„Ÿã˜ã€‚<br>
shapeãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $k = 1$ ã®ã¨ãæŒ‡æ•°åˆ†å¸ƒã¨ä¸€è‡´ã€‚</p>

</section>
<section>
<h2 id="æ­£è¦åˆ†å¸ƒ-mathcalnmusigma">æ­£è¦åˆ†å¸ƒ $~\mathcal{N}(\mu,~\sigma)$</h2>
<p>å¹³å‡ $\mu$ã€æ¨™æº–åå·® $\sigma$ ã®ç¾ã—ã„åˆ†å¸ƒã€‚ã‚ˆãç™»å ´ã™ã‚‹ã€‚<br>
e.g., $\mu = 50, ~\sigma = 10$ (æ¿ƒã„ç°è‰²ã«ãƒ‡ãƒ¼ã‚¿ã®95%, 99%ãŒå«ã¾ã‚Œã‚‹):</p>
<p><img src="./figure/gaussian-1.png" alt="plot of chunk gaussian"></p>
<p>\[
\text{Prob}(x \mid \mu,~\sigma) = \frac 1 {\sqrt{2 \pi \sigma^2}} \exp \left(\frac {-(x - \mu)^2} {2\sigma^2} \right)
\]</p>

</section>
<section>
<h2 id="æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ãã‚‚ã®ãŒã„ã‚ã„ã‚ã‚ã‚‹">æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ãã‚‚ã®ãŒã„ã‚ã„ã‚ã‚ã‚‹</h2>
<p>æ¨™æœ¬å¹³å‡ã®åå¾©(<strong>ä¸­å¿ƒæ¥µé™å®šç†</strong>);
e.g., ä¸€æ§˜åˆ†å¸ƒ [0, 100) ã‹ã‚‰40ã‚µãƒ³ãƒ—ãƒ«</p>
<p><img src="./figure/central-limit-1.png" alt="plot of chunk central-limit"></p>
<p>å¤§ãã„$n$ã®äºŒé …åˆ†å¸ƒ</p>
<p><img src="./figure/binom-normal-1.png" alt="plot of chunk binom-normal"></p>

</section>
<section>
<h2 id="æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ãã‚‚ã®ãŒã„ã‚ã„ã‚ã‚ã‚‹">æ­£è¦åˆ†å¸ƒã«è¿‘ã¥ãã‚‚ã®ãŒã„ã‚ã„ã‚ã‚ã‚‹</h2>
<p>å¤§ãã„$\lambda$ã®ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ</p>
<p><img src="./figure/poisson-normal-1.png" alt="plot of chunk poisson-normal"></p>
<p>å¹³å‡å€¤å›ºå®šãªã‚‰$k$ãŒå¤§ãããªã‚‹ã»ã©å·¦å³å¯¾ç§°ã«å°–ã‚‹ã‚¬ãƒ³ãƒåˆ†å¸ƒ</p>
<p><img src="./figure/gamma-normal-1.png" alt="plot of chunk gamma-normal"></p>

</section>
<section>
<h2 id="major-probability-distributions-and-related-phenomena">Major probability distributions and related phenomena</h2>
<figure style="float: right;">
<img src="../tokiomarine2021/math-model.drawio.svg" width="420"><br>
</figure>
<dl>
<dt>Discrete uniform distribution</dt>
<dd>ã‚³ã‚¤ãƒ³ã®è¡¨è£ã€ã‚µã‚¤ã‚³ãƒ­ã®å‡ºç›®1â€“6</dd>
<dt>Negative binomial distribution (Geometric distribution if n = 1)</dt>
<dd>æˆåŠŸç‡pã®è©¦è¡ŒãŒåˆã‚ã¦æˆåŠŸã™ã‚‹ã¾ã§ã®å¤±æ•—å›æ•°</dd>
<dt>Binomial distribution</dt>
<dd>æˆåŠŸç‡pã€è©¦è¡Œå›æ•°nã®ã†ã¡ã®æˆåŠŸå›æ•°</dd>
<dt>Poisson distribution</dt>
<dd>å˜ä½æ™‚é–“ã‚ãŸã‚Šå¹³å‡$\lambda$å›èµ·ã“ã‚‹äº‹è±¡ã®ç™ºç”Ÿå›æ•°</dd>
<dt>Gamma distribution (Exponential distribution if k = 1)</dt>
<dd>ãƒã‚¢ã‚½ãƒ³éç¨‹ã§kå›èµ·ã“ã‚‹ã¾ã§ã®å¾…ã¡æ™‚é–“</dd>
<dt>Normal/Gaussian distribution</dt>
<dd>ç¢ºç‡å¤‰æ•°ã®å’Œã€å¹³å‡å€¤</dd>
</dl>

</section>
<section>
<h2 id="ç¾å®Ÿã«ã¯ç¢ºç‡åˆ†å¸ƒã«å¾“ã‚ãªã„ã“ã¨ãŒå¤šã„">ç¾å®Ÿã«ã¯ã€ç¢ºç‡åˆ†å¸ƒã«ã€Œå¾“ã‚ãªã„ã€ã“ã¨ãŒå¤šã„</h2>
<p>æ¤ç‰©100å€‹ä½“ã‹ã‚‰8å€‹ãšã¤ç¨®å­ã‚’å–ã£ã¦æ¤ãˆãŸã‚‰å…¨ä½“ã§åŠåˆ†ã¡ã‚‡ã„ç™ºèŠ½ã€‚<br>
è¦ª1å€‹ä½“ã‚ãŸã‚Šã®ç”Ÿå­˜æ•°ã¯<span style="color: #56B4E9;">n=8ã®äºŒé …åˆ†å¸ƒ</span>ã«ãªã‚‹ã¯ãšã ã‘ã©ã€<br>
æ¥µç«¯ãªå€¤(å…¨éƒ¨æ­»äº¡ã€å…¨éƒ¨ç”Ÿå­˜)ãŒå¤šã‹ã£ãŸã€‚</p>
<img src="figure/overdispersion-1.png" alt="plot of chunk overdispersion">
<p>ã€Œãã‚Œã¯ãªãœï¼Ÿã€ã¨è€ƒãˆã¦è¦å› ã‚’æ¢ã‚‹ã®ã‚‚çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ä»•äº‹ã€‚<br>
<strong>ã€Œæ™®é€šã¯ã“ã‚Œã«å¾“ã†ã¯ãšã€ã‚’ç†è§£ã—ã¦ã“ã</strong>ã§ãã‚‹æ€è€ƒã€‚</p>

</section>
<section>
<h2 id="pseudo-random-number-generator">Pseudo Random Number Generator</h2>
<p>ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ä¸Šã§ãƒ©ãƒ³ãƒ€ãƒ <strong>ã£ã½ã„</strong>æ•°å€¤ã‚’å‡ºåŠ›ã™ã‚‹è£…ç½®ã€‚<br>
å®Ÿéš›ã«ã¯<strong>æ±ºå®šè«–çš„</strong>ã«è¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€<br>
ã‚·ãƒ¼ãƒ‰(å‡ºç™ºç‚¹)ã¨å‘¼ã³å‡ºã—å›æ•°ãŒåŒã˜ãªã‚‰å‡ºã‚‹æ•°ã‚‚åŒã˜ã«ãªã‚‹ã€‚</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">runif</span><span class="p">(</span><span class="m">3L</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 0.9148060 0.9370754 0.2861395</span>
</span></span><span class="line"><span class="cl"><span class="nf">runif</span><span class="p">(</span><span class="m">3L</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 0.8304476 0.6417455 0.5190959</span>
</span></span><span class="line"><span class="cl"><span class="nf">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">runif</span><span class="p">(</span><span class="m">6L</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 0.9148060 0.9370754 0.2861395 0.8304476 0.6417455 0.5190959</span>
</span></span></code></pre></div><p>ã‚·ãƒ¼ãƒ‰ã«é©å½“ãªå›ºå®šå€¤ã‚’ä¸ãˆã¦ãŠãã“ã¨ã§å†ç¾æ€§ã‚’ä¿ã¦ã‚‹ã€‚<br>
ãŸã ã—ã€Œã“ã®ã‚·ãƒ¼ãƒ‰ã˜ã‚ƒãªã„ã¨è‰¯ã„çµæœãŒå‡ºãªã„ã€ã¯ãƒ€ãƒ¡ã€‚</p>
<p>ã•ã¾ã–ã¾ãªã€Œåˆ†å¸ƒã«å¾“ã†ã€ä¹±æ•°ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚‚ã§ãã‚‹ã€‚</p>

</section>
<section>
<h2 id="ã„ã‚ã‚“ãªä¹±æ•°ã‚’ç”Ÿæˆå¯è¦–åŒ–ã—ã¦æ„Ÿè¦šã‚’æ´ã‚‚ã†">ã„ã‚ã‚“ãªä¹±æ•°ã‚’ç”Ÿæˆãƒ»å¯è¦–åŒ–ã—ã¦æ„Ÿè¦šã‚’æ´ã‚‚ã†</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">n</span> <span class="o">=</span> <span class="m">100</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nf">sample.int</span><span class="p">(</span><span class="m">6</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nf">runif</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">min</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">max</span> <span class="o">=</span> <span class="m">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nf">rgeom</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="m">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nf">rbinom</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="m">3</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="m">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nf">rpois</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">lambda</span> <span class="o">=</span> <span class="m">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mean</span> <span class="o">=</span> <span class="m">50</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="m">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">p1</span> <span class="o">=</span> <span class="nf">ggplot</span><span class="p">(</span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">p1</span> <span class="o">+</span> <span class="nf">geom_histogram</span><span class="p">()</span> <span class="c1"># for continuous values</span>
</span></span><span class="line"><span class="cl"><span class="n">p1</span> <span class="o">+</span> <span class="nf">geom_bar</span><span class="p">()</span>       <span class="c1"># for discrete values</span>
</span></span></code></pre></div><p>ğŸ”° å°ã•ã„ <code>n</code> ã‹ã‚‰å¾ã€…ã«å¤§ããã—ã¦å¤‰åŒ–ã‚’ç¢ºèªã—ã‚ˆã†ã€‚</p>
<p>ğŸ”° ã»ã‹ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚‚ã„ã‚ã„ã‚å¤‰ãˆã¦å¤‰åŒ–ã‚’ç¢ºèªã—ã‚ˆã†ã€‚</p>
<p>ğŸ”° 1%ã®å½“ãŸã‚Šã‚’ç‹™ã£ã¦10é€£ã‚¬ãƒãƒ£ã‚’å›ã™äººãŒ100ä¸‡äººã„ãŸã‚‰ã€<br>
å…¨éƒ¨ã¯ãšã‚Œã€1ã¤å½“ãŸã‚Šã€2ã¤å½“ãŸã‚Š&hellip; ã®äººã¯ã©ã‚Œãã‚‰ã„ã„ã‚‹ã‹ï¼Ÿ</p>
<p>(Quartoã§ã©ã†ã¾ã¨ã‚ã‚‹ã‹ã€è…•ã®è¦‹ã›æ‰€)</p>

</section>
<section>
<h2 id="fitting-probability-distributions-to-data">Fitting probability distributions to data</h2>
<p>The number of seeds were counted for each of 50 plant individuals.<br>
Individual A has 2 seeds, B has 4 seeds, &hellip;</p>
<p><img src="./figure/poisson-seed-1.png" alt="plot of chunk poisson-seed"></p>
<p>This count data looks <span class="fragment custom blur">Poisson</span>-distributed.<br>
What is the optimal $\lambda$ value?</p>

</section>
<section>
<h2 id="fitting-probability-distributions-to-data">Fitting probability distributions to data</h2>
<p>The number of seeds were counted for each of 50 plant individuals.<br>
Individual A has 2 seeds, B has 4 seeds, &hellip;</p>
<p><img src="./figure/poisson-seed-lambda-1.png" alt="plot of chunk poisson-seed-lambda"></p>
<p>This count data looks Poisson-distributed.<br>
What is the optimal $\lambda$ value?</p>
<p>Observations in black.
<span style="color: #56B4E9;">Poisson distribution in blue</span>.
$\lambda \approx 3$ looks good.</p>

</section>
<section>
<h2 id="likelihood-a-measure-for-goodness-of-fit">Likelihood: a measure for goodness-of-fit</h2>
<p>The probability to observe the data $D$ given the model $M$.<br>
$\text{Prob}(D \mid M)$</p>
<p><strong>Likelihood function</strong> is the same probability from different viewpoints:</p>
<ul>
<li>as a function of model $M$ given the data $D$,<br>
$L(M \mid D)$<br></li>
<li>as a function of parameters $\theta$,<br>
$L(\theta \mid D)$ or $L(\theta)$</li>
</ul>

</section>
<section>
<h2 id="example-of-likelihood-calculation">Example of likelihood calculation</h2>
<p>Data $D$: 4 heads (H) and 1 tail (T) in tossing a coin 5 times</p>
<p>Assuming the probability of coming up head $p = 0.5$:</p>
<div>\[\begin{split}
L(0.5 \mid D)
  &= \binom 5 1 \times \text{Prob}(H \mid 0.5) ^ 4 \times \text{Prob}(T \mid 0.5) ^ 1 \\
  &= 5 \times 0.5 ^ 4 \times 0.5 ^ 1 = 0.15625
\end{split}\]</div>
<p>Assuming the probability of coming up head $p = 0.8$:</p>
<div>\[\begin{split}
L(0.8 \mid D)
  &= \binom 5 1 \times \text{Prob}(H \mid 0.8) ^ 4 \times \text{Prob}(T \mid 0.8) ^ 1 \\
  &= 5 \times 0.8 ^ 4 \times 0.2 ^ 1 = 0.4096
\end{split}\]</div>
<p>$L(0.8 \mid D) &gt; L(0.5 \mid D)$</p>
<p>$p = 0.8$ is more likely.</p>

</section>
<section>
<h2 id="likelihood-in-the-example-of-poisson-distribution">Likelihood in the example of Poisson distribution</h2>
<p>The number of seeds were counted for each of 50 plant individuals.</p>
<div>\[\begin{split}
L(\lambda \mid D)
  = \prod _i ^n \text{Prob}(X_i \mid \lambda)
  = \prod _i ^n \frac {\lambda ^ {X_i} e ^ {-\lambda}} {X_i !}
\end{split}\]</div>
<p><img src="./figure/poisson-seed-likelihood-1.png" alt="plot of chunk poisson-seed-likelihood"></p>
<p>OK, $\lambda = 3$ is better than the other two. What is the best.</p>

</section>
<section>
<h2 id="umuaximum-uluikelihood-ueustimation"><u>M</u>aximum <u>L</u>ikelihood <u>E</u>stimation</h2>
<p><strong>Log likelihood</strong> is often easier to handle.<br>
Solving the differential equation for $\lambda$ &hellip;&hellip; finds <strong>the sample mean</strong></p>
<div>\[\begin{split}
\log L(\lambda \mid D)
  &= \sum _i ^n \left[ X_i \log (\lambda) - \lambda - \log (X_i !) \right] \\
\frac {\mathrm d \log L(\lambda \mid D)} {\mathrm d \lambda}
  &= \frac 1 \lambda \sum _i ^n X_i - n = 0 \\
\hat \lambda &= \frac 1 n \sum _i ^n X_i
\end{split}\]</div>
<p><img src="./figure/poisson-mle-1.png" alt="plot of chunk poisson-mle"></p>

</section>
<section>
<h2 id="mle-does-not-give-you-true-Î»">MLE does not give you â€œtrue Î»â€</h2>
<p>The data was actually generated from â€œ$X \sim \text{Poisson}(\lambda = 3.0)$â€.</p>
<p>By replicating &ldquo;sample 50 individuals â†’ MLE&rdquo; 1,000 times,<br>
we find great variability in estimation and empirical distributions:</p>
<p><img src="./figure/poisson-mle-repl-1.png" alt="plot of chunk poisson-mle-repl"></p>
<p>Note: Fitting to each sample looks not bad!</p>

</section>
<section>
<h2 id="alleviated-by-increasing-sample-size">Alleviated by increasing sample size</h2>
<p>1,000 replications of MLE with $n$ individuals from $X \sim \text{Poisson}(\lambda = 3.0)$:</p>
<p><img src="./figure/poisson-mle-nsam-1.png" alt="plot of chunk poisson-mle-nsam"></p>
<p>Q. How much is enough?<br>
A. Depends on what you estimate, acceptable error range, etc.</p>

</section>
<section>
<h2 id="mathematical-models-in-data-science">Mathematical models in data science</h2>
<blockquote>
<p>All models are wrong, but some are useful. &mdash; George E. P. Box</p>
</blockquote>
<figure>
<img src="../tokiomarine2021/math-model.drawio.svg" width="900"><br>
<figcaption><cite>ã€Œãƒ‡ãƒ¼ã‚¿åˆ†æã®ãŸã‚ã®æ•°ç†ãƒ¢ãƒ‡ãƒ«å…¥é–€ã€æ±Ÿå´è²´è£• 2020 ã‚ˆã‚Šæ”¹å¤‰</cite></figcaption>
</figure>

</section>
<section>
<h2 id="toolbox-of-statistical-modelling">Toolbox of statistical modelling</h2>
<ul>
<li><strong>Random variable</strong> $X$</li>
<li><strong>Probability distribution</strong> $X \sim f(\theta)$
<ul>
<li><strong>parameters</strong> $\theta$</li>
</ul>
</li>
<li><strong>Likelihood</strong>
<ul>
<li>The probability to observe the data given the model: $\text{Prob}(D \mid M)$</li>
<li>as a function of model given the data â†’ <strong>likelihood function</strong> $L(M \mid D),~L(\theta \mid D)$</li>
<li><strong>Maximum Likelihood Estimation</strong> to fit parameters $\hat \theta$</li>
</ul>
</li>
</ul>

</section>
<section>
<h2 id="-challenge-1-likelihood">ğŸ”° Challenge 1: likelihood</h2>
<p>Rolling a dice 10 times, 3 sixes were observed.</p>
<ol>
<li>
<p>Calculate likelihood assuming the probability to come up 6 $p = 1/6$.</p>
</li>
<li>
<p>Calculate likelihood assuming the probability to come up 6 $p = 0.2$.</p>
</li>
<li>
<p>Draw a graph with $p$ as horizontal axis, log likelihood as vertical axis.</p>
</li>
<li>
<p>Estimate $p$ with MLE.<br>
Excellent, if solved with math; Good, if solved with R; OK, by eye or intuition.</p>
</li>
</ol>
<dl>
<dt>Hint</dt>
<dd>$\binom 5 2 = {}_5 \mathrm{C} _2 = 10$ can be achieved with <code>choose(5, 2)</code> in R.</dd>
</dl>

</section>
<section>
<h2 id="å‚è€ƒæ–‡çŒ®">å‚è€ƒæ–‡çŒ®</h2>
<ul>
<li><a href="https://amzn.to/33suMIZ">ãƒ‡ãƒ¼ã‚¿è§£æã®ãŸã‚ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€</a> ä¹…ä¿æ‹“å¼¥ 2012</li>
<li><a href="https://amzn.to/3uwx7Pb">Stanã¨Rã§ãƒ™ã‚¤ã‚ºçµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°</a> æ¾æµ¦å¥å¤ªéƒ 2016</li>
<li><a href="https://amzn.to/3o1eCzP">Rã¨Stanã§ã¯ã˜ã‚ã‚‹ ãƒ™ã‚¤ã‚ºçµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†æå…¥é–€</a> é¦¬å ´çœŸå“‰ 2019</li>
<li><a href="https://amzn.to/3uCxTKo">ãƒ‡ãƒ¼ã‚¿åˆ†æã®ãŸã‚ã®æ•°ç†ãƒ¢ãƒ‡ãƒ«å…¥é–€</a> æ±Ÿå´è²´è£• 2020</li>
<li><a href="https://amzn.to/3uznzCK">åˆ†æè€…ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿è§£é‡ˆå­¦å…¥é–€</a> æ±Ÿå´è²´è£• 2020</li>
<li><a href="https://amzn.to/3ty80Kv">çµ±è¨ˆå­¦ã‚’å“²å­¦ã™ã‚‹</a> å¤§å¡šæ·³ 2020</li>
<li><a href="https://amzn.to/2Q0f6JQ">ç§‘å­¦ã¨ãƒ¢ãƒ‡ãƒ«&mdash;ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å“²å­¦ å…¥é–€</a> Michael Weisberg 2017<br>
(åŸè‘—: <a href="https://amzn.to/3bdvhuI">Simulation and Similarity</a> 2013)</li>
</ul>
<a href="8-glm.html" class="readmore">
8. Statistical modeling 2: linear regression
</a>

</section>
</div>
</div>
</body>
</html>
