<!DOCTYPE html>
<html lang="ja">
<head prefix="og: http://ogp.me/ns#">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
<title>7. Statistical modeling 1: probability distribution, likelihood — Hands-on Introduction to R 2023</title>
<meta name="author" content="Watal M. Iwasaki">
<meta property="og:title" content="7. Statistical modeling 1: probability distribution, likelihood — Hands-on Introduction to R 2023">
<meta property="og:type" content="article">
<meta property="og:url" content="https://heavywatal.github.io/slides/english2023r/7-distribution.html">
<meta property="og:image" content="https://avatars.githubusercontent.com/heavywatal">
<meta property="og:description" content="">
<meta property="og:site_name" content="Slide decks — Heavy Watal">
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@heavywatal">
<meta name="twitter:creator" content="@heavywatal">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-V60H2JH0G6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-V60H2JH0G6');
</script>
<link rel="stylesheet" href="/slides/lib/reveal.js/reveal.css">
<script type="module">
import { Reveal } from "/slides/lib/reveal.js/reveal.js";
Reveal.configure({width: 1440, height: 1080});
</script>
<style>
html { font-size: 240%; }
</style>
<link rel="stylesheet" href="/slides/lib/katex/katex.min.css">
<script type="module" src="/slides/lib/katex/katex.min.js"></script>
<script type="module" src="/slides/lib/iconify.js"></script>
<script defer src="/slides/lib/reload-img-onclick.js"></script>
<link rel="stylesheet" href="/slides/css/style-reveal.css">
<link rel="stylesheet" href="style.css">
</head>
<body>
<div class="reveal">
<div class="slides">
<section>
<h1 id="hands-on-introduction-to-r-2023"><a href=".">Hands-on Introduction to R 2023</a></h1>
<div class="author">
岩嵜 航 (Watal M. Iwasaki, PhD)
</div>
<div class="affiliation">
Graduate School of Life Sciences, Tohoku University
</div>
<ol>
<li><a href="1-introduction.html">Introduction: what is data analysis and R basics</a>
<li><a href="2-visualization.html">Data visualization and reporting</a>
<li><a href="3-structure1.html">Data transformation 1: extract, summarize</a>
<li><a href="4-structure2.html">Data transformation 2: join, pivot</a>
<li><a href="5-content.html">Data cleansing and conversion: numbers, text</a>
<li><a href="6-input.html">Data input and interpretation</a>
<li class="current-deck"><a href="7-distribution.html">Statistical modeling 1: probability distribution, likelihood</a>
<li><a href="8-glm.html">Statistical modeling 2: linear regression</a>
</ol>
<div class="footnote">
2023-12-05 Tohoku University<br>
<a href="https://heavywatal.github.io/slides/english2023r/">https://heavywatal.github.io/slides/english2023r/</a>
</div>

</section>
<section>
<h2 id="purposes-of-this-hands-on-lectures">Purposes of this hands-on lectures</h2>
<h3 id="-every-biological-research-involves-data-and-models">✅ <del>Every biological research involves data and models</del></h3>
<h3 id="-you-want-to-do-reproducible-analysis">✅ <del>You want to do reproducible analysis</del></h3>
<h3 id="-learn-how-to-do-it-and-how-to-learn-more">✅ <del>Learn how to do it and how to learn more</del></h3>
<h3 id="-glance-at-the-basics-of-data-analysis">⬜ Glance at the basics of data analysis</h3>
<hr>
<p>You don&rsquo;t have to remember every command.<br>
Just repeat forgetting and searching.</p>
</section>
<section>
<h2 id="what-do-you-want-to-do-with-data">What do you want to do with data?</h2>
<ul>
<li>to <strong>understand</strong> phenomena</li>
<li>to <strong>predict</strong> future</li>
<li>to <strong>classify</strong> objects</li>
<li>to <strong>control</strong> behavior</li>
<li>to <strong>generate</strong> something new</li>
</ul>
<p>Is analysis necessary for that?<br>
Why not just raw data?</p>
</section>
<section>
<h2 id="look-back-day-1">Look back day 1</h2>
<iframe width="600" height="450" src="./1-introduction.html#/4"></iframe>
<iframe width="600" height="450" src="./1-introduction.html#/5"></iframe>
<iframe width="600" height="450" src="./1-introduction.html#/6"></iframe>
<iframe width="600" height="450" src="./1-introduction.html#/7"></iframe>
</section>
<section>
<h2 id="mathematical-models-in-data-science">Mathematical models in data science</h2>
<p>Mathematical expression of assumptions to simulate data generation<br>
e.g., the larger the more expensive: $\text{price} = A \times \text{carat} + B + \epsilon$</p>
<p><img src="./figure/lm-diamonds-1.png" alt="plot of chunk lm-diamonds"></p>
<dl>
<dt>Regression</dt>
<dd>express y as a function of x.</dd>
</dl>

</section>
<section>
<h2 id="extending-linear-regression">Extending linear regression</h2>
<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="280" alt="データ解析のための統計モデリング入門 久保拓弥 2012">
</a>
</figure>
<p><strong>Linear Model (LM)</strong> 👈 #7 today</p>
<p><span style="color: #888888;">    ↓ probability distribution</span></p>
<p><strong>Generalized Linear Model (GLM)</strong> <a href="8-glm.html">&mdash; #8 next time</a></p>
<p><span style="color: #888888;">    ↓ individual difference, random effect</span></p>
<p><strong>Generalized Linear Mixed Model (GLMM)</strong></p>
<p><span style="color: #888888;">    ↓ flexible modelling</span></p>
<p><strong>Hierarchical Bayesian Model (HBM)</strong></p>
<p><cite><a href="https://amzn.to/33suMIZ">データ解析のための統計モデリング入門</a> 久保拓弥 2012 より改変</cite></p>

</section>
<section>
<h2 id="two-parts-to-a-regression-model">Two parts to a regression model</h2>
<ol>
<li>
<p>Define a <strong>family of models</strong>: express generic pattern</p>
<ul>
<li>straight line: $y = a_1 + a_2 x$</li>
<li>log curve: $\log(y) = a_1 + a_2 x$</li>
<li>quadratic curve: $y = a_1 + a_2 x^2$</li>
</ul>
</li>
<li>
<p>Generate a <strong>fitted model</strong>: adjust parameters to get closer to the data</p>
<ul>
<li>$y = 3x + 7$</li>
<li>$y = 9x^2$</li>
</ul>
</li>
</ol>
<cite>
<a href="https://r4ds.had.co.nz/model-basics.html" class="url">https://r4ds.had.co.nz/model-basics.html</a>
</cite>
</section>
<section>
<h2 id="can-see-a-strong-pattern-the-taller-the-heavier">Can see a strong pattern: the taller the heavier</h2>
<p>The relationship looks linear, $y = a x + b$.<br>
 </p>
<p><img src="./figure/weight-height-1.png" alt="plot of chunk weight-height"></p>

</section>
<section>
<h2 id="can-see-a-strong-pattern-the-taller-the-heavier">Can see a strong pattern: the taller the heavier</h2>
<p>The relationship looks linear, $y = a x + b$.<br>
OK, let&rsquo;s try random slope <em>a</em> and intersect <em>b</em>:</p>
<p><img src="./figure/weight-lines-1.png" alt="plot of chunk weight-lines"></p>
<p>Need to find a good slope and intersect.</p>

</section>
<section>
<h2 id="ordinary-least-square-ols">Ordinary Least Square (OLS)</h2>
<p>minimizes the <strong style="color: #E69F00">residual</strong> sum of squares (RSS)
from <span style="color: #3366ff">the regression line</span>.</p>
<p><img src="./figure/weight-residual-1.png" alt="plot of chunk weight-residual"></p>

</section>
<section>
<h2 id="searching-for-models-to-minimize-rss">Searching for models to minimize RSS</h2>
<p>Try random values, and pick the best ones.<br>
May need to generate much more to find good one.</p>
<p><img src="./figure/weight-goodlines-1.png" alt="plot of chunk weight-goodlines"></p>

</section>
<section>
<h2 id="searching-for-models-to-minimize-rss">Searching for models to minimize RSS</h2>
<p><strong>Grid search</strong>: generate an evenly spaced grid of points.<br>
Slightly more efficient than random search?</p>
<p><img src="./figure/weight-grid-1.png" alt="plot of chunk weight-grid"></p>
<p>There are many other <strong>optimization</strong> techniques although not covered here.</p>

</section>
<section>
<h2 id="r-can-find-the-optimum-in-an-instant">R can find the optimum in an instant</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">par_init</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="n">intercept</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">slope</span> <span class="o">=</span> <span class="m">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span> <span class="o">=</span> <span class="nf">optim</span><span class="p">(</span><span class="n">par_init</span><span class="p">,</span> <span class="n">fn</span> <span class="o">=</span> <span class="n">rss_weight</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">df_weight</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span><span class="o">$</span><span class="n">par</span>
</span></span></code></pre></div><pre tabindex="0"><code>intercept     slope 
-69.68394  78.53490 
</code></pre><p><img src="./figure/weight-lm-1.png" alt="plot of chunk weight-lm"></p>
<p>The code above is for general optimization.<br>
For simple linear regression, an easier way is as follows&hellip;</p>

</section>
<section>
<h2 id="lm-function-to-fit-linear-models"><code>lm()</code> function to fit linear models</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">fit</span> <span class="o">=</span> <span class="nf">lm</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">mpg</span><span class="p">,</span> <span class="n">formula</span> <span class="o">=</span> <span class="n">hwy</span> <span class="o">~</span> <span class="n">displ</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">broom</span><span class="o">::</span><span class="nf">tidy</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span>
</span></span></code></pre></div><pre tabindex="0"><code>         term  estimate std.error statistic       p.value
1 (Intercept) 35.697651 0.7203676  49.55477 2.123519e-125
2       displ -3.530589 0.1945137 -18.15085  2.038974e-46
</code></pre><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">mpg_added</span> <span class="o">=</span> <span class="n">modelr</span><span class="o">::</span><span class="nf">add_predictions</span><span class="p">(</span><span class="n">mpg</span><span class="p">,</span> <span class="n">fit</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&#34;response&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">ggplot</span><span class="p">(</span><span class="n">mpg_added</span><span class="p">)</span> <span class="o">+</span> <span class="nf">aes</span><span class="p">(</span><span class="n">displ</span><span class="p">,</span> <span class="n">hwy</span><span class="p">)</span> <span class="o">+</span> <span class="nf">geom_point</span><span class="p">()</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">geom_line</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">y</span> <span class="o">=</span> <span class="n">pred</span><span class="p">),</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">&#34;#3366ff&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p><img src="./figure/lm-mpg-1.png" alt="plot of chunk lm-mpg"></p>
<p>🔰 Try <code>lm()</code> with <code>diamonds</code> and <code>iris</code>.</p>

</section>
<section>
<h2 id="straight-lm-does-not-fit-all">Straight LM does not fit all</h2>
<p><img src="./figure/lm-bad-1.png" alt="plot of chunk lm-bad"></p>
<ul>
<li>Prediction goes below zero whereas all the observations are <strong>positive</strong>.</li>
<li>Y values are <strong>integer</strong>. Their <strong>dispersion</strong> is larger when X is larger.</li>
</ul>
</section>
<section>
<h2 id="straight-lm-does-not-fit-all">Straight LM does not fit all</h2>
<p><img src="./figure/glm-better-1.png" alt="plot of chunk glm-better"></p>
<ul>
<li>Prediction goes below zero whereas all the observations are <strong>positive</strong>.</li>
<li>Y values are <strong>integer</strong>. Their <strong>dispersion</strong> is larger when X is larger.</li>
<li>Let&rsquo;s learn statistical modelling for better fitting to the data.</li>
</ul>
</section>
<section>
<h2 id="extending-linear-regression">Extending linear regression</h2>
<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="280" alt="データ解析のための統計モデリング入門 久保拓弥 2012">
</a>
</figure>
<p><strong>Linear Model (LM)</strong> 👈 #7 today</p>
<p><span style="color: #888888;">    ↓ <span class="fragment highlight-blue custom bold">probability distribution</span></span></p>
<p><strong>Generalized Linear Model (GLM)</strong> <a href="8-glm.html">&mdash; #8 next time</a></p>
<p><span style="color: #888888;">    ↓ individual difference, random effect</span></p>
<p><strong>Generalized Linear Mixed Model (GLMM)</strong></p>
<p><span style="color: #888888;">    ↓ flexible modelling</span></p>
<p><strong>Hierarchical Bayesian Model (HBM)</strong></p>
<p><cite><a href="https://amzn.to/33suMIZ">データ解析のための統計モデリング入門</a> 久保拓弥 2012 より改変</cite></p>

</section>
<section>
<h2 id="probability-distribution">Probability distribution</h2>
<p>The relationship between phenomena and their frequencies.</p>
<dl>
<dt>empirical distribution</dt>
<dd>created by collecting samples.<br></dd>
<dd>e.g., rolling a dice 12 times, heights of 1000 students:</dd>
</dl>
<p><img src="./figure/distribution-1.png" alt="plot of chunk distribution"></p>
<dl>
<dt>theoretical distribution</dt>
<dd>described with math equation and a few parameters.</dd>
</dl>

</section>
<section>
<h2 id="random-variable-x-follows-probability-distribution-f">Random variable $X$ follows probability distribution $f$</h2>
<p>$X \sim f(\theta)$</p>
<p>e.g.,<br>
The number of heads in tossing 3 fair coins $X$ <strong>follows binomial distribution</strong>.<br>
$X \sim \text{Binomial}(n = 3, p = 0.5)$</p>
<div class="column-container">
  <div class="column" style="flex-shrink: 2.0;">
<p><img src="./figure/dbinom-1.png" alt="plot of chunk dbinom"></p>
  </div>
  <div class="column" style="padding-top: 10px;">
\[\begin{split}
\Pr(X = k) &= \binom n k p^k (1 - p)^{n - k} \\
k &\in \{0, 1, 2, \ldots, n\}
\end{split}\]
  </div>
</div>
<p>Let&rsquo;s experiment.</p>

</section>
<section>
<h2 id="record-repeated-trials">Record repeated trials</h2>
<p>The number of heads observed in tossing 3 fair coins: $X$</p>
<p>trial 1: <strong>H</strong> T <strong>H</strong> → $X = 2$<br>
trial 2: T T T → $X = 0$<br>
trial 3: <strong>H</strong> T T → $X = 1$, subsequently, $2, 1, 3, 0, 2, \ldots$</p>
<p><img src="./figure/rbinom-1.png" alt="plot of chunk rbinom"></p>
<div style="text-align: right;">
It approaches asymptotically to <b>binomial distribution</b>.<br>
0 and 3 are rare. 1 and 2 are three times more likely.
</div>
</section>
<section>
<h2 id="similar-values-can-be-generated-without-tossing-coins">Similar values can be generated without tossing coins</h2>
<ul>
<li>The number of heads $X$ observed in tossing 3 fair coins.</li>
<li>Random samples $X$ from the binomial distribution with $n = 3, p = 0.5$.</li>
</ul>
<div class="column-container">
  <div class="column" style="flex-shrink: 2.0;">
<img src="figure/dbinom-1.png" alt="plot of chunk dbinom">
  </div>
  <div class="column" style="padding-top: 10px;">
$X \sim \text{Binomial}(n = 3, p = 0.5)$
<p>   ↓ sample</p>
<p>{2, 0, 1, 2, 1, 3, 0, 2, &hellip;}</p>
  </div>
</div>
<p>These are so similar that we can say<br>
&ldquo;The number of heads in <em>n</em> tosses follows binomial distribution.&rdquo;</p>
<p>Conversely, we can understand it like<br>
&ldquo;Random variable of binomial distribution is the number of successes in <em>n</em> trials.&rdquo;</p>

</section>
<section>
<h2 id="a-kind-of-statistical-modelling">A kind of statistical modelling</h2>
<p>Tossing 3 fair coins repeatedly {2, 0, 1, 2, 1, 3, 0, 2, &hellip;}</p>
<p>   ↑ simulate phenomena with a few parameters</p>
<p>Sample from binomial distribution with $n = 3, p = 0.5$</p>
<figure>
<img src="../english2023r/image/math-model.drawio.svg" width="900"><br>
<figcaption><cite>「データ分析のための数理モデル入門」江崎貴裕 2020 より改変</cite></figcaption>
</figure>
<p>Any other probability distributions related to real phenomena like this?</p>

</section>
<section>
<h2 id="major-probability-distributions-and-related-phenomena">Major probability distributions and related phenomena</h2>
<dl>
<dt>Discrete uniform distribution</dt>
<dd>tossing fair coins, rolling fair dice</dd>
<dt>Negative binomial distribution<br>(Geometric distribution if n = 1)</dt>
<dd>failures before the n-th success in trials with p</dd>
<dt>Binomial distribution</dt>
<dd>successes in n trials with p</dd>
<dt>Poisson distribution</dt>
<dd>occurrences of a Poisson process with $\lambda$</dd>
<dt>Gamma distribution (Exponential distribution if k = 1)</dt>
<dd>waiting time until k-th occurrence of a Poisson process with $\lambda$</dd>
<dt>Normal/Gaussian distribution</dt>
<dd>sum of random variables, sample means, etc.</dd>
</dl>

</section>
<section>
<h2 id="discrete-uniform-distribution">Discrete uniform distribution</h2>
<p>Every X in n values has equal probability $1/n$.</p>
<p>e.g., fair coin [0,1], fair dice [1,6]</p>
<p><img src="./figure/dunif-1.png" alt="plot of chunk dunif"></p>
<p>🔰 Other examples of discrete uniform distribution?</p>

</section>
<section>
<h2 id="geometric-distribution-textgeomp">Geometric distribution $~\text{Geom}(p)$</h2>
<p>$X$ failures before the first success with success probability $p$ for each trial.</p>
<p>e.g., How many tails before first head with a coin?</p>
<p><img src="./figure/geometric-1.png" alt="plot of chunk geometric"></p>
<p>\[
\Pr(X = k \mid p) = p (1 - p)^k
\]</p>
<p>There is another definition: $X$ trials until the first success.</p>
<p>🔰 Other examples?</p>

</section>
<section>
<h2 id="negative-binomial-distribution-textnbn-p">Negative binomial distribution $~\text{NB}(n, p)$</h2>
<p>$X$ failures before the n-th success with success probability $p$ for each trial.<br>
(identical to geometric distribution when n = 1)</p>
<p><img src="./figure/nbinom-1.png" alt="plot of chunk nbinom"></p>
<p>\[
\Pr(X = k \mid n,~p) = \binom {n + k - 1} k p^n (1 - p)^k
\]</p>
<p>There is another definition: $X$ trials until the n-th success.</p>
<p>🔰 Other examples?</p>
<!--
平均$\lambda$がガンマ分布でばらついたポアソン分布、とも解釈できる。<br>
($k \to \infty$でポアソン分布と一致)
-->

</section>
<section>
<h2 id="binomial-distribution-textbinomialnp">Binomial distribution $~\text{Binomial}(n,~p)$</h2>
<p>$X$ successes in $n$ trials with success probability $p$ for each trial.</p>
<p><img src="./figure/dbinom-n-1.png" alt="plot of chunk dbinom-n"></p>
<p>\[
\Pr(X = k \mid n,~p) = \binom n k p^k (1 - p)^{n - k}
\]</p>
<p>🔰 Other examples?</p>

</section>
<section>
<h2 id="poisson-distribution-textpoissonlambda">Poisson distribution $~\text{Poisson}(\lambda)$</h2>
<p>$X$ occurrences of a <strong>Poisson process</strong> in a fixed interval of time (space).<br>
<strong>Poisson process</strong>: Events occur at a constant rate $\lambda$</p>
<p>e.g., messages received per hour, number of individuals in a quadrat</p>
<p><img src="./figure/dpoisson-1.png" alt="plot of chunk dpoisson"></p>
<p>\[
\Pr(X = k \mid \lambda) = \frac {\lambda^k e^{-\lambda}} {k!}
\]</p>
<p>The limit of binomial distribution $(\lambda = np;~n \to \infty;~p \to 0)$<br>
≈ many trials of extremely rare events.</p>

</section>
<section>
<h2 id="exponential-distribution-textexplambda">Exponential distribution $~\text{Exp}(\lambda)$</h2>
<p>Interval $X$ between occurrences of a <strong>Poisson process</strong>.<br>
<strong>Poisson process</strong>: Events occur at a constant rate $\lambda$</p>
<p>e.g., intervals between messages received, between gloves left on a road</p>
<p><img src="./figure/dexp-1.png" alt="plot of chunk dexp"></p>
<p>\[
\Pr(x \mid \lambda) = \lambda e^{-\lambda x}
\]</p>
<p>The continuous counterpart of geometric distribution.</p>
<p>🔰 Other examples?</p>

</section>
<section>
<h2 id="gamma-distribution-textgammaklambda">Gamma distribution $~\text{Gamma}(k,~\lambda)$</h2>
<p>Waiting time $X$ until $k$-th occurrence of a <strong>Poisson process</strong>.
<strong>Poisson process</strong>: Events occur at a constant rate $\lambda$</p>
<p>e.g., Waiting time until receiving two messages</p>
<p><img src="./figure/dgamma-1.png" alt="plot of chunk dgamma"></p>
<p>\[
\Pr(x \mid k,~\lambda) = \frac {\lambda^k x^{k - 1} e^{-\lambda x}} {\Gamma(k)}
\]</p>
<p>Identical when shape parameter $k = 1$.</p>

</section>
<section>
<h2 id="normalgaussian-distribution-mathcalnmusigma">Normal/Gaussian distribution $~\mathcal{N}(\mu,~\sigma)$</h2>
<p>Beautiful distribution with two parameters: mean $\mu$, standard deviation $\sigma$.<br>
e.g., $\mu = 50, ~\sigma = 10$:</p>
<p><img src="./figure/gaussian-1.png" alt="plot of chunk gaussian"></p>
<p>\[
\Pr(x \mid \mu,~\sigma) = \frac 1 {\sqrt{2 \pi \sigma^2}} \exp \left(\frac {-(x - \mu)^2} {2\sigma^2} \right)
\]</p>

</section>
<section>
<h2 id="many-distributions-approach-normal-distribution">Many distributions approach normal distribution</h2>
<p>Distribution of sample means (<strong>central limit theorem</strong>);
e.g., average of 40 samples from uniform distribution [0, 100):</p>
<p><img src="./figure/central-limit-1.png" alt="plot of chunk central-limit"></p>
<p>Binomial distribution with large $n$:</p>
<p><img src="./figure/binom-normal-1.png" alt="plot of chunk binom-normal"></p>

</section>
<section>
<h2 id="many-distributions-approach-normal-distribution">Many distributions approach normal distribution</h2>
<p>Poisson distribution with large $\lambda$:</p>
<p><img src="./figure/poisson-normal-1.png" alt="plot of chunk poisson-normal"></p>
<p>Gamma distribution with large $k$:</p>
<p><img src="./figure/gamma-normal-1.png" alt="plot of chunk gamma-normal"></p>

</section>
<section>
<h2 id="major-probability-distributions-and-related-phenomena">Major probability distributions and related phenomena</h2>
<figure style="float: right;">
<img src="../english2023r/image/math-model.drawio.svg" width="420"><br>
</figure>
<dl>
<dt>Discrete uniform distribution</dt>
<dd>tossing fair coins, rolling fair dice</dd>
<dt>Negative binomial distribution<br>(Geometric distribution if n = 1)</dt>
<dd>failures before the n-th success in trials with p</dd>
<dt>Binomial distribution</dt>
<dd>successes in n trials with p</dd>
<dt>Poisson distribution</dt>
<dd>occurrences of a Poisson process with $\lambda$</dd>
<dt>Gamma distribution (Exponential distribution if k = 1)</dt>
<dd>waiting time until k-th occurrence of a Poisson process with $\lambda$</dd>
<dt>Normal/Gaussian distribution</dt>
<dd>sum of random variables, sample means, etc.</dd>
</dl>

</section>
<section>
<h2 id="real-data-rarely-follow-theoretical-distributions">Real data rarely follow theoretical distributions</h2>
<p>Collect and sow 8 seeds from each of 100 plant individuals.<br>
The number of survived seeds per parent should follow $\text{Binomial}(n = 8, p)$.<br>
But extreme cases (all survived, all dead) were frequently observed.</p>
<p><img src="./figure/overdispersion-1.png" alt="plot of chunk overdispersion"></p>
<p>&ldquo;Why? What other factors affect?&rdquo; is the question of statistical modelling.<br>
It requires the understanding of the <strong>null distribution</strong>.</p>

</section>
<section>
<h2 id="pseudo-random-number-generator">Pseudo Random Number Generator</h2>
<p>Algorithm to generate a sequence of random**-ish** numbers.<br>
Its computation is not stochastic, but <strong>deterministic</strong>.<br>
Exactly same values are generated if the same <strong>seed</strong> is set.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">runif</span><span class="p">(</span><span class="m">3L</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 0.9148060 0.9370754 0.2861395</span>
</span></span><span class="line"><span class="cl"><span class="nf">runif</span><span class="p">(</span><span class="m">3L</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 0.8304476 0.6417455 0.5190959</span>
</span></span><span class="line"><span class="cl"><span class="nf">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">runif</span><span class="p">(</span><span class="m">6L</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 0.9148060 0.9370754 0.2861395 0.8304476 0.6417455 0.5190959</span>
</span></span></code></pre></div><p>Reproducible results can be obtained by fixing a seed.<br>
Do NOT search for the seeds that produce favorable results.</p>
<p>Possible to generate various random numbers that follows probability distributions.</p>

</section>
<section>
<h2 id="generate-random-numbers-from-various-distributions">Generate random numbers from various distributions</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">n</span> <span class="o">=</span> <span class="m">100</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nf">sample.int</span><span class="p">(</span><span class="m">6</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nf">runif</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">min</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">max</span> <span class="o">=</span> <span class="m">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nf">rgeom</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="m">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nf">rbinom</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="m">3</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="m">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nf">rpois</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">lambda</span> <span class="o">=</span> <span class="m">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mean</span> <span class="o">=</span> <span class="m">50</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="m">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">p1</span> <span class="o">=</span> <span class="nf">ggplot</span><span class="p">(</span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">p1</span> <span class="o">+</span> <span class="nf">geom_histogram</span><span class="p">()</span> <span class="c1"># for continuous values</span>
</span></span><span class="line"><span class="cl"><span class="n">p1</span> <span class="o">+</span> <span class="nf">geom_bar</span><span class="p">()</span>       <span class="c1"># for discrete values</span>
</span></span></code></pre></div><p>🔰 Observe the effects of altering sample size <code>n</code>.</p>
<p>🔰 Observe the effects of altering parameters for each distribution.</p>
<p>(Use Quarto effectively)</p>

</section>
<section>
<h2 id="fitting-probability-distributions-to-data">Fitting probability distributions to data</h2>
<p>The number of seeds were counted for each of 50 plant individuals.<br>
Individual A has 2 seeds, B has 4 seeds, &hellip;</p>
<p><img src="./figure/poisson-seed-1.png" alt="plot of chunk poisson-seed"></p>
<p>This count data looks <span class="fragment custom blur">Poisson</span>-distributed.<br>
What is the optimal $\lambda$ value?</p>

</section>
<section>
<h2 id="fitting-probability-distributions-to-data">Fitting probability distributions to data</h2>
<p>The number of seeds were counted for each of 50 plant individuals.<br>
Individual A has 2 seeds, B has 4 seeds, &hellip;</p>
<p><img src="./figure/poisson-seed-lambda-1.png" alt="plot of chunk poisson-seed-lambda"></p>
<p>This count data looks Poisson-distributed.<br>
What is the optimal $\lambda$ value?</p>
<p>Observations in black.
<span style="color: #56B4E9;">Poisson distribution in blue</span>.
$\lambda = 3$ looks good.</p>

</section>
<section>
<h2 id="likelihood-a-measure-for-goodness-of-fit">Likelihood: a measure for goodness-of-fit</h2>
<p>The probability to observe the data $D$ given the model $M$.<br>
$\Pr(D \mid M)$</p>
<p><strong>Likelihood function</strong> is the same probability from different viewpoints:</p>
<ul>
<li>as a function of model $M$ given the data $D$,<br>
$L(M \mid D)$<br></li>
<li>as a function of parameters $\theta$,<br>
$L(\theta \mid D)$ or $L(\theta)$</li>
</ul>

</section>
<section>
<h2 id="example-of-likelihood-calculation">Example of likelihood calculation</h2>
<p>Data $D$: 4 heads (H) and 1 tail (T) in tossing a coin 5 times</p>
<p>Assuming the probability of coming up head $p = 0.5$:</p>
<div>\[\begin{split}
L(0.5 \mid D)
  &= \binom 5 1 \times \Pr(H \mid 0.5) ^ 4 \times \Pr(T \mid 0.5) ^ 1 \\
  &= 5 \times 0.5 ^ 4 \times 0.5 ^ 1 = 0.15625
\end{split}\]</div>
<p>Assuming the probability of coming up head $p = 0.8$:</p>
<div>\[\begin{split}
L(0.8 \mid D)
  &= \binom 5 1 \times \Pr(H \mid 0.8) ^ 4 \times \Pr(T \mid 0.8) ^ 1 \\
  &= 5 \times 0.8 ^ 4 \times 0.2 ^ 1 = 0.4096
\end{split}\]</div>
<p>$L(0.8 \mid D) &gt; L(0.5 \mid D)$</p>
<p>$p = 0.8$ is more likely.</p>

</section>
<section>
<h2 id="likelihood-in-the-example-of-poisson-distribution">Likelihood in the example of Poisson distribution</h2>
<p>The number of seeds were counted for each of 50 plant individuals.</p>
<div>\[\begin{split}
L(\lambda \mid D)
  = \prod _i ^n \Pr(X_i \mid \lambda)
  = \prod _i ^n \frac {\lambda ^ {X_i} e ^ {-\lambda}} {X_i !}
\end{split}\]</div>
<p><img src="./figure/poisson-seed-likelihood-1.png" alt="plot of chunk poisson-seed-likelihood"></p>
<p>OK, $\lambda = 3$ is better than the other two. What is the best.</p>

</section>
<section>
<h2 id="maximum-likelihood-estimation"><u>M</u>aximum <u>L</u>ikelihood <u>E</u>stimation</h2>
<p><strong>Log likelihood</strong> is often easier to handle.<br>
Solving the differential equation for $\lambda$ &hellip;&hellip; finds <strong>the sample mean</strong></p>
<div>\[\begin{split}
\log L(\lambda \mid D)
  &= \sum _i ^n \left[ X_i \log (\lambda) - \lambda - \log (X_i !) \right] \\
\frac {\mathrm d \log L(\lambda \mid D)} {\mathrm d \lambda}
  &= \frac 1 \lambda \sum _i ^n X_i - n = 0 \\
\hat \lambda &= \frac 1 n \sum _i ^n X_i
\end{split}\]</div>
<p><img src="./figure/poisson-mle-1.png" alt="plot of chunk poisson-mle"></p>

</section>
<section>
<h2 id="mle-does-not-give-you-true-λ">MLE does not give you “true λ”</h2>
<p>The data was actually generated from “$X \sim \text{Poisson}(\lambda = 3.0)$”.</p>
<p>By replicating &ldquo;sample 50 individuals → MLE&rdquo; 1,000 times,<br>
we find great variability in estimation and empirical distributions:</p>
<p><img src="./figure/poisson-mle-repl-1.png" alt="plot of chunk poisson-mle-repl"></p>
<p>Note: Fitting to each sample looks not bad!</p>

</section>
<section>
<h2 id="alleviated-by-increasing-sample-size">Alleviated by increasing sample size</h2>
<p>1,000 replications of MLE with $n$ individuals from $X \sim \text{Poisson}(\lambda = 3.0)$:</p>
<p><img src="./figure/poisson-mle-nsam-1.png" alt="plot of chunk poisson-mle-nsam"></p>
<p>Q. How much is enough?<br>
A. Depends on what you estimate, acceptable error range, etc.</p>

</section>
<section>
<h2 id="mathematical-models-in-data-science">Mathematical models in data science</h2>
<blockquote>
<p>All models are wrong, but some are useful. &mdash; George E. P. Box</p></blockquote>
<figure>
<img src="../english2023r/image/math-model.drawio.svg" width="900"><br>
<figcaption><cite>「データ分析のための数理モデル入門」江崎貴裕 2020 より改変</cite></figcaption>
</figure>
</section>
<section>
<h2 id="toolbox-of-statistical-modelling">Toolbox of statistical modelling</h2>
<ul>
<li><strong>Random variable</strong> $X$</li>
<li><strong>Probability distribution</strong> $X \sim f(\theta)$
<ul>
<li><strong>parameters</strong> $\theta$</li>
</ul>
</li>
<li><strong>Likelihood</strong>
<ul>
<li>The probability to observe the data given the model: $\Pr(D \mid M)$</li>
<li>as a function of model given the data<br>
→ <strong>likelihood function</strong> $L(M \mid D),~L(\theta \mid D)$</li>
<li><strong>Maximum Likelihood Estimation</strong> to fit parameters $\hat \theta$</li>
</ul>
</li>
</ul>

</section>
<section>
<h2 id="-challenge-likelihood-and-mle-by-hand">🔰 Challenge: likelihood and MLE by hand</h2>
<p>Rolling a dice 10 times, 3 sixes were observed.</p>
<ol>
<li>
<p>Calculate likelihood assuming the probability to come up 6 $p = 1/6$.</p>
</li>
<li>
<p>Calculate likelihood assuming the probability to come up 6 $p = 0.2$.</p>
</li>
<li>
<p><strong>Draw a graph</strong> with $p$ as horizontal axis, log likelihood as vertical axis.</p>
</li>
<li>
<p>Estimate $p$ with MLE.<br>
Excellent, if solved with math; Good, if solved with R; OK, by eye or intuition.</p>
</li>
</ol>
<dl>
<dt>Hint</dt>
<dd>$\binom 5 2 = {}_5 \mathrm{C} _2 = 10$ can be achieved with <code>choose(5, 2)</code> in R.</dd>
</dl>

</section>
<section>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://amzn.to/33suMIZ">データ解析のための統計モデリング入門</a> 久保拓弥 2012</li>
<li><a href="https://amzn.to/3uwx7Pb">StanとRでベイズ統計モデリング</a> 松浦健太郎 2016</li>
<li><a href="https://amzn.to/3o1eCzP">RとStanではじめる ベイズ統計モデリングによるデータ分析入門</a> 馬場真哉 2019</li>
<li><a href="https://amzn.to/3uCxTKo">データ分析のための数理モデル入門</a> 江崎貴裕 2020</li>
<li><a href="https://amzn.to/3uznzCK">分析者のためのデータ解釈学入門</a> 江崎貴裕 2020</li>
<li><a href="https://amzn.to/3ty80Kv">統計学を哲学する</a> 大塚淳 2020</li>
<li><a href="https://amzn.to/2Q0f6JQ">科学とモデル&mdash;シミュレーションの哲学 入門</a> Michael Weisberg 2017<br>
(原著: <a href="https://amzn.to/3bdvhuI">Simulation and Similarity</a> 2013)</li>
</ul>
<a href="8-glm.html" class="readmore">
8. Statistical modeling 2: linear regression
</a>

</section>
</div>
</div>
</body>
</html>
