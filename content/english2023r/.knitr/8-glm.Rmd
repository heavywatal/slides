```{r, setup-common}
#| file: "setup.R"
#| echo: false
#| results: "asis"
```
```{r, setup-local}
#| include: false
#| cache: false
```

---
## Extending linear regression

<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="280" alt="ãƒ‡ãƒ¼ã‚¿è§£æã®ãŸã‚ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€ ä¹…ä¿æ‹“å¼¥ 2012">
</a>
</figure>

**Linear Model (LM)** [--- #7 yesterday](7-distribution.html)

<span style="color: #888888;">&nbsp; &nbsp; â†“ probability distribution</span>

**Generalized Linear Model (GLM)** ğŸ‘ˆ #8 today

<span style="color: #888888;">&nbsp; &nbsp; â†“ individual difference, random effect</span>

**Generalized Linear Mixed Model (GLMM)**

<span style="color: #888888;">&nbsp; &nbsp; â†“ flexible modelling</span>

**Hierarchical Bayesian Model (HBM)**

<cite>[ãƒ‡ãƒ¼ã‚¿è§£æã®ãŸã‚ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€](https://amzn.to/33suMIZ) ä¹…ä¿æ‹“å¼¥ 2012 ã‚ˆã‚Šæ”¹å¤‰</cite>


---
## Straight LM does not fit all

<img `r src_alt_fig_chunk("glm-better")`>

- Prediction goes below zero whereas all the observations are **positive**.
- Y values are **integer**. Their **dispersion** is larger when X is larger.
- Let's learn statistical modelling for better fitting to the data.


---
## Statistical modelling in the previous session

Random variable $X$ follows the probability distribution $f$ with parameters $\theta$.<br>
$X \sim f(\theta) $

e.g., Number of seeds $X$ follows Poisson distribution with mean $\lambda$:

<div>\[\begin{split}
X \sim \text{Poisson}(\lambda)
\end{split}\]</div>

```{r, df-poisson-playback}
#| echo: false
set.seed(24601)
df_rpois = tibble::tibble(X = rpois(50L, 3))
max_x = 11L
df_dpois = tibble(X = seq(0, max_x), Prob = dpois(X, mean(df_rpois$X)))
```

```{r, only-dist}
#| echo: false
#| fig.height: 4
#| fig.width: 4
p_pois = ggplot(df_rpois) + aes(X) +
  geom_bar(aes(y = after_stat(prop)), width = 0.3) +
  geom_col(data = df_dpois, aes(y = Prob), alpha = 0.5, fill = "#56B4E9") +
  theme_bw(base_size = 18)
p_pois +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```


---
## Viewing the same model as GLM

The number of individual $i$'s seeds $y_i$ follows Poisson distribution with mean $\lambda_i$.<br>
Mean $\lambda_i$ is **common constant $\beta_0$**.

<div>\[\begin{split}
y_i &\sim \text{Poisson}(\lambda_i) \\
\lambda_i &= \beta_0
\end{split}\]</div>

```{r, glm-without-x}
#| echo: false
#| fig.height: 4
#| fig.width: 4
p_pois +
  labs(x = "y") +
  coord_flip() +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_blank(), axis.ticks = element_blank())
```

Just separated the equation into two, and flipped X-Y axes...? Why?<br>
The reason may be clearer in a model with **explanatory variables**.

---
## GLM with one explanatory variable

The number of individual $i$'s seeds $y_i$ follows Poisson distribution with mean $\lambda_i$.<br>
Mean $\lambda_i$ is **a linear function of its body mass $\x_i$**.

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0;">

<figure style="margin-block: 1em 0;">
<img src="../tokiomarine2021/glm.drawio.svg" width="600"><br>
</figure>

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r, df-seeds}
#| include: false
#| cache.vars: ["df_seeds", "a", "b"]
set.seed(24601)
n = 300L
a = 3
b = -3
df_seeds = tibble::tibble(
  body_mass = runif(n, 0.4, 1.7),
  num_seeds = rpois(n, exp(a * body_mass + b))
) |>
  print()
```
```{r, glm-poisson}
#| echo: false
#| cache.vars: []
#| fig.height: 5
#| fig.width: 5
x_breaks = c(0.5, 1.0, 1.5)
df_ridges = tidyr::crossing(x = x_breaks, y = seq_len(30L) - 1L) |>
  dplyr::mutate(density = dpois(y, exp(a * x + b))) |>
  dplyr::filter(density > 1e-4)
df_bars = df_ridges |> wtl::ridges2bars(y, density)

p_pois = ggplot(df_seeds) + aes(body_mass, num_seeds) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  scale_x_continuous(breaks = x_breaks) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())

p_pois +
  stat_smooth(formula = y ~ x, method = glm, method.args = list(family = poisson), se = FALSE) +
  ggridges::geom_vridgeline(aes(x, y, width = density * 0.5, group = x),
    data = df_bars, fill = "#56B4E9AA", linetype = 0)
```

  </div>
</div>

**simple linear regression** with a single explanatory variable â†‘ <br>
**multiple linear regression** with multiple explanatory variables â†’


---
## Multiple regression with multiple expl. variables

<p>\[\begin{split}
y_i &\sim \text{Poisson}(\lambda_i) \\
\log(\lambda_i) &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots
\end{split}\]</p>

ğŸº Beer sells better on hot and humid days:

```{r, df-beer}
#| include: false
#| cache.vars: "df_beer"
set.seed(24601)
n = 200L
true_coef = c(3, 0.05, 0.006)
df_beer = tibble::tibble(
  temperature = runif(n, 8, 32),
  humidity = runif(n, 20, 80),
  beer_sales = rpois(n, exp(true_coef[1] + true_coef[2] * temperature + true_coef[3] * humidity))
) |>
  print()
```
```{r, multiple-regression}
#| echo: false
#| cache.vars: []
#| fig.height: 5
#| fig.width: 10
glm_multi = glm(beer_sales ~ temperature + humidity, family = poisson, data = df_beer)

df_pred = tidyr::crossing(temperature = seq(8, 32, 2), humidity = seq(20, 80, 5)) |>
  modelr::add_predictions(glm_multi, type = "response")

p1 = ggplot(df_beer) + aes(temperature, beer_sales, color = humidity) +
  geom_line(data = df_pred, aes(y = pred, group = humidity), alpha = 0.7) +
  geom_point(alpha = 0.5) +
  scale_color_viridis_c(option = "cividis", direction = -1) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))
p2 = ggplot(df_beer) + aes(humidity, beer_sales, color = temperature) +
  geom_line(data = df_pred, aes(y = pred, group = temperature), alpha = 0.7) +
  geom_point(alpha = 0.5) +
  scale_color_viridis_c(option = "turbo") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))

cowplot::plot_grid(p1, p2, nrow = 1L)
```

Let's see other **probability distributions** and **link functions** â†’


---
## Logistic regression

- Probability distribution: **binomial distribution**
- Link function: $\text{logit}(p) = \log \frac {p} {1 - p}$

estimates the probability of event occurrence based on some variables.

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

$y_i$ in 10 customers ordered beer.<br>
$p_i$ varies by temperature $x_i$.

<p>\[\begin{split}
y_i &\sim \text{Binomial}(n,~p_i) \\
\text{logit}(p_i) &= \beta_0 + \beta_1 x_i \\
p_i &= \frac 1 {1 + e^{-(\beta_0 + \beta_1 x_i)}}
\end{split}\]</p>

Logistic function â†‘

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r, df-logistic}
#| include: false
#| cache.vars: ["df_logistic", "n"]
set.seed(24601)
sigmoid = function(x, gain = 1) {1 / (1 + exp(-gain * x))}
nrep = 200L
n = 10L
df_logistic = tibble::tibble(
  x = runif(nrep, -10, 35),
  logit_p = -3 + 0.3 * x,
  p = sigmoid(logit_p),
  y = rbinom(nrep, n, p),
  response = matrix(c(y, n - y), ncol = 2)
) |>
  print()
```
```{r, glm-logistic}
#| echo: false
#| cache.vars: []
#| fig.height: 5
#| fig.width: 5
glm_logistic = glm(response ~ x, df_logistic, family = binomial)
df_pred = df_logistic |>
  modelr::add_predictions(glm_logistic, type = "response") |>
  dplyr::mutate(pred = n * pred)

coef = glm_logistic$coefficients

x_breaks = c(-10, 0, 10, 20, 30)
df_ridges = tidyr::crossing(x = x_breaks, y = seq.int(0, n)) |>
  dplyr::mutate(p = sigmoid(coef[1] + coef[2] * x), density = dbinom(y, n, p)) |>
  dplyr::filter(density > 1e-4)
df_bars = df_ridges |> wtl::ridges2bars(y, density)

ggplot(df_pred) + aes(x, y) +
  geom_point(alpha = 0.5, shape = 16) +
  ggridges::geom_vridgeline(data = df_bars, aes(width = density * 6, group = x), fill = "#56B4E9AA", linetype = 0) +
  geom_line(aes(y = pred), linewidth = 2, color = "#3366ff") +
  scale_x_continuous(breaks = x_breaks) +
  scale_y_continuous(breaks = seq.int(0, 10)) +
  labs(x = "temperature", y = "beer_sales") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())
```

  </div>
</div>

???
ãƒ­ã‚¸ãƒƒãƒˆ = å¯¾æ•°ã‚ªãƒƒã‚º
ã‚ªãƒƒã‚º = å¤±æ•—ã®ä½•å€æˆåŠŸã—ã‚„ã™ã„ã‹
XãŒ1å¢—ãˆã‚‹ã¨ã‚ªãƒƒã‚ºãŒe^aå€ã«å¢—ãˆã‚‹ã€‚


---
## Logistic regression (narrow sense)

- Probability distribution: **Bernoulli distribution** (binomial with $n = 1$)
- Link function: $\text{logit}(p) = \log \frac {p} {1 - p}$

estimates the probability of event occurrence based on some variables.

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

Buckets sells better on windy days.

<p>\[\begin{split}
y_i &\sim \text{Bernoulli}(p_i) \\
  &= \text{Binomial}(1,~p_i) \\
\text{logit}(p_i) &= \beta_0 + \beta_1 x_i \\
p_i &= \frac 1 {1 + e^{-(\beta_0 + \beta_1 x_i)}}
\end{split}\]</p>

Logistic function â†‘

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r, df-wind}
#| include: false
#| cache.vars: "df_wind"
set.seed(24601)
n = 200
df_wind = tibble::tibble(
  max_wind = runif(n, 0, 40),
  bucket_sales = rbinom(n, 1L, sigmoid(max_wind - 20, 0.2)) + 0L) |>
  print()
```
```{r, wind}
#| echo: false
#| cache.vars: []
#| fig.height: 4
#| fig.width: 5
glm_bernoulli = glm(bucket_sales ~ max_wind, df_wind, family = "binomial")

coef = glm_bernoulli$coefficients
x_breaks = c(0, 10, 20, 30, 40)
df_ridges = tidyr::crossing(x = x_breaks, y = c(0, 1)) |>
  dplyr::mutate(p = sigmoid(coef[1] + coef[2] * x), density = dbinom(y, 1, p)) |>
  dplyr::filter(density > 1e-4)
df_bars = df_ridges |> wtl::ridges2bars(y, density, width = 0.2)

df_wind |>
  modelr::add_predictions(glm_bernoulli, type = "response") |>
  ggplot() + aes(max_wind, bucket_sales) +
  geom_point(alpha = 0.3, shape = 124, size = 6) +
  ggridges::geom_vridgeline(aes(x, y, width = density * 6, group = x),
    data = df_bars, fill = "#56B4E9AA", linetype = 0) +
  geom_line(aes(y = pred), color = "#3366ff") +
  scale_y_continuous(breaks = c(0, 1)) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())
```

  </div>
</div>


---
## LM is a special case of GLM

- Probability distribution: **Normal distribution**
- Link function: **identity** (returns everything unchanged)

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

<p>\[\begin{split}
y_i &\sim \mathcal{N}(\mu_i,~\sigma^2) \\
\text{identity}(\mu_i) &= \beta_0 + \beta_1 x_i
\end{split}\]</p>

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r, df-weight}
#| include: false
#| cache.vars: "df_weight"
set.seed(19937)
n = 50
df_weight = tibble::tibble(
  height = rnorm(n, 1.70, 0.05),
  bmi = rnorm(n, 22, 1),
  weight = bmi * (height**2)
) |>
  print()
```
```{r, glm-weight}
#| echo: false
#| cache.vars: []
#| fig.height: 4
#| fig.width: 4
fit = lm(weight ~ height, df_weight)
coef = coef(fit)

x_breaks = c(1.65, 1.7, 1.75)
df_ridges = tidyr::crossing(height = x_breaks, weight = seq(50, 80, 0.2)) |>
  dplyr::mutate(density = dnorm(weight, coef[1] + coef[2] * height, 1.8)) |>
  dplyr::filter(density > 1e-4)

ggplot(df_weight) + aes(height, weight) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  ggridges::geom_vridgeline(aes(width = density * 0.08, group = height),
    data = df_ridges, fill = "#56B4E9AA", linetype = 0) +
  stat_smooth(method = lm, formula = y ~ x, se = FALSE) +
  scale_x_continuous(breaks = x_breaks) +
  theme_bw(base_size = 20) + theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())
```

  </div>
</div>

resulting in the same regression line as OLS.


---
## <u>An</u>alysis <u>o</u>f <u>va</u>riance (ANOVA) as GLM

with **qualitative** explanatory variables, **normal** distribution, **identity** link.<br>
needs conversion to <span title="a.k.a dummy variables">**index variables**</span> (0 or 1).

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

| weather | â†’ | $x_1$ â˜€ï¸ æ™´ | $x_2$ â˜”ï¸ é›¨ |
| ------- | :-: | :---: | :---: |
| â˜ï¸ cloudy | | 0 | 0 |
| â˜€ï¸ sunny | | 1 | 0 |
| â˜”ï¸ rainy | | 0 | 1 |

<p>\[\begin{split}
y_i &\sim \mathcal{N}(\mu_i,\sigma^2) \\
\mu_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}
\end{split}\]</p>

  </div>
  <div class="column" style="flex-shrink: 1.3;">

```{r, df-ancova}
#| include: false
#| cache.vars: ["df_ancova", "weather_levels"]
set.seed(19937)
n = 200L
b = c(70, 3, 20, -20)  # true coef
weather_levels = c("sunny", "cloudy", "rainy")
df_ancova = tibble::tibble(
    temperature = runif(n, 8, 32),
    weather = factor(sample(weather_levels, n, TRUE), levels = weather_levels)
  ) |>
  dplyr::mutate(name = weather, value = 1L) |>
  tidyr::pivot_wider(values_fill = 0L) |>
  dplyr::select(!cloudy) |>
  dplyr::mutate(mu = b[1] + b[2] * temperature + b[3] * sunny + b[4] * rainy) |>
  dplyr::mutate(beer_sales = rnorm(n, mu, 10)
) |>
  print()
```

```{r, glm-anova}
#| echo: false
#| cache.vars: []
#| fig.height: 4.5
#| fig.width: 4.5
lm_anova = lm(beer_sales ~ weather, df_ancova)
df_ridges = tidyr::crossing(weather = factor(weather_levels, levels = weather_levels), beer_sales = seq(50, 200, 1)) |>
  modelr::add_predictions(lm_anova) |>
  dplyr::mutate(density = dnorm(beer_sales, pred, 10)) |>
  dplyr::filter(density > 1e-4)

tidy_anova = broom::tidy(lm_anova)

avgs = tidyr::crossing(weather = factor(weather_levels, levels = weather_levels)) |>
  modelr::add_predictions(lm_anova) |>
  tibble::deframe()

dfl = tibble::tribble(
  ~x, ~xend, ~y, ~yend,
  -Inf, Inf, avgs["cloudy"], avgs["cloudy"],
  1.5, 2.5, avgs["sunny"], avgs["sunny"],
  2.5, 3.5, avgs["rainy"], avgs["rainy"]
)

dfa = tibble::tribble(
  ~x, ~xend, ~y, ~yend,
  1.75, 1.75, avgs["cloudy"], avgs["sunny"],
  2.75, 2.75, avgs["cloudy"], avgs["rainy"]
)

dfs = tibble::tribble(
  ~x, ~y, ~label,
  0.6, avgs["cloudy"] + (avgs["sunny"] - avgs["cloudy"]) * 0.3, "beta[0]",
  1.55, (avgs["cloudy"] + avgs["sunny"]) / 2, "beta[1]",
  2.55, (avgs["cloudy"] + avgs["rainy"]) / 2, "beta[2]"
)

set.seed(1)
.arr = grid::arrow(length = grid::unit(0.1, "inches"))
df_ancova |>
  ggplot() + aes(weather, beer_sales, color = weather) +
  ggridges::geom_vridgeline(aes(width = density * 6, group = weather),
    data = df_ridges, fill = "#56B4E9AA", linetype = 0) +
  annotate("segment", x = dfl$x, xend = dfl$xend, y = dfl$y, yend = dfl$yend, color = "#56B4E9AA") +
  annotate("segment", x = dfa$x, xend = dfa$xend, y = dfa$y, yend = dfa$yend, arrow = .arr, color = "#56B4E9AA") +
  annotate("text", x = dfs$x, y = dfs$y, label = dfs$label, parse = TRUE, size = 6, color = "#56B4E9AA") +
  geom_jitter(width = 0.08, height = 0, alpha = 0.66, shape = 16, size = 3) +
  scale_color_viridis_d(direction = -1, guide = guide_legend(title = NULL)) +
  scale_x_discrete(limits = c("cloudy", "sunny", "rainy")) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank(),
        legend.position = "none")
```

  </div>
</div>

The effects of sunnyâ˜€ï¸ $\beta_1$ and rainyâ˜”ï¸ $\beta_2$ are relative to cloudyâ˜ï¸ $\beta_0$.

GLM supports more flexible modelling with other distribution & link.


---
## <u>An</u>alysis of <u>cova</u>riance (ANCOVA) as GLM

with **both qualitative and quantitative** explanatory variables<br>
assuming **normal** distribution, **homogeneity of variance**, **identity** link.


<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

| weather | â†’ | $x_1$ â˜€ï¸ æ™´ | $x_2$ â˜”ï¸ é›¨ |
| ------- | :-: | :---: | :---: |
| â˜ï¸ cloudy | | 0 | 0 |
| â˜€ï¸ sunny | | 1 | 0 |
| â˜”ï¸ rainy | | 0 | 1 |

<p>\[\begin{split}
y_i &\sim \mathcal{N}(\mu_i,\sigma^2) \\
\mu_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i}
\end{split}\]</p>

  </div>
  <div class="column" style="flex-shrink: 1.3;">


```{r, glm-ancova}
#| echo: false
#| cache.vars: []
#| fig.height: 4.5
#| fig.width: 4.5
lm_ancova = lm(beer_sales ~ temperature + weather, df_ancova)
df_pred = tidyr::crossing(temperature = seq(8, 32, 2), weather = factor(weather_levels, levels = weather_levels)) |>
  modelr::add_predictions(lm_ancova, type = "response")

ggplot(df_ancova) + aes(temperature, beer_sales, color = weather) +
  geom_line(data = df_pred, aes(y = pred, group = weather), alpha = 0.7, linewidth = 2) +
  geom_point(alpha = 0.6, shape = 16, size = 3) +
  scale_color_viridis_d(direction = -1, guide = guide_legend(title = NULL)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))
```

  </div>
</div>

GLM supports more flexible modelling with other distribution & link.


---
## Interaction

The effects of two or more variables are not additive.<br>
e.g., The temperature dependency of beer sales differs by weather.

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 0.1rem;">

| weather | $x_1$ |
| ------- | :---: |
| â˜€ï¸ sunny | 1 |
| â˜”ï¸ rainy | 0 |

<p>\[\begin{split}
y_i &\sim \mathcal{N}(\mu_i,\sigma^2) \\
\mu_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_{1,2} x_{1i} x_{2i}
\end{split}\]</p>

â˜€ï¸ all $\beta$<br>
â˜”ï¸ only $\beta_0,~\beta_2$ because $x_{1i} = 0$.

  </div>
  <div class="column" style="flex-shrink: 1.3;">

```{r, df-interact}
#| include: false
#| cache.vars: ["df_interact", "weather_levels"]
set.seed(19937)
n = 200L
b = c(70, 3, 100, -2)  # true coef
weather_levels = c("sunny", "rainy")
df_interact = tibble::tibble(
    temperature = runif(n, 8, 32),
    weather = factor(sample(weather_levels, n, TRUE), levels = weather_levels)
  ) |>
  dplyr::mutate(name = weather, value = 1L) |>
  tidyr::pivot_wider(values_fill = 0L) |>
  dplyr::mutate(mu = b[1] * sunny + b[2] * temperature + b[3] * rainy + b[4] * temperature * rainy) |>
  dplyr::mutate(beer_sales = rnorm(n, mu, 10)) |>
  print()
```
```{r, interaction}
#| echo: false
#| cache.vars: []
#| fig.height: 4.5
#| fig.width: 4.5
lm_int = lm(beer_sales ~ temperature * weather, df_interact)
df_pred = tidyr::crossing(temperature = seq(8, 32, 2), weather = factor(weather_levels, levels = weather_levels)) |>
  modelr::add_predictions(lm_int, type = "response")

ggplot(df_interact) + aes(temperature, beer_sales, color = weather) +
  geom_line(data = df_pred, aes(y = pred, group = weather), alpha = 0.7, linewidth = 2) +
  geom_point(alpha = 0.6, shape = 16, size = 3) +
  scale_color_viridis_d(direction = -1, guide = guide_legend(title = NULL)) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))
```

  </div>
</div>

Drawback: it makes interpretation difficult dramatically.



---
## Roundup: Generalized Linear Model (GLM)

supports flexible modelling with various distribution & link.<br>
There are some named (frequently used) combinations.

| Name | Distribution | Link | expl. variables |
| ---- | ------------ | ---- | --------------- |
|Poisson regression|Poisson|log| |
|Logistic regression|Binomial|logit| |
|LM    |Normal|identity| |
|ANOVA |Normal|identity|qualitative|
|ANCOVA|Normal|identity|qualitative+quantitative|


---
## Link functions

çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ãŠã„ã¦ã€Œã¾ã£ã™ãä»¥å¤–ã‚‚è¡¨ç¾ã§ãã‚‹ã€æ„å‘³

$\text{identity}(\mu_i)$
: $\mu_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots$
: The effects of expl. variables are **additive**.

$\log(\lambda_i)$
: $\lambda_i = e^{\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots} = e^{\beta_0} \times e^{\beta_1 x_{1i}} \times e^{\beta_2 x_{2i}} \times \ldots$
: The effects of expl. variables are **multiplicative**.<br>
  e.g., adding $\Delta x_1$ brings multiplying $e^{\beta_1 \Delta x_{1}}$.

$\text{logit}(p_i)$
: $p_i = \frac 1 {1 + e^{-(\beta_0 + \beta_1 x_i + \ldots)}} $ (logistic function)
: The effects of expl. variables are **plateaued**.<br>
  e.g., $\lim_{x \to -\infty} p = 0;~\lim_{x \to \infty} p = 1$

`probit`, `inverse`, `sqrt`, etc. are not covered here.


---
## GLM in R

Almost the same way as with `lm`:

```{r, glm}
formula = weight ~ height
fit = glm(formula, data = df_weight)
coef(fit)
```

Normal distribution and identity link is selected by default.<br>
Use `family=` option for modification:
```r
glm(formula, family = gaussian(link = identity), data = mydata)
glm(formula, family = poisson(link = log), data = mydata)
glm(formula, family = binomial(link = logit), data = mydata)
```

See [`?family`](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/family.html) for more details.


---
## ğŸ”° Practice of `glm()`

Express weight as a linear function of height.

```{r, generate-df-weight}
#| ref.label: "df-weight"
#| echo: -1
#| cache.vars: []
```

Don't care about interpretation and goodness of fit for now.

---
## ğŸ”° Practice of `glm()`, example solution

```{r, glm-df-weight}
#| fig.width: 4
#| fig.height: 4
fit_wh = glm(weight ~ height, family = gaussian(link = identity), data = df_weight)
coef(fit_wh)
df_fit_wh = modelr::add_predictions(df_weight, fit_wh, type = "response")
ggplot(df_fit_wh) +
  aes(height, weight) +
  geom_point() +
  geom_line(aes(y = pred), linewidth = 1, color = "#3366ff")
```

---
## ğŸ”° Poisson regression ğŸŒ±

```{r, generate-df-seeds}
#| ref.label: "df-seeds"
#| echo: -1
#| cache.vars: []
```

---
## ğŸ”° Multiple regression ğŸº

It requires minor tricks with `add_predictions()` to draw regression lines.<br>
Compromise with "regression points" for now.

```{r, generate-df-beer}
#| ref.label: "df-beer"
#| echo: -1
#| cache.vars: []
```

---
## ğŸ”° Logistic regression (hint on the next page â†’)

```{r, generate-df-logistic}
#| ref.label: "df-logistic"
#| echo: -1
#| cache.vars: []
```

---
## Hint for Logistic regression

Two types of response variables can be given to a formula:
- Integer vector: success 1, failure 0 (logistic regression in narrow sense)
- Integer matrix: # of successes in 1st column, # of failures in 2nd column

i.e., giving only # of success (`y`) results in an error:
```{r, logistic-error}
#| error: true
glm(y ~ x, df_logistic, family = binomial)
```
instead, the formula should be `response ~ x` to include failures as well.

(It also means the number of trials does not have to be constant.)


---
## ğŸ”° ANCOVA: GLM with qualitative + quantitative vars

First, try ANOVA with only weather, then ANCOVA with temperature.

```{r, generate-df-ancova}
#| ref.label: "df-ancova"
#| echo: -1
#| cache.vars: []
```

---
## ğŸ”° Interaction


```{r, generate-df-interact}
#| ref.label: "df-interact"
#| echo: -1
#| cache.vars: []
```

---
## One data, many possible models

How can we choose?

1. Based on underlying mechanisms
    - Poisson if **count** data, Gamma if **interval** data, from Poisson process.
    - Binomial if **proportional count** like "k in n times".
1. Visualize the data, and choose one with similar shape.
    - Normal distribution if **symmetric**
    - Gamma if **always positive**
    - straight, exponential, plateaued, etc.

There should be some measures for goodness-of-fit...


---
## Likelihood: a measure for goodness-of-fit

The probability to observe the data $D$ given the model $M$.<br>
$\text{Prob}(D \mid M)$

**Likelihood function** is the same probability from different viewpoints:

- as a function of model $M$ given the data $D$,<br>
  $L(M \mid D)$<br>
- as a function of parameters $\theta$,<br>
  $L(\theta \mid D)$ or $L(\theta)$

<hr>

Compare maximum likelihood of competing models:<br>
$\log L^* (M_1) \text{ vs. } \log L^* (M_2) \text{ vs. } \log L^* (M_3) \ldots$

```{r, llf}
broom::glance(fit)
```

---
## The better fit, the higher likelihood

OK, makes sense in this case:

```{r, compare-loglik}
#| echo: false
#| fig.height: 5
#| fig.width: 9
models = setNames(nm = c("gaussian", "poisson")) |> purrr::map(~{
  glm(num_seeds ~ body_mass, family = .x, data = df_seeds)
})

x_breaks = c(0.5, 1.0, 1.5)
df_lm = tidyr::crossing(body_mass = x_breaks, num_seeds = seq(-5, 20, 0.1)) |>
  modelr::add_predictions(models[["gaussian"]], type = "response") |>
  dplyr::mutate(density = dnorm(num_seeds, pred, 1.4)) |>
  dplyr::filter(density > 1e-4)

p_pois = ggplot(df_seeds) + aes(body_mass, num_seeds) +
  ggridges::geom_vridgeline(data = df_lm, aes(width = density * 0.4, group = body_mass), linetype = 0, alpha = 0) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  scale_x_continuous(breaks = x_breaks) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())

label = sprintf("logLik = %.1f", broom::glance(models[["gaussian"]])$logLik)
p_lm = p_pois +
  labs(title = "gaussian, identity link") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = label, color = "#3366ff", size = 8) +
  stat_smooth(formula = y ~ x, method = lm, se = FALSE) +
  ggridges::geom_vridgeline(aes(width = density * 0.4, group = body_mass),
    data = df_lm, fill = "#56B4E9AA", linetype = 0)
# p_lm

df_ridges = tidyr::crossing(body_mass = x_breaks, num_seeds = seq_len(30L) - 1L) |>
  modelr::add_predictions(models[["poisson"]], type = "response") |>
  dplyr::mutate(density = dpois(num_seeds, pred)) |>
  dplyr::filter(density > 1e-4)
df_bars = df_ridges |> wtl::ridges2bars(num_seeds, density)

label = sprintf("logLik = %.1f", broom::glance(models[["poisson"]])$logLik)
p_poisson = p_pois +
  labs(title = "poisson, log link") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = label, color = "#3366ff", size = 8) +
  stat_smooth(formula = y ~ x, method = glm, method.args = list(family = poisson), se = FALSE) +
  ggridges::geom_vridgeline(aes(width = density * 0.5, group = body_mass),
    data = df_bars, fill = "#56B4E9AA", linetype = 0)
# p_poisson

cowplot::plot_grid(p_lm, p_poisson, nrow = 1L)
```

Should we continue searching for higher likelihood?

---
## ã‚ã¦ã¯ã¾ã‚ŠãŒè‰¯ã‘ã‚Œã°ã„ã„ã£ã¦ã‚‚ã‚“ã§ã‚‚ãªã„

Overfitting
: Too many parameters leads to fitting too closely **to the dataset**.<br>
  â†’ poor generalization on unseen data<br>
  â†’ useless for prediction and understanding

```{r, saturated-model}
#| echo: false
#| fig.height: 4
#| fig.width: 11
#| cache.vars: []
set.seed(19937)
n = 16L
true_coef = c(0.1, 0.2)
df_plant = tibble::tibble(
  x = runif(n, 7, 12.5),
  lambda = exp(true_coef[1] + true_coef[2] * x),
  y = rpois(n, lambda)
) |> tibble::rownames_to_column("id")

models = df_plant |> modelr::fit_with(glm, family = "poisson", modelr::formulas(~y,
  null = ~ 1,
  x = ~ x,
  saturated = ~ id
))
labels = setNames(sprintf("logLik = %.1f", purrr::map_dbl(models, logLik)), names(models))

p_plant = df_plant |>
  modelr::add_predictions(models$null, type = "response") |>
  ggplot() + aes(x, y) +
  geom_line(aes(y = pred), color = "#3366ff", linewidth = 2, alpha = 0.7) +
  geom_point(shape = 16, alpha = 0.6) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")

p_null = p_plant +
  labs(title = "null model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2,
    label = labels[["null"]], color = "#3366ff", size = 6)
p_x = p_plant %+% modelr::add_predictions(df_plant, models$x, type = "response") +
  labs(title = expression(y %~% beta[0] + beta[1] * x)) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2,
    label = labels[["x"]], color = "#3366ff", size = 6)
p_saturated = p_plant %+% modelr::add_predictions(df_plant, models$saturated, type = "response") +
  labs(title = "saturated model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2,
    label = labels[["saturated"]], color = "#3366ff", size = 6)

cowplot::plot_grid(p_null, p_x, p_saturated, nrow = 1L)
```

**null model**: only intersect; without explanatory variable.<br>
**saturated model**: # data points â‰¤ # parameters; just like connecting points.


---
## Higher likelihood by adding a useless parameter

The number of seeds $y$ increases with the increasing body mass $x$.<br>
Adding a variable $x_2$, sox color of the observer, improves likelihood......?

```{r, many-models}
#| echo: false
#| fig.height: 7
#| fig.width: 7
#| cache.vars: ["df_plant", "models", "p_plant"]
set.seed(24601)
n = 120L
true_coef = c(1, 0.12, 0)
df_plant = tibble::tibble(
  x = runif(n, 7, 12.5),
  x2 = sample(c(FALSE, TRUE), n, replace = TRUE),
  lambda = exp(true_coef[1] + true_coef[2] * x + true_coef[3] * x2),
  y = rpois(n, lambda)
) |> tibble::rownames_to_column("id")

models = df_plant |> modelr::fit_with(glm, family = "poisson", modelr::formulas(~y,
  null = ~ 1,
  x = ~ x,
  x2 = ~ x2,
  both = ~ x + x2,
  saturated = ~ id
))
labels = setNames(sprintf("logLik = %.1f", purrr::map_dbl(models, logLik)), names(models))

p_plant = df_plant |>
  modelr::add_predictions(models$null, type = "response") |>
  ggplot() + aes(x, y) +
  geom_line(aes(y = pred), linewidth = 1.5, alpha = 0.6) +
  geom_point(shape = 16, alpha = 0.6) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")

p_null = p_plant +
  labs(title = "null model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["null"]], color = "#3366ff", size = 6)
p_x = p_plant %+% (df_plant |> modelr::add_predictions(models$x, type = "response")) +
  labs(title = expression(y %~% beta[0] + beta[1] * x)) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x"]], color = "#3366ff", size = 6)
p_x2 = p_plant %+% (df_plant |> modelr::add_predictions(models$x2, type = "response")) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x2"]], color = "#3366ff", size = 6)
p_both = p_plant %+% (df_plant |> modelr::add_predictions(models$both, type = "response")) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[1] * x + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["both"]], color = "#3366ff", size = 6)

cowplot::plot_grid(p_null, p_x, p_x2, p_both, nrow = 2L)
```



---
## AIC: Akaike Information Criterion

<p>\[\begin{split}
\text{AIC} = -2 (\log L^* - k) = -2 \log L^* + 2k
\end{split}\]</p>

- **The model with the minimum AIC is preferred**
    - higher likelihood $L$.
    - smaller number of parameters $k$
- Goodness of fit to which data?
    - $\log L^*$ is for the given dataset.
    - $(\log L^* - k)$ is the unbiased estimate of mean log likelihood to the future data generated by true mechanism.
      (derived from Kullback--Leibler divergence)


```{r, print-llf}
#| ref.label: "llf"
```

???
https://www.slideshare.net/logics-of-blue/1-6aic


---
## Higher AIC by adding a useless parameter

The number of seeds $y$ increases with the increasing body mass $x$.<br>
Adding a variable $x_2$, sox color of the observer, brings larger AIC.

```{r, many-models-aic}
#| echo: false
#| fig.height: 7
#| fig.width: 7
labels = setNames(sprintf("AIC = %.1f", purrr::map_dbl(models, AIC)), names(models))
p_null = p_plant +
  labs(title = "null model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["null"]], color = "#3366ff", size = 6)
p_x = p_plant %+% (df_plant |> modelr::add_predictions(models$x, type = "response")) +
  labs(title = expression(y %~% beta[0] + beta[1] * x)) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x"]], color = "#3366ff", size = 6)
p_x2 = p_plant %+% (df_plant |> modelr::add_predictions(models$x2, type = "response")) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x2"]], color = "#3366ff", size = 6)
p_both = p_plant %+% (df_plant |> modelr::add_predictions(models$both, type = "response")) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[1] * x + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["both"]], color = "#3366ff", size = 6)
cowplot::plot_grid(p_null, p_x, p_x2, p_both, nrow = 2L)
```

---
## Other information criteria

- $\text{BIC} = -2 \log L^* + k \log n$
    - Similar to AIC: penalizes for parameters $k$.
    - Different from AIC: depends on sample size $n$.<br>
      (AIC lacks penalty by sample size.)
    - (derived from maximization of marginal likelihood)
- [WAIC and WBIC](http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/waicwbic_e.html)
    - Widely Applicable variants of AIC and BIC.
    - WAIC for better prediction. WBIC for identifying the true model.


---
## Model selection by information criteria

does not select the "right" one;<br>
selects useful one for prediction and understanding.

> All models are wrong, but some are useful. --- George E. P. Box

<figure>
<img src="../tokiomarine2021/math-model.drawio.svg" width="900"><br>
<figcaption><cite>ã€Œãƒ‡ãƒ¼ã‚¿åˆ†æã®ãŸã‚ã®æ•°ç†ãƒ¢ãƒ‡ãƒ«å…¥é–€ã€æ±Ÿå´è²´è£• 2020 ã‚ˆã‚Šæ”¹å¤‰</cite></figcaption>
</figure>


---
## Pitfalls and notes

- Multicollinearity:
    - Correlated explanatory variables cause problems.
- Variable transformation should be avoided:
    - Log transformation is often useful, though.
    - Divided numbers are dangerous.
- Interaction makes interpretation difficult dramatically.


---
## ä¸€èˆ¬åŒ–ç·šå½¢ãƒ¢ãƒ‡ãƒ«åº§å­¦ã¾ã¨ã‚

- ä½•ã¯ã¨ã‚‚ã‚ã‚Œæ•£å¸ƒå›³ã‚’æã
- é©åˆ‡ãªç¢ºç‡åˆ†å¸ƒãƒ»ãƒªãƒ³ã‚¯é–¢æ•°ãƒ»èª¬æ˜å¤‰æ•°ã‚’è€ƒãˆã‚‹
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€å°¤æ¨å®šã™ã‚‹
- å°¤åº¦ã¯ã€Œæ‰‹å…ƒã®ãƒ‡ãƒ¼ã‚¿ã¸ã®ã‚ã¦ã¯ã¾ã‚Šã€
- ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã™ã‚‹ã¨ãã¯æƒ…å ±é‡åŸºæº–ã‚’å‚è€ƒã«ã™ã‚‹



---
## penguins dataset

<a href="https://allisonhorst.github.io/palmerpenguins/">
<cite>https://allisonhorst.github.io/palmerpenguins/</cite><br>
<img src="/slides/image/rstats/lter_penguins.png" width="45%">
<img src="/slides/image/rstats/culmen_depth.png" width="45%">
</a>

```r
install.packages("palmerpenguins")
library(palmerpenguins)
penguins_colors = c(Adelie = "darkorange", Chinstrap = "purple", Gentoo = "cyan4")
print(penguins)
```
```{r, penguins}
#| include: false
withr::local_package("palmerpenguins")
penguins_colors = c(Adelie = "darkorange", Chinstrap = "purple", Gentoo = "cyan4")
```

---
## penguins dataset

<a href="https://allisonhorst.github.io/palmerpenguins/">
<cite>https://allisonhorst.github.io/palmerpenguins/</cite><br>
<img src="/slides/image/rstats/lter_penguins.png" width="45%">
<img src="/slides/image/rstats/culmen_depth.png" width="45%">
</a>

```{r, penguins-print}
#| echo: false
print(penguins)
```

---
## Remove rows with missing values first

æ€§åˆ¥ã¯ã¨ã‚Šã‚ãˆãšä½¿ã‚ãªã„ã®ã§ã€ä½“é•·é–¢é€£ã ã‘ã§ã‚‚ã€‚

```{r, penguins-dropna}
penguins |> dplyr::filter(dplyr::if_any(everything(), is.na))
penguins_dropna = penguins |> tidyr::drop_na(body_mass_g)
dim(penguins_dropna)
```

---
## ğŸ”° Practice GLM with penguins

æ¬¡ã®èª²é¡Œã‚’è§£ã„ã¦ã¿ã‚ˆã†ã€‚<br>
(æ¬¡ãƒšãƒ¼ã‚¸ä»¥é™ã«è§£ç­”ã€‚ã¾ãšã¯è‡ªåŠ›ã§ã€‚)

1. `body_mass_g` ã‚’æ¨ªè»¸ã€ `flipper_length_mm` ã‚’ç¸¦è»¸ã«ã€ã¾ãšä½œå›³ã€‚
1. å˜å›å¸°ã—ã¦ã€åˆ‡ç‰‡ã¨å‚¾ãã‚’æ±‚ã‚ã‚‹ã€‚ãã—ã¦ä½œå›³ã€‚
1. `species` ã§è‰²åˆ†ã‘ã—ã¦ä½œå›³ã€‚
1. `species` ã‚‚èª¬æ˜å¤‰æ•°ã«åŠ ãˆã¦é‡å›å¸°ã—ã€åˆ‡ç‰‡ã¨å‚¾ãã‚’æ±‚ã‚ã‚‹ã€‚ãã—ã¦ä½œå›³ã€‚
1. ä½™è£•ãŒã‚ã‚Œã°ã€ã‚¯ãƒãƒã‚·ã®é•·ã•ã¨æ·±ã•ã‚’ç¸¦æ¨ªè»¸ã«ã—ã¦åŒæ§˜ã®è§£æã€‚


---
## Simple regression: 1. Plot data

ã©ã†ã‚„ã‚‰ã€é‡ã„ãƒšãƒ³ã‚®ãƒ³ã»ã©ç¿¼é•·ã‚‚é•·ã„ã€‚

```{r, penguins-weight}
#| fig.height: 4.5
#| fig.width: 4.5
p_penweight = ggplot(penguins_dropna) +
  aes(body_mass_g, flipper_length_mm) +
  geom_point(shape = 16, alpha = 0.66) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())
p_penweight
```


---
## Simple regression: 2. Fit

ã¨ã‚Šã‚ãˆãšãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ­£è¦åˆ†å¸ƒãƒ»æ’ç­‰ãƒªãƒ³ã‚¯ã€‚
$y = 136.7 + 0.0153 x$

```{r, penguins-fit1}
fit1 = glm(flipper_length_mm ~ body_mass_g, data = penguins_dropna)
broom::tidy(fit1)
broom::glance(fit1)
```

---
## Simple regression: 3. Plot the fitting result

çµæœã¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰äºˆæ¸¬å€¤ã‚’ä½œã£ã¦å›å¸°ç·šã‚’å¼•ãã€‚

```{r, penguins-weight-glm}
#| fig.height: 5
#| fig.width: 5
added1 = modelr::add_predictions(penguins_dropna, fit1, type = "response")
p1 = p_penweight +
  geom_line(aes(y = pred), data = added1, linewidth = 1, color = "#3366ff")
p1
```

---
## Multiple regression: 1. Plot data

ç¨®ã«ã‚ˆã£ã¦è‰²åˆ†ã‘ã—ã¦ã¿ã‚‹ã¨ã€å‚¾å‘ã®é•ã„ãŒè¦‹ãˆã‚‹ã€‚

```{r, penguins-weight-sp}
#| fig.height: 5
#| fig.width: 7
p_penweight_color = p_penweight + aes(color = species) +
  scale_color_manual(values = penguins_colors)
p_penweight_color
```


---
## Multiple regression: 2. Fit

Adelieã‚’åŸºæº–ã«ã€Chinstrapã¨Gentooã¯ãã‚Œã‚ˆã‚Šé•·ã‚ã€‚<br>
ä½“é‡ã®åŠ¹æœã¯å˜å›å¸°ã®ã¨ã(0.0153)ã‚ˆã‚Šå°ã•ã„ã€‚

```{r, penguins-fit2}
fit2 = glm(flipper_length_mm ~ body_mass_g + species, data = penguins_dropna)
broom::tidy(fit2)
broom::glance(fit2)
```

---
## Multiple regression: 3. Plot the fitting result

```{r, penguins-weight-sp-glm}
#| fig.height: 5
#| fig.width: 7
added2 = modelr::add_predictions(penguins_dropna, fit2, type = "response")
p2 = p_penweight_color +
  geom_line(aes(y = pred), data = added2, linewidth = 1)
p2
```

**å‚¾ã**ã‚‚ç¨®ã«ã‚ˆã£ã¦é•ã†ã‹ã‚‚ã€‚**äº¤äº’ä½œç”¨**ã‚’å…¥ã‚Œã¦ã¿ãŸã„ã€‚


---
## Interaction: Fit

Adelieã‚’åŸºæº–ã«ã€Chinstrapã®å‚¾ããŒçµæ§‹é•ã†ã€‚<br>
åˆ‡ç‰‡ã®é•ã„ã¯è§£é‡ˆã—ã«ãããªã£ãŸã€‚

```{r, penguins-fit3}
fit3 = glm(flipper_length_mm ~ body_mass_g * species, data = penguins_dropna)
broom::tidy(fit3)
broom::glance(fit3)
```

---
## Interaction: Plot the fitting result

```{r, penguins-interaction}
#| fig.height: 5
#| fig.width: 7
added3 = modelr::add_predictions(penguins_dropna, fit3, type = "response")
p3 = p_penweight_color +
  geom_line(aes(y = pred), data = added3, linewidth = 1)
p3
```

---
## Which is the best of three?

AICã§é¸ã¶ãªã‚‰äº¤äº’ä½œç”¨å…¥ã‚Šé‡å›å¸°ãŒè‰¯ã•ãã†ã€‚

```{r, penguins-aic}
#| fig.height: 4
#| fig.width: 11
labels = sprintf("AIC = %.1f", AIC(fit1, fit2, fit3)$AIC)
cowplot::plot_grid(p1 + labs(title = labels[1]),
                   p2 + labs(title = labels[2]) + theme(legend.position = "none"),
                   p3 + labs(title = labels[3]) + theme(legend.position = "none"), nrow = 1L)
```


---
## ä½™è£•ãŒã‚ã£ãŸã‚‰è¿½åŠ ã®ç·´ç¿’

ğŸ”°ã‚¯ãƒãƒã‚·ã®é•·ã•ã¨æ·±ã•ã§åŒã˜è§£æã‚’ã‚„ã£ã¦ã¿ã‚ˆã†ã€‚

```{r, penguins-bill}
#| echo: false
#| fig.height: 4
#| fig.width: 11
#| cache.vars: [p_bill]
p_bill = penguins_dropna |>
  ggplot() + aes(bill_length_mm, bill_depth_mm) +
  geom_point(shape = 16, alpha = 0.66) +
  scale_color_manual(values = penguins_colors) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())

fit1 = glm(bill_depth_mm ~ bill_length_mm, data = penguins_dropna)
fit2 = glm(bill_depth_mm ~ bill_length_mm + species, data = penguins_dropna)
fit3 = glm(bill_depth_mm ~ bill_length_mm + species + bill_length_mm:species, data = penguins_dropna)
d1 = modelr::add_predictions(penguins_dropna, fit1, type = "response")
d2 = modelr::add_predictions(penguins_dropna, fit2, type = "response")
d3 = modelr::add_predictions(penguins_dropna, fit3, type = "response")
p1 = p_bill + geom_line(aes(y = pred), d1, linewidth = 1, color = "#3366ff")
p2 = p_bill + aes(color = species) + geom_line(aes(y = pred), d2, linewidth = 1)
p3 = p_bill + aes(color = species) + geom_line(aes(y = pred), d3, linewidth = 1)
labels = sprintf("AIC = %.1f", AIC(fit1, fit2, fit3)$AIC)
cowplot::plot_grid(p1 + labs(title = labels[1]),
                   p2 + labs(title = labels[2]) + theme(legend.position = "none"),
                   p3 + labs(title = labels[3]) + theme(legend.position = "none"), nrow = 1L)
```

```{r, penguins-multiple}
#| echo: false
#| include: false
fit4 = glm(bill_depth_mm ~ bill_length_mm + sex, data = penguins)
broom::tidy(fit4)
broom::glance(fit4)
added4 = modelr::add_predictions(penguins, fit4)
p_bill + geom_line(aes(y = pred, color = sex), data = added4, linewidth = 1) +
  scale_color_discrete()
```


---
## ğŸ”° Challenge: GLM with fictional data

å®¿å±‹ã®ä¸»äººãŒå®¿æ³Šå®¢ã®å†’é™ºè€…ã«è©±ã‚’èã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ãŸã€‚<br>
å†’é™ºè€…ã®**ãƒ¬ãƒ™ãƒ«**ã€**è·æ¥­**ã€é€šã£ã¦ããŸ**ãƒ«ãƒ¼ãƒˆ**ã€é­”ç‰©ã¨ã®**é­é‡å›æ•°**ã€‚<br>
ã“ã‚Œã‚‰ã®å¤‰æ•°ã®é–“ã«ã€ãªã‚“ã‚‰ã‹ã®é–¢ä¿‚ã¯è¦‹ã‚‰ã‚Œã‚‹ã ã‚ã†ã‹ï¼Ÿ

[adventure.tsv](adventure.tsv)

```{r, adventure-data}
#| echo: false
set.seed(19937)
samplesize = 1000L
b0 = 0.7
b1 = 0.01
b2 = -0.5
b3 = 0.02
b12 = 0.03

dfraw = tibble::tibble(
  level = sample.int(50L, samplesize, replace = TRUE),
  is_mage = rbinom(samplesize, 1, 0.4),
  via_cave = rbinom(samplesize, 1, ifelse(is_mage, 0.5, 0.3)),
  lambda = exp(b0 + b1 * level + b2 * is_mage + b3 * via_cave + b12 * level * is_mage),
  encounter = rpois(samplesize, lambda)
)

dfout = dfraw |> dplyr::mutate(
    job = ifelse(is_mage, "mage", "fighter"),
    route = ifelse(via_cave, "cave", "field"),
    .before = encounter
  ) |>
  dplyr::select(!lambda & !is_mage & !via_cave)

readr::write_tsv(dfout, "../adventure.tsv") |> print()
```

```{r, adventure-analysis}
#| include: false
fit = glm(encounter ~ level + job + route + level:job, data = dfout, family = poisson)
summary(fit)
df_pred = modelr::add_predictions(dfout, fit, type = "response")

p_enc = ggplot(df_pred) +
  aes(level, encounter, color = job) +
  geom_point() +
  geom_line(aes(y = pred)) +
  facet_wrap(vars(route))

p_route = ggplot(df_pred) +
  aes(job, fill = job) +
  geom_bar() +
  facet_wrap(vars(route))
```

---
## ğŸ”° Final challenge: Analyze public datasets

- [e-Stat](https://www.e-stat.go.jp/): æ”¿åºœçµ±è¨ˆã®ç·åˆçª“å£
- [data.go.jp ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ã‚µã‚¤ãƒˆ](https://www.data.go.jp/data/dataset?res_format=CSV): ä¸­å¤®çœåº
- [BODIKã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ã‚µã‚¤ãƒˆ](https://odcs.bodik.jp/): åœ°æ–¹è‡ªæ²»ä½“
- [æ°—è±¡åº](https://www.data.jma.go.jp/gmd/risk/obsdl/index.php)
- [DATA.GOV](https://www.data.gov/): U.S. Governmentâ€™s open data
- ã»ã‹ã€ãªã‚“ã§ã‚‚èˆˆå‘³ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿

ç™ºè¡¨ãƒ»ãƒ¬ãƒãƒ¼ãƒˆã®æ¡ä»¶
: æœ€ä½1æšã®å›³ã¨ã€ãã“ã«è‡³ã‚‹å‰å‡¦ç†ï¼‹å¯è¦–åŒ–ã®ã‚³ãƒ¼ãƒ‰ã€‚
: ã‚°ãƒ©ãƒ•ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ã“ã¨ã‚’ä¸€è¨€ã€‚
: GLMã§å¤‰æ•°é–“ã®é–¢ä¿‚ã‚’è€ƒå¯Ÿã§ãã‚‹ã¨ãªãŠã‚ˆã—ã€‚

é€±æ˜ã‘ã«ç­å†…ã§è©±ã—åˆã£ã¦ãƒ™ã‚¹ãƒˆ2ã‚’é¸ã³ã€å…¨ä½“ã«å‘ã‘ã¦ç™ºè¡¨ã€‚


---
## I guess we don't have time to go further

---
## nå€‹ã®ã†ã¡yå€‹ç”Ÿå­˜ã€‚äºŒé …åˆ†å¸ƒã«å¾“......ã‚ãªã„ï¼

æ¤ç‰©100å€‹ä½“ã‹ã‚‰8å€‹ãšã¤ç¨®å­ã‚’å–ã£ã¦æ¤ãˆãŸã‚‰å…¨ä½“ã§åŠåˆ†ã¡ã‚‡ã„ç™ºèŠ½ã€‚<br>
è¦ª1å€‹ä½“ã‚ãŸã‚Šã®ç”Ÿå­˜æ•°ã¯<span style="color: #56B4E9;">n=8ã®äºŒé …åˆ†å¸ƒ</span>ã«ãªã‚‹ã¯ãšã ã‘ã©ã€<br>
æ¥µç«¯ãªå€¤(å…¨éƒ¨æ­»äº¡ã€å…¨éƒ¨ç”Ÿå­˜)ãŒå¤šã‹ã£ãŸã€‚å€‹ä½“å·®ï¼Ÿ

```{r, df-seeds-od}
#| echo: false
set.seed(24601)
samplesize = 100L
df_seeds_od = tibble::tibble(
  z = rnorm(samplesize, 0.5, 3),
  p = wtl::sigmoid(z),
  y = rbinom(samplesize, 8L, p))
```
```{r, overdispersion}
#| echo: false
#| fig.height: 5
#| fig.width: 6
sum_y = sum(df_seeds_od$y)
p_hat = sum_y / 800
label = sprintf("hat(p) == %d/800 %%~~%% %.2f", sum_y, p_hat)
tidy_od = df_seeds_od |>
  dplyr::count(y, name = "observed") |>
  dplyr::mutate(expected = samplesize * dbinom(y, 8, p_hat))
ggplot(tidy_od) +
  geom_col(aes(y, observed), width = 0.4, fill = "#333333") +
  geom_col(aes(y, expected), tidy_od, fill = "#56B4E9", alpha = 0.5, width = 0.8) +
  annotate("text", label = label, parse = TRUE, color = "#56B4E9",
           x = -Inf, y = Inf, hjust = -0.1, vjust = 2, size = 6) +
  labs(x = "# survived seeds", y = "count") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(),
        legend.title = element_blank(), legend.position = "top")
```


---
## å€‹ä½“å·®ã‚’ãƒ¢ãƒ‡ãƒ«ã«çµ„ã¿è¾¼ã¿ãŸã„

å„å€‹ä½“ã®ç”Ÿå­˜ç‡$p_i$ã‚’ãã®ã¾ã¾ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã™ã‚‹ã¨**éå‰°é©åˆ**ã€‚<br>
ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° â‰¥ ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã€ã®â€œãƒ‡ãƒ¼ã‚¿èª­ã¿ä¸Šã’â€ãƒ¢ãƒ‡ãƒ«ã€‚<br>
i.e., ã“ã®å€‹ä½“ã¯4å€‹ç”Ÿãæ®‹ã£ã¦ç”Ÿå­˜ç‡0.5ã ã­ã€‚æ¬¡ã®å€‹ä½“ã¯2å€‹ä½“ã ã‹ã‚‰......

```{r, saturated-glmm}
#| echo: false
#| fig.height: 3
#| fig.width: 11
df_seeds_od |>
  tibble::rowid_to_column("id") |>
  ggplot() + aes(id, y / 8) +
  geom_col(width = 0.6, fill = "#56B4E9", alpha = 0.66) +
  labs(y = expression(italic(p[i]))) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(),
        legend.title = element_blank(), legend.position = "top")
```

å€‹ä½“ã®ç”Ÿå­˜èƒ½åŠ›ã‚’ã‚‚ã£ã¨å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è¡¨ç¾ã§ããªã„ã‹ï¼Ÿ


---
## å€‹ä½“å·®ã‚’ãƒ¢ãƒ‡ãƒ«ã«çµ„ã¿è¾¼ã¿ãŸã„

å„å€‹ä½“ã®ç”Ÿå­˜ç‡$p_i$ãŒèƒ½åŠ›å€¤$z_i$ã®ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§æ±ºã¾ã‚‹ã¨ä»®å®šã€‚<br>
ãã®èƒ½åŠ›å€¤ã¯å…¨å€‹ä½“å…±é€šã®æ­£è¦åˆ†å¸ƒã«å¾“ã†ã¨ä»®å®š:
$z_i \sim \mathcal{N}(\hat z, \sigma)$

```{r, sigmoid}
#| echo: false
#| fig.height: 4
#| fig.width: 11
z_hat = wtl::logit(p_hat)
p_sigmoid = df_seeds_od |>
  ggplot() + aes(z, p) +
  geom_line(linewidth = 2, alpha = 0.6, color = "#56B4E9") +
  annotate("line", x = c(-Inf, z_hat, z_hat), y = c(p_hat, p_hat, -Inf), color = "#56B4E9") +
  annotate("point", x = z_hat, y = p_hat, shape = 16, size = 3, color = "#56B4E9") +
  scale_x_continuous(limits = range(df_seeds_od$z), expand = c(0, 0)) +
  labs(x = expression(italic(z[i])), y = expression(italic(p[i]))) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())
p_normal = tibble::tibble(
    z = seq(min(df_seeds_od$z), max(df_seeds_od$z), length.out = 100),
    Density = dnorm(z, z_hat, 3)
  ) |>
  ggplot() + aes(z, Density) +
  geom_area(fill = "#56B4E9", alpha = 0.5) +
  geom_vline(xintercept = z_hat, color = "#56B4E9") +
  scale_x_continuous(limits = range(df_seeds_od$z), expand = c(0, 0)) +
  labs(x = expression(italic(z))) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())

cowplot::plot_grid(p_sigmoid, p_normal, nrow = 1L)
```

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿2ã¤ã§æ¸ˆã‚€: å¹³å‡ $\hat z$, ã°ã‚‰ã¤ã $\sigma$ ã€‚

å‰è€…ã¯æ¨™æœ¬å¹³å‡ $\hat p$ ã‹ã‚‰æ±‚ã¾ã‚‹ã¨ã—ã¦ã€å¾Œè€…ã©ã†ã™ã‚‹ï¼Ÿ

---
## å€‹ä½“èƒ½åŠ›ã®ã°ã‚‰ã¤ã $\sigma$ ãŒå¤§ãã„ã¨ä¸¡ç«¯ãŒå¢—ãˆã‚‹

æ™®é€šã®äºŒé …åˆ†å¸ƒã¯å€‹ä½“å·®ç„¡ã— $\sigma = 0$ ã‚’ä»®å®šã—ã¦ã‚‹ã®ã¨åŒã˜ã€‚

```{r, alter-sigma}
#| echo: false
#| fig.height: 3.5
#| fig.width: 11
sigma = c(0.5, 1.5, 3)
breaks = qnorm(c(0.2, 0.4, 0.6, 0.8), z_hat, 3)
.z = seq(min(df_seeds_od$z), max(df_seeds_od$z), 0.1)
df_z = tidyr::crossing(sigma, z = .z) |>
  dplyr::mutate(Density = dnorm(z, z_hat, sigma)) |>
  dplyr::mutate(sigma = ifelse(sigma > 0.1, sigma, 0)) |>
  dplyr::bind_rows(tibble::tibble(sigma = 0, z = .z, Density = ifelse(abs(z - z_hat) < 0.2, Inf, 0))) |>
  dplyr::mutate(Z = cut(z, c(-Inf, breaks, Inf)))

p_z = df_z |>
  ggplot() + aes(z, Density) +
  scale_x_continuous(limits = range(df_seeds_od$z), expand = c(0, 0)) +
  facet_wrap(vars(sigma), nrow = 1L, labeller = label_both) +
  labs(x = expression(italic(z))) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank(),
        legend.position = "none")

p_z + geom_col(aes(fill = sigma == 0), width = 0.1, alpha = 0.6)

df_unmixed = tibble::tibble(z = seq(min(df_seeds_od$z), max(df_seeds_od$z), length.out = 20)) |>
  dplyr::mutate(p = wtl::sigmoid(z)) |>
  purrr::pmap(function(z, p) {
    tibble::tibble(z, y = seq.int(0L, 8L), Density = dbinom(y, 8L, p))
  }) |>
  purrr::list_rbind() |>
  dplyr::arrange(y)

df_binom = tibble::tibble(
  sigma = 0,
  y = seq.int(0, 8),
  z = z_hat,
  Density = dbinom(y, 8, p_hat)
)

df_mixed = tibble::tibble(sigma) |>
  dplyr::mutate(data = purrr::map(sigma, \(s) {
    df_unmixed |>
      dplyr::mutate(w = dnorm(z, z_hat, s), Density = w * Density) |>
      dplyr::mutate(Density = Density / sum(Density))
  })) |>
  tidyr::unnest(data) |>
  dplyr::bind_rows(df_binom) |>
  dplyr::mutate(Z = cut(z, c(-Inf, breaks, Inf)))

p_mixed = df_mixed |>
  ggplot() + aes(y, Density) +
  facet_wrap(vars(sigma), nrow = 1L, labeller = label_both) +
  labs(x = "# survived seeds") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank(),
        legend.position = "none")

p_mixed + geom_col(aes(fill = sigma == 0), width = 0.4, alpha = 0.66)
```

---
## zã®å€¤ã§è‰²åˆ†ã‘ã—ã¦ã¿ã‚‹ã¨æƒ³åƒã—ã‚„ã™ã„

æ­£è¦åˆ†å¸ƒã¨äºŒé …åˆ†å¸ƒã®æ··ãœåˆã‚ã›......?

```{r, alter-sigma-z}
#| echo: false
#| fig.height: 3.5
#| fig.width: 11
scale_fill_z = colorspace::scale_fill_discrete_divergingx("Zissou 1")
p_z + geom_col(aes(fill = Z)) + scale_fill_z
p_mixed + geom_col(aes(fill = Z), width = 0.4) + scale_fill_z
```

---
## æ··åˆåˆ†å¸ƒã€‚ãŸã ã®äºŒé …åˆ†å¸ƒã‚ˆã‚Šã‚‚è‰¯ã„ã‚ã¦ã¯ã¾ã‚Šã€‚

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿p(ã‚’æ±ºã‚ã‚‹z)ã”ã¨ã«äºŒé …åˆ†å¸ƒã‚’ä½œã£ã¦ã€é‡ã¿ä»˜ã‘ã—ã¦è¶³ã—ãŸã‚‚ã®ã€‚

```{r, before-mixing}
#| echo: false
#| fig.height: 3.5
#| fig.width: 11
df_mixed |>
  dplyr::filter(sigma == max(sigma)) |>
  dplyr::group_by(Z) |>
  dplyr::mutate(Density = Density / sum(Density)) |>
  dplyr::ungroup() |>
  ggplot() + aes(y, Density) +
  geom_col(aes(fill = Z)) +
  scale_fill_z +
  facet_wrap(vars(Z), nrow = 1, labeller = label_both) +
  labs(x = "# survived seeds") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank(),
        legend.position = "none")
```

<div align="center">

```{r, after-mixing}
#| echo: false
#| fig.height: 3.5
#| fig.width: 7
p1 = df_mixed |>
  dplyr::filter(sigma == max(sigma)) |>
  ggplot() + aes(y, Density) +
  geom_col(aes(fill = Z)) +
  scale_fill_z +
  labs(x = "# survived seeds", title = "expected") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank(),
        legend.position = "none")

p2 = tidy_od |>
  ggplot() + aes(y, observed) +
  geom_col() +
  labs(x = "# survived seeds", y = "count", title = "observed") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank(),
        legend.title = element_blank(), legend.position = "top")

cowplot::plot_grid(p1, p2, nrow = 1L)
```

</div>

---
## ä¸€èˆ¬åŒ–ç·šå½¢æ··åˆãƒ¢ãƒ‡ãƒ« GLMM

**å›ºå®šåŠ¹æœ(fixed effects)** ã®ã¿æ‰±ã£ã¦ã„ãŸGLMã‚’æ‹¡å¼µã—ã¦ã€<br>
**å¤‰é‡åŠ¹æœ(random effect)** ã‚’æ··åˆã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚<br>
<small style="color: #999999;">ã€Œæ··åˆåˆ†å¸ƒã‚’ä½¿ã†ãƒ¢ãƒ‡ãƒ«ã€ã¨ã„ã†æ„å‘³ã§ã¯ãªã„ã‚‰ã—ã„ã€‚</small>

<p>\[\begin{split}
y_i &\sim \text{Binomial}(n,~p_i) \\
\text{logit}(p_i) &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots
  + z_{1i} + \ldots \\
z_{1i} &\sim \mathcal{N}(\mu_1,~\sigma_1)
\end{split}\]</p>

e.g.,<br>
å€‹ä½“$i$ã®ç¨®å­ç”Ÿå­˜ç‡$p_i$ã¯ã€<br>
(å›ºå®šåŠ¹æœ) ä½“ã‚µã‚¤ã‚º$x_{1i}$ã¨æ—¥å½“ãŸã‚Š$x_{2i}$ã«ä¾å­˜ã—ã€<br>
(å¤‰é‡åŠ¹æœ) ã‚ˆãã‚ã‹ã‚‰ã‚“å€‹ä½“å·®$z_{1i}$ã¨æ¤æœ¨é‰¢å·®$z_{2i}$ã‚‚ã‚ã‚‹ã€‚

---
## å›ºå®šåŠ¹æœã«ã™ã‚‹ã‹ã€å¤‰é‡åŠ¹æœã«ã™ã‚‹ã‹

æ¨å®šã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’äºˆæ¸¬ã«ä½¿ã†ãªã‚‰å›ºå®šåŠ¹æœ

äºˆæ¸¬ã«ä½¿ãˆãã†ãªã®ã§å›ºå®šåŠ¹æœå‘ã
: - è¦³æ¸¬ãƒ»æ“ä½œã—ãŸé€£ç¶šå€¤å¤‰æ•°: é•·ã•ã€é‡ã•ã€æ¸©åº¦ã€etc.
: - è¦³æ¸¬ãƒ»æ“ä½œã—ãŸã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°: æ€§åˆ¥ã€æŠ•è–¬ã€etc.

äºˆæ¸¬ã«ä½¿ãˆãªã„ã®ã§å¤‰é‡åŠ¹æœå‘ã
: - è¦³æ¸¬ãƒ»æ“ä½œã§ããªã‹ã£ãŸå€‹ä½“å·®:<br>
    ãŸã¾ãŸã¾é›†ã¾ã£ã¦ãã‚ŒãŸå­¦ç”Ÿ15äºº {A, B, C, ...}ã€‚<br>
    Aã•ã‚“ã®å›ºå®šåŠ¹æœã‚’æ¨å®šã§ãã¦ã‚‚ã€Zã•ã‚“ã®äºˆæ¸¬ã«ã¯ä½¿ãˆãªã„ã€‚
: - è¦³æ¸¬ãƒ»æ“ä½œã§ããªã‹ã£ãŸã‚°ãƒ«ãƒ¼ãƒ—å·®:<br>
    â†‘ã®å­¦ç”Ÿã‚’ãƒ©ãƒ³ãƒ€ãƒ 5äººãšã¤ã«åˆ†ã‘ãŸã‚°ãƒ«ãƒ¼ãƒ— {ã„ã€ã‚ã€ã¯}ã€‚<br>
    ã„ã‚°ãƒ«ãƒ¼ãƒ—ã®å›ºå®šåŠ¹æœã‚’æ¨å®šã§ãã¦ã‚‚ã€ã¾ãŸé›†ã¾ã‚‹ã“ã¨ã¯ãªã„ã€‚

---
## ã©ã†ã„ã†ã¨ãã«å¤‰é‡åŠ¹æœã‚’è€ƒãˆã‚‹å¿…è¦ãŒã‚ã‚‹ã‹

ãƒ‡ãƒ¼ã‚¿ã«**æ“¬ä¼¼åå¾©**ãŒå«ã¾ã‚Œã‚‹ã¨ãã€‚<br>
ãœã‚“ã¶ç‹¬ç«‹ã®ã¤ã‚‚ã‚Šã§è§£æã™ã‚‹ã¨æ¨å®šãŒåã£ãŸã‚Šèª¤ã£ãŸã‚Šã€‚

| æ¤æœ¨é‰¢ | å€‹ä½“/æ¤æœ¨é‰¢ | ç¨®å­/å€‹ä½“ | ç–‘ä¼¼åå¾© | æ¨å®šä¸å¯ |
| -----  | ----------- | ----------| ---- | ------ |
| 100å€‹  | 1å€‹ä½“ãšã¤   | 1å€‹ãšã¤   | â€“ | å€‹ä½“å·®ãƒ»é‰¢å·® |
| 25å€‹   | 1å€‹ä½“ãšã¤   | 4å€‹ãšã¤   | å€‹ä½“ | é‰¢å·® |
| 20å€‹   | 5å€‹ä½“ãšã¤   | 1å€‹ãšã¤   | æ¤æœ¨é‰¢ | å€‹ä½“å·® |
| 5å€‹    | 5å€‹ä½“ãšã¤   | 4å€‹ãšã¤   | æ¤æœ¨é‰¢ãƒ»å€‹ä½“ | â€“ |

ç–‘ä¼¼åå¾©ã‚ã‚Š<br>
â†’ è¦³æ¸¬ã§ããªã‹ã£ãŸå€‹ä½“å·®ãƒ»å ´æ‰€å·®(å¤‰é‡åŠ¹æœ)ã‚’æ¨å®šå¯èƒ½<br>
â†’ ãã®ã¶ã‚“ã‚’å·®ã—å¼•ã„ã¦å›ºå®šåŠ¹æœã‚’æ¨å®šã—ãŸã„


---
## GLMMã®å•é¡Œç‚¹ãƒ»å±•æœ›

- æœ€å°¤æ¨å®šã®è¨ˆç®—ãŒé›£ã—ããªã‚‹ã®ã§ã€ã‚ã¾ã‚Šè¤‡é›‘ã«ã¯ã§ããªã„
    - ãƒ™ã‚¤ã‚ºæ¨å®šã‚’ä½¿ãˆã°ã‚¯ãƒªã‚¢ã§ãã‚‹
- GLMã®æ‹¡å¼µã¨ã—ã¦ç†è§£ã¯ã§ãã¦ã‚‚ã€å®Ÿéš›ã«æ›¸ãã®ã¯é›£ã—ã‚
    - éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã®ä¸€ç¨®ã¨ã—ã¦è¦‹ã‚‹ã»ã†ãŒä¾¿åˆ©

â†’ ã“ã“ã§GLMMã®ç·´ç¿’ã¯ã›ãšã€éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã«é€²ã‚€ã€‚

<figure>
<a href="https://kuboweb.github.io/-kubo/ce/LinksGlm.html">
<img src="../tokiomarine2021/image/kubo-p2.png" width="60%">
<figcaption class="url">ä¹…ä¿ã•ã‚“ https://kuboweb.github.io/-kubo/ce/LinksGlm.html</figcaption>
</a>
</figure>


---
## ä¸€èˆ¬åŒ–ç·šå½¢(æ··åˆ)ãƒ¢ãƒ‡ãƒ«ã¾ã¨ã‚

- ä½•ã¯ã¨ã‚‚ã‚ã‚Œä½œå›³ã—ã¦ä¿¯ç°
- GLMã¯çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®è€ƒãˆæ–¹ã®æ ¹å¹¹
    - ç¢ºç‡åˆ†å¸ƒãƒ»ãƒªãƒ³ã‚¯é–¢æ•°ãƒ»èª¬æ˜å¤‰æ•°
    - å°¤åº¦ãƒ»æœ€å°¤æ³•ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š
    - æƒ…å ±é‡åŸºæº–ãªã©ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«é¸æŠ
- GLMMã¯ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿è§£æã«å‘ã‘ãŸå¼·åŒ–
    - ç–‘ä¼¼åå¾©ã«ã‚ˆã‚‹å¤‰é‡åŠ¹æœã‚’è€ƒæ…®
    - éšå±¤ãƒ™ã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ‰±ã†ã»ã†ãŒæ¥½


---
## ä»Šæ—¥ã®æ®‹ã‚Šæ™‚é–“

- ç­ã‚„TAã«ç›¸è«‡ã—ã€æ¶ˆåŒ–ã—ãã‚Œãªã‹ã£ãŸéƒ¨åˆ†ã‚’ãªã‚‹ã¹ãè§£æ¶ˆã™ã‚‹ã€‚
- ã¾ãšã¯å€‹äººã§èª²é¡Œ1ã¨2ã«å–ã‚Šçµ„ã¿ã€ã‚ã‚‹ç¨‹åº¦ã§ããŸã‚‰ç­å†…ã§è¦‹ã›åˆã†ã€‚
- æœ€çµ‚èª²é¡Œã«å‘ã‘ã¦è©±ã—åˆã£ãŸã‚Šã€‚


---
## å‚è€ƒæ–‡çŒ®

- [ãƒ‡ãƒ¼ã‚¿è§£æã®ãŸã‚ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€](https://amzn.to/33suMIZ) ä¹…ä¿æ‹“å¼¥ 2012
- [ãƒ‡ãƒ¼ã‚¿åˆ†æã®ãŸã‚ã®æ•°ç†ãƒ¢ãƒ‡ãƒ«å…¥é–€](https://amzn.to/3uCxTKo) æ±Ÿå´è²´è£• 2020
- [åˆ†æè€…ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿è§£é‡ˆå­¦å…¥é–€](https://amzn.to/3uznzCK) æ±Ÿå´è²´è£• 2020
- [çµ±è¨ˆå­¦ã‚’å“²å­¦ã™ã‚‹](https://amzn.to/3ty80Kv) å¤§å¡šæ·³ 2020
- [ç§‘å­¦ã¨ãƒ¢ãƒ‡ãƒ«---ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å“²å­¦ å…¥é–€](https://amzn.to/2Q0f6JQ) Michael Weisberg 2017<br>
  (åŸè‘—: [Simulation and Similarity](https://amzn.to/3bdvhuI) 2013)

`r .meta$next_link`
