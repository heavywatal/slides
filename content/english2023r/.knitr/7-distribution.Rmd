```{r, setup-common}
#| file: "setup.R"
#| echo: false
#| results: "asis"
```
```{r, setup-local}
#| include: false
#| cache: false
```

---
## Purposes of this hands-on lectures

### âœ… <del>Every biological research involves data and models</del>

### âœ… <del>You want to do reproducible analysis</del>

### âœ… <del>Learn how to do it and how to learn more</del>

### â¬œ Glance at the basics of data analysis

<hr>

You don't have to remember every command.<br>
Just repeat forgetting and searching.

---
## What do you want to do with data?

- to **understand** phenomena
- to **predict** future
- to **classify** objects
- to **control** behavior
- to **generate** something new

Is analysis necessary for that?<br>
Why not just raw data?

---
## Look back day 1

<iframe width="600" height="450" src="./1-introduction.html#/4"></iframe>
<iframe width="600" height="450" src="./1-introduction.html#/5"></iframe>
<iframe width="600" height="450" src="./1-introduction.html#/6"></iframe>
<iframe width="600" height="450" src="./1-introduction.html#/7"></iframe>

---
## Mathematical models in data science

Mathematical expression of assumptions to simulate data generation<br>
e.g., the larger the more expensive: $\text{price} = A \times \text{carat} + B + \epsilon$

```{r, lm-diamonds}
#| echo: false
#| fig.height: 5
#| fig.width: 6
diamonds |>
  dplyr::filter(clarity %in% c("I1", "SI2", "IF")) |>
  ggplot(aes(carat, price)) +
  geom_point(alpha = 0.3, size = 3) +
  stat_smooth(formula = y ~ x, method = lm, se = FALSE) +
  coord_cartesian(ylim = c(0, 20000)) +
  labs(title = "Diamonds") +
  theme_classic(base_size = 22)
```

Regression
: express y as a function of x.


---
## Extending linear regression

<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="280" alt="ãƒ‡ãƒ¼ã‚¿è§£æã®ãŸã‚ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€ ä¹…ä¿æ‹“å¼¥ 2012">
</a>
</figure>

**Linear Model (LM)** ğŸ‘ˆ #7 today

<span style="color: #888888;">&nbsp; &nbsp; â†“ probability distribution</span>

**Generalized Linear Model (GLM)** [--- #8 next time](8-glm.html)

<span style="color: #888888;">&nbsp; &nbsp; â†“ individual difference, random effect</span>

**Generalized Linear Mixed Model (GLMM)**

<span style="color: #888888;">&nbsp; &nbsp; â†“ flexible modelling</span>

**Hierarchical Bayesian Model (HBM)**

<cite>[ãƒ‡ãƒ¼ã‚¿è§£æã®ãŸã‚ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€](https://amzn.to/33suMIZ) ä¹…ä¿æ‹“å¼¥ 2012 ã‚ˆã‚Šæ”¹å¤‰</cite>


---
## Two parts to a regression model

1. Define a **family of models**: express generic pattern
    - straight line: $y = a_1 + a_2 x$
    - log curve: $\log(y) = a_1 + a_2 x$
    - quadratic curve: $y = a_1 + a_2 x^2$

2. Generate a **fitted model**: adjust parameters to get closer to the data
    - $y = 3x + 7$
    - $y = 9x^2$

<cite>
<a href="https://r4ds.had.co.nz/model-basics.html" class="url">https://r4ds.had.co.nz/model-basics.html</a>
</cite>

---
## Can see a strong pattern: the taller the heavier

The relationship looks linear, $y = a x + b$.<br>
&nbsp;

```{r, df-linear}
#| echo: false
set.seed(19937)
n = 50
df_weight = tibble::tibble(
  height = rnorm(n, 1.70, 0.05),
  bmi = rnorm(n, 22, 1),
  weight = bmi * (height**2)
)
```
```{r, weight-height}
#| echo: false
#| fig.height: 5
#| fig.width: 5
p_weight = ggplot(df_weight) +
  aes(height, weight) +
  geom_point(alpha = 0.5) +
  theme_bw(base_size = 20) +
  theme(panel.grid = element_blank())
p_weight
```


---
## Can see a strong pattern: the taller the heavier

The relationship looks linear, $y = a x + b$.<br>
OK, let's try random slope *a* and intersect *b*:

```{r, weight-lines}
#| echo: false
#| fig.height: 5
#| fig.width: 5
set.seed(19937)
df_ab = tibble(intercept = runif(n, -50, 50), slope = runif(n, -200, 200)) |>
  dplyr::mutate(intercept = intercept - 1.7 * slope + 50)
p_weight +
  geom_abline(data = df_ab, aes(intercept = intercept, slope = slope), color = "#3366ff", alpha = 0.5)
```

Need to find a good slope and intersect.

---
## Ordinary Least Square (OLS)

minimizes the <strong style="color: #E69F00">residual</strong> sum of squares (RSS)
from <span style="color: #3366ff">the regression line</span>.

```{r, weight-residual}
#| echo: false
#| fig.height: 5
#| fig.width: 10
predict_weight = function(parameters, data) {
  parameters[1] + parameters[2] * data$height
}

add_pred_weight = function(data, parameters) {
  dplyr::mutate(data, pred = predict_weight(param1, data = data))
}

rss_weight = function(parameters, data) {
  pred = predict_weight(parameters, data)
  sqdev = (data[["weight"]] - pred)**2
  sum(sqdev)
}
# rss_weight(c(40, -5), df_weight)

param1 = c(10, 30)
lm_weight = lm(weight ~ height, data = df_weight)
rss1 = rss_weight(param1, df_weight)
rss2 = rss_weight(lm_weight$coefficients, df_weight)
stopifnot(abs(sum(lm_weight$residuals**2) - rss2) < 1e-6)

p1 = p_weight %+%
  add_pred_weight(df_weight, param1) +
  geom_line(aes(y = pred), color = "#3366ff") +
  geom_linerange(aes(ymin = weight, ymax = pred), color = "#E69F00")
p2 = p1 %+%
  (df_weight |> modelr::add_predictions(lm_weight))
cowplot::plot_grid(nrow = 1L,
  p1 + annotate("text", x = -Inf, y = Inf, label = sprintf("RSS = %.1f", rss1), size = 8, hjust = -0.1, vjust = 2),
  p2 + annotate("text", x = -Inf, y = Inf, label = sprintf("RSS = %.1f", rss2), size = 8, hjust = -0.1, vjust = 2))
```



---
## Searching for models to minimize RSS

Try random values, and pick the best ones.<br>
May need to generate much more to find good one.

```{r, weight-goodlines}
#| echo: false
#| fig.height: 5
#| fig.width: 10
set.seed(19937)
n = 200
df_ab_random = tibble::tibble(intercept = runif(n, -200, 100), slope = runif(n, 0, 150)) |>
  dplyr::mutate(rss = purrr::map2_dbl(intercept, slope, ~ rss_weight(c(.x, .y), data = df_weight)))
p_ab = ggplot(df_ab_random) +
  aes(intercept, slope) +
  geom_point(data = \(x) dplyr::filter(x, rank(rss) < 6), shape = 1, size = 4) +
  geom_point(aes(color = log10(rss))) +
  theme_bw(base_size = 20) +
  theme(panel.grid = element_blank(), legend.position = c(0.99, 0.99), legend.justification = c(1, 1))

df_randmin = df_ab_random |> dplyr::slice_min(rss, n = 5)
p2 = p_weight + geom_abline(
  aes(slope = slope, intercept = intercept),
  data = df_randmin, color = "#3366ff", alpha = 0.5
)
cowplot::plot_grid(p_ab, p2, nrow = 1L)
```

---
## Searching for models to minimize RSS

**Grid search**: generate an evenly spaced grid of points.<br>
Slightly more efficient than random search?

```{r, weight-grid}
#| echo: false
#| fig.height: 5
#| fig.width: 10
df_ab_grid = tidyr::crossing(intercept = seq(-100, -30, 4), slope = seq(50, 100, 4)) |>
  dplyr::mutate(rss = purrr::map2_dbl(intercept, slope, ~ rss_weight(c(.x, .y), data = df_weight)))

p1 = p_ab %+% df_ab_grid

df_gridmin = df_ab_grid |> dplyr::slice_min(rss, n = 5)
p2 = p_weight + geom_abline(
  aes(slope = slope, intercept = intercept),
  data = df_gridmin, color = "#3366ff", alpha = 0.5
)
cowplot::plot_grid(p1, p2, nrow = 1L)
```

There are many other **optimization** techniques although not covered here.


---
## R can find the optimum in an instant

```{r, lm}
par_init = c(intercept = 0, slope = 0)
result = optim(par_init, fn = rss_weight, data = df_weight)
result$par
```

```{r, weight-lm}
#| echo: false
#| fig.height: 4
#| fig.width: 4
label = sprintf("y = %.1f x + %.1f", result$par["slope"], result$par["intercept"])
p_weight +
  stat_smooth(formula = y ~ x, method = lm, se = FALSE, color = "#3366ff", alpha = 0.5) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = label)
```

The code above is for general optimization.<br>
For simple linear regression, an easier way is as follows...

---
## `lm()` function to fit linear models

```{r, lm-mpg}
#| fig.height: 2.5
#| fig.width: 2.5
fit = lm(data = mpg, formula = hwy ~ displ)
broom::tidy(fit)

mpg_added = modelr::add_predictions(mpg, fit, type = "response")
ggplot(mpg_added) + aes(displ, hwy) + geom_point() +
  geom_line(aes(y = pred), linewidth = 1, color = "#3366ff")
```

ğŸ”° Try `lm()` with `diamonds` and `iris`.


---
## Straight LM does not fit all

```{r, df-pois}
#| echo: false
set.seed(24601)
n = 300L
a = 3
b = -3
df_pois = tibble::tibble(
  body_mass = runif(n, 0.4, 1.7),
  num_seeds = rpois(n, exp(a * body_mass + b))
)
```
```{r, lm-bad}
#| echo: false
#| fig.height: 5
#| fig.width: 5
x_breaks = c(0.5, 1.0, 1.5)
coeff = lm(num_seeds ~ body_mass, data = df_pois)$coefficients
df_lm = tidyr::crossing(body_mass = x_breaks, num_seeds = seq(-5, 20, 0.1)) |>
  dplyr::mutate(density = dnorm(num_seeds, coeff[1] + coeff[2] * body_mass, 1.4)) |>
  dplyr::filter(density > 1e-4)

p_pois = ggplot(df_pois) +
  aes(body_mass, num_seeds) +
  ggridges::geom_vridgeline(data = df_lm, aes(width = density * 0.4, group = body_mass), linetype = 0, alpha = 0) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  scale_x_continuous(breaks = x_breaks) +
  coord_cartesian(ylim = c(-5, 15)) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())
p_pois + stat_smooth(formula = y ~ x, method = lm, se = FALSE)
```

- Prediction goes below zero whereas all the observations are **positive**.
- Y values are **integer**. Their **dispersion** is larger when X is larger.


---
## Straight LM does not fit all

```{r, glm-better}
#| echo: false
#| fig.height: 5
#| fig.width: 10
p_lm = p_pois +
  stat_smooth(formula = y ~ x, method = lm, se = FALSE) +
  ggridges::geom_vridgeline(aes(width = density * 0.4, group = body_mass), df_lm,
                            fill = "#56B4E9AA", linetype = 0)
# p_lm

df_ridges = tidyr::crossing(body_mass = x_breaks, num_seeds = seq_len(30L) - 1L) |>
  dplyr::mutate(density = dpois(num_seeds, exp(a * body_mass + b))) |>
  dplyr::filter(density > 1e-4)
df_bars = df_ridges |> wtl::ridges2bars(num_seeds, density)

p_poisson = p_pois +
  stat_smooth(formula = y ~ x, method = glm, method.args = list(family = poisson), se = FALSE) +
  ggridges::geom_vridgeline(aes(width = density * 0.5, group = body_mass), df_bars,
                            fill = "#56B4E9AA", linetype = 0)
# p_poisson

cowplot::plot_grid(p_lm, p_poisson, nrow = 1L)
```

- Prediction goes below zero whereas all the observations are **positive**.
- Y values are **integer**. Their **dispersion** is larger when X is larger.
- Let's learn statistical modelling for better fitting to the data.

---
## Extending linear regression

<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="280" alt="ãƒ‡ãƒ¼ã‚¿è§£æã®ãŸã‚ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€ ä¹…ä¿æ‹“å¼¥ 2012">
</a>
</figure>

**Linear Model (LM)** ğŸ‘ˆ #7 today

<span style="color: #888888;">&nbsp; &nbsp; â†“ <span class="fragment highlight-blue custom bold">probability distribution</span></span>

**Generalized Linear Model (GLM)** [--- #8 next time](8-glm.html)

<span style="color: #888888;">&nbsp; &nbsp; â†“ individual difference, random effect</span>

**Generalized Linear Mixed Model (GLMM)**

<span style="color: #888888;">&nbsp; &nbsp; â†“ flexible modelling</span>

**Hierarchical Bayesian Model (HBM)**

<cite>[ãƒ‡ãƒ¼ã‚¿è§£æã®ãŸã‚ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€](https://amzn.to/33suMIZ) ä¹…ä¿æ‹“å¼¥ 2012 ã‚ˆã‚Šæ”¹å¤‰</cite>


---
## Probability distribution

The relationship between phenomena and their frequencies.

empirical distribution
: created by collecting samples.<br>
: e.g., rolling a dice 12 times, heights of 1000 students:

```{r, distribution}
#| echo: false
#| fig.height: 4
#| fig.width: 8
set.seed(19937)
p1 = tibble::tibble(face = sample.int(6, 12, replace = TRUE)) |>
  ggplot() +
  aes(face) +
  geom_bar(aes(y = after_stat(prop))) +
  scale_x_continuous(breaks = seq_len(6L)) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.y = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
    axis.ticks = element_blank()
  )

p2 = tibble::tibble(height = rnorm(1000L, c(160, 170), 5.5)) |>
  ggplot() +
  aes(height) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 1, boundary = 0) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), axis.ticks = element_blank())
cowplot::plot_grid(p1, p2, nrow = 1L)
```

theoretical distribution
: described with math equation and a few parameters.


---
## Random variable $X$ follows probability distribution $f$

$X \sim f(\theta)$

e.g.,<br>
The number of heads in tossing 3 fair coins $X$ **follows binomial distribution**.<br>
$X \sim \text{Binomial}(n = 3, p = 0.5)$

<div class="column-container">
  <div class="column" style="flex-shrink: 2.0;">

```{r, dbinom}
#| echo: false
#| fig.height: 3
#| fig.width: 3
size = 3L
p = 0.5
tibble(X = seq(0, 3), Prob = dbinom(X, size, p), obs = Inf) |>
  ggplot() +
  aes(X, Prob) +
  geom_col() +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

  </div>
  <div class="column" style="padding-top: 10px;">
\[\begin{split}
\text{Prob}(X = k) &= \binom n k p^k (1 - p)^{n - k} \\
k &\in \{0, 1, 2, \ldots, n\}
\end{split}\]
  </div>
</div>

Let's experiment.


---
## Record repeated trials

The number of heads observed in tossing 3 fair coins: $X$

trial 1: **H** T **H** â†’ $X = 2$<br>
trial 2: T T T â†’ $X = 0$<br>
trial 3: **H** T T â†’ $X = 1$, subsequently, $2, 1, 3, 0, 2, \ldots$

```{r, rbinom}
#| echo: false
#| fig.height: 3
#| fig.width: 11
set.seed(19937)
size = 3L
p = 0.5
X = c(2L, 0L, 1L, 2L, 1L, 3L, 0L, 2L, rbinom(92L, size, p))
df_rbinom = purrr::map(c(1, 2, 3, 10, 100), \(n) {
  tibble::tibble(X = head(X, n)) |>
    dplyr::count(X, name = "k") |>
    dplyr::mutate(Freq = k / n, Repl = n)
}) |>
  purrr::list_rbind() |>
  dplyr::bind_rows(tibble(X = seq(0, 3), Freq = dbinom(X, size, p), Repl = Inf))
df_rbinom |>
  ggplot() +
  aes(X, Freq) +
  geom_col() +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  facet_wrap(vars(Repl), nrow = 1L, labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

<div style="text-align: right;">
It approaches asymptotically to <b>binomial distribution</b>.<br>
0 and 3 are rare. 1 and 2 are three times more likely.
</div>

---
## Similar values can be generated without tossing coins

- The number of heads $X$ observed in tossing 3 fair coins.
- Random samples $X$ from the binomial distribution with $n = 3, p = 0.5$.

<div class="column-container">
  <div class="column" style="flex-shrink: 2.0;">
<img `r src_alt_fig_chunk("dbinom")`>
  </div>
  <div class="column" style="padding-top: 10px;">
$X \sim \text{Binomial}(n = 3, p = 0.5)$

&nbsp;&nbsp; â†“ sample

{2, 0, 1, 2, 1, 3, 0, 2, ...}
  </div>
</div>

These are so similar that we can say<br>
"The number of heads in *n* tosses follows binomial distribution."

Conversely, we can understand it like<br>
"Random variable of binomial distribution is the number of successes in *n* trials."

---
## A kind of statistical modelling

Tossing 3 fair coins repeatedly {2, 0, 1, 2, 1, 3, 0, 2, ...}

&nbsp;&nbsp; â†‘ simulate phenomena with a few parameters

Sample from binomial distribution with $n = 3, p = 0.5$

<figure>
<img src="../tokiomarine2021/math-model.drawio.svg" width="900"><br>
<figcaption><cite>ã€Œãƒ‡ãƒ¼ã‚¿åˆ†æã®ãŸã‚ã®æ•°ç†ãƒ¢ãƒ‡ãƒ«å…¥é–€ã€æ±Ÿå´è²´è£• 2020 ã‚ˆã‚Šæ”¹å¤‰</cite></figcaption>
</figure>

Any other probability distributions related to real phenomena like this?

???
ãŸã ã—ã€Œã“ã‚ŒãŒ3é€£ã‚³ã‚¤ãƒ³ãƒˆã‚¹ã®çœŸç†ã ã€ã§ã¯ãªã„ã€‚<br>
ã‚ãã¾ã§ã€Œã“ã†å˜ç´”åŒ–ã—ã¦ç†è§£ã§ããã†ãƒ»ä½¿ãˆãã†ã€ãªã ã‘ã€‚

ã»ã‹ã®ä»®å®š: ã‚³ã‚¤ãƒ³ãŒç«‹ã¤ã‹ã‚‚ã€‚åã£ãŸã‚³ã‚¤ãƒ³ã‹ã‚‚ã€‚ä¸¡è¡¨ã‹ã‚‚ã€‚

äºŒé …åˆ†å¸ƒã¯næšã‚³ã‚¤ãƒ³ãƒˆã‚¹ã‚’ãŸã£ãŸ2ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§èª¬æ˜ã™ã‚‹å„ªç§€ãƒ¢ãƒ‡ãƒ«


---
## Major probability distributions and related phenomena

Discrete uniform distribution
: tossing fair coins, rolling fair dice

Negative binomial distribution<br>(Geometric distribution if n = 1)
: failures before the n-th success in trials with p

Binomial distribution
: successes in n trials with p

Poisson distribution
: occurrences of a Poisson process with $\lambda$

Gamma distribution (Exponential distribution if k = 1)
: waiting time until k-th occurrence of a Poisson process with $\lambda$

Normal/Gaussian distribution
: sum of random variables, sample means, etc.


---
## Discrete uniform distribution

Every X in n values has equal probability $1/n$.

e.g., fair coin [0,1], fair dice [1,6]

```{r, dunif}
#| echo: false
#| fig.height: 4
#| fig.width: 6
df_coin = tibble::tibble(X = c("head", "tail"), Prob = c(0.5, 0.5), group = "Coin")
df_dice = tibble::tibble(X = as.character(seq_len(6L)), Prob = rep(1 / 6, 6), group = "Dice")
dplyr::bind_rows(df_coin, df_dice) |>
  ggplot() +
  aes(X, Prob) +
  geom_col() +
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1)) +
  facet_grid(cols = vars(group), scale = "free_x", space = "free_x") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

ğŸ”° Other examples of discrete uniform distribution?


---
## Geometric distribution $~\text{Geom}(p)$

$X$ failures before the first success with success probability $p$ for each trial.

e.g., How many tails before first head with a coin?

```{r, geometric}
#| echo: false
#| fig.height: 4
#| fig.width: 11
df = purrr::map(c(0.2, 0.5, 0.9), \(p) {
  tibble::tibble(p, X = seq(0, 25), Prob = dgeom(X, p))
}) |>
  purrr::list_rbind() |>
  dplyr::filter(Prob > 0.001)
x_br = c(seq.int(0, 10), seq.int(15, 100, 5))
ggplot(df) +
  aes(X, Prob) +
  scale_x_continuous(breaks = x_br) +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  geom_col() +
  facet_grid(cols = vars(p), scales = "free_x", space = "free_x", labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

\\[
\text{Prob}(X = k \mid p) = p (1 - p)^k
\\]

There is another definition: $X$ trials until the first success.

ğŸ”° Other examples?


---
## Negative binomial distribution $~\text{NB}(n, p)$

$X$ failures before the n-th success with success probability $p$ for each trial.<br>
(identical to geometric distribution when n = 1)

```{r, nbinom}
#| echo: false
#| fig.height: 3.8
#| fig.width: 11
p = 0.5
df = purrr::map(seq.int(1, 3), function(n) {
  tibble::tibble(X = seq.int(0, 10), Prob = dnbinom(X, n, p), n = n)
}) |>  purrr::list_rbind()
ggplot(df) +
  aes(X, Prob) +
  scale_x_continuous(breaks = df[["X"]]) +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  geom_col() +
  facet_grid(cols = vars(n), scales = "free_x", space = "free_x", labeller = label_both) +
  labs(title = paste0("p = ", p)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

\\[
\text{Prob}(X = k \mid n,~p) = \binom {n + k - 1} k p^n (1 - p)^k
\\]

There is another definition: $X$ trials until the n-th success.

ğŸ”° Other examples?

<!--
å¹³å‡$\lambda$ãŒã‚¬ãƒ³ãƒåˆ†å¸ƒã§ã°ã‚‰ã¤ã„ãŸãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã€ã¨ã‚‚è§£é‡ˆã§ãã‚‹ã€‚<br>
($k \to \infty$ã§ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã¨ä¸€è‡´)
-->


---
## Binomial distribution $~\text{Binomial}(n,~p)$

$X$ successes in $n$ trials with success probability $p$ for each trial.

```{r, dbinom-n}
#| echo: false
#| fig.height: 4
#| fig.width: 11
p = 0.25
df = purrr::map(2**seq.int(0, 4), \(n) {
  tibble::tibble(X = seq(0, n), Prob = dbinom(X, n, p), n = n)
}) |> purrr::list_rbind()
ggplot(df) +
  aes(X, Prob) +
  scale_x_continuous(breaks = df[["X"]]) +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  geom_col() +
  facet_grid(cols = vars(n), scales = "free_x", space = "free_x", labeller = label_both) +
  labs(title = paste0("p = ", p)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

\\[
\text{Prob}(X = k \mid n,~p) = \binom n k p^k (1 - p)^{n - k}
\\]

ğŸ”° Other examples?


---
## Poisson distribution $~\text{Poisson}(\lambda)$

$X$ occurrences of a **Poisson process** in a fixed interval of time (space).<br>
**Poisson process**: Events occur at a constant rate $\lambda$

e.g., messages received per hour, number of individuals in a quadrat

```{r, dpoisson}
#| echo: false
#| fig.height: 4
#| fig.width: 11
set.seed(19937)
lambda = c(1, 5, 10)
.arrow = grid::arrow(length = unit(0.1, "inches"), type = "closed")
p_poisson_process = tibble::tibble(y = rev(seq_along(lambda)), lambda) |>
  dplyr::mutate(time = purrr::map(lambda, ~ runif(.x * 3, 0, 3))) |>
  tidyr::unnest(time) |>
  ggplot() +
  aes(time, y) +
  annotate("segment", x = -0.1, xend = 3.1, y = 1:3, yend = 1:3, linewidth = 2, arrow = .arrow, linejoin = "mitre") +
  geom_point(aes(color = lambda, fill = lambda), size = 8, shape = 124, key_glyph = draw_key_rect) +
  scale_color_continuous(guide = NULL) +
  scale_fill_continuous(guide = guide_legend(label.position = "top", title.vjust = 1), breaks = lambda) +
  scale_y_continuous(limits = c(0.5, 3.5), breaks = NULL) +
  labs(y = NULL, x = "time (space)") +
  theme_bw(base_size = 18) +
  theme(
    axis.text.y = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(),
    panel.grid.minor = element_blank(), legend.position = "top"
  )

p2 = tidyr::crossing(X = seq.int(0L, 20L), lambda) |>
  dplyr::mutate(Prob = dpois(X, lambda)) |>
  ggplot() +
  aes(X, Prob) +
  geom_col(aes(fill = lambda), position = "identity", alpha = 0.5) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank(), legend.position = "none"
  )
cowplot::plot_grid(p_poisson_process, p2, nrow = 1L, rel_widths = c(4, 3))
```

\\[
\text{Prob}(X = k \mid \lambda) = \frac {\lambda^k e^{-\lambda}} {k!}
\\]

The limit of binomial distribution $(\lambda = np;~n \to \infty;~p \to 0)$<br>
â‰ˆ many trials of extremely rare events.


---
## Exponential distribution $~\text{Exp}(\lambda)$

Interval $X$ between occurrences of a **Poisson process**.<br>
**Poisson process**: Events occur at a constant rate $\lambda$

e.g., intervals between messages received, between gloves left on a road

```{r, dexp}
#| echo: false
#| fig.height: 4
#| fig.width: 11
p2 = tidyr::crossing(x = seq(0, 3, length.out = 201), lambda) |>
  dplyr::mutate(Prob = dexp(x, rate = lambda)) |>
  ggplot() +
  aes(x, Prob) +
  geom_area(aes(fill = lambda, group = lambda), position = "identity", alpha = 0.5) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank(), legend.position = "none"
  )
cowplot::plot_grid(p_poisson_process, p2, nrow = 1L, rel_widths = c(4, 3))
```

\\[
\text{Prob}(x \mid \lambda) = \lambda e^{-\lambda x}
\\]

The continuous counterpart of geometric distribution.

ğŸ”° Other examples?


---
## Gamma distribution $~\text{Gamma}(k,~\lambda)$

Waiting time $X$ until $k$-th occurrence of a **Poisson process**.
**Poisson process**: Events occur at a constant rate $\lambda$

e.g., Waiting time until receiving two messages

```{r, dgamma}
#| echo: false
#| fig.height: 4
#| fig.width: 11
p2 = tidyr::crossing(x = seq(0, 3, length.out = 201), lambda) |>
  dplyr::mutate(Prob = dgamma(x, rate = lambda, shape = 3)) |>
  ggplot() +
  aes(x, Prob) +
  geom_area(aes(fill = lambda, group = lambda), position = "identity", alpha = 0.5) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank(), legend.position = "none"
  )
cowplot::plot_grid(p_poisson_process, p2, nrow = 1L, rel_widths = c(4, 3))
```

\\[
\text{Prob}(x \mid k,~\lambda) = \frac {\lambda^k x^{k - 1} e^{-\lambda x}} {\Gamma(k)}
\\]

Identical when shape parameter $k = 1$.


---
## Normal/Gaussian distribution $~\mathcal{N}(\mu,~\sigma)$

Beautiful distribution with two parameters: mean $\mu$, standard deviation $\sigma$.<br>
e.g., $\mu = 50, ~\sigma = 10$:

```{r, gaussian}
#| echo: false
#| fig.height: 5
#| fig.width: 11
ci = qnorm(c(0.005, 0.025, 0.975, 0.995), 50, 10)
tibble::tibble(x = seq(0, 100, 0.1), Prob = dnorm(x, 50, 10)) |>
  ggplot() +
  aes(x, Prob) +
  geom_area(alpha = 0.4) +
  geom_area(data = function(.x) {
    dplyr::filter(.x, dplyr::between(x, ci[2], ci[3]))
  }, alpha = 0.4) +
  geom_area(data = function(.x) {
    dplyr::filter(.x, dplyr::between(x, ci[1], ci[4]))
  }, alpha = 0.4) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank(), legend.position = "none"
  )
```

\\[
\text{Prob}(x \mid \mu,~\sigma) = \frac 1 {\sqrt{2 \pi \sigma^2}} \exp \left(\frac {-(x - \mu)^2} {2\sigma^2} \right)
\\]

---
## Many distributions approach normal distribution

Distribution of sample means (**central limit theorem**);
e.g., average of 40 samples from uniform distribution [0, 100):

```{r, central-limit}
#| echo: false
#| fig.height: 3
#| fig.width: 11
set.seed(19937)
n = 40L
X = replicate(10000L, mean(runif(n, 0, 100)))
purrr::map(c(10, 100, 1000, 10000), \(n) {
  tibble::tibble(X = head(X, n), Repl = n)
}) |>
  purrr::list_rbind() |>
  ggplot() +
  aes(X) +
  geom_histogram(bins = 25) +
  facet_wrap(vars(Repl), nrow = 1L, scale = "free_y", labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
    axis.ticks = element_blank()
  )
```

Binomial distribution with large $n$:

```{r, binom-normal}
#| echo: false
#| fig.height: 3
#| fig.width: 11
purrr::map(c(1, 4, 16, 64, 256), \(n) {
  tibble::tibble(X = seq(0, n), Prob = dbinom(X, n, 0.25), n = n) |>
    dplyr::filter(Prob > 1e-5)
}) |>
  purrr::list_rbind() |>
  ggplot() +
  aes(X, Prob) +
  geom_col(width = 1) +
  facet_wrap(vars(n), nrow = 1L, scale = "free", labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
    axis.ticks = element_blank()
  )
```

---
## Many distributions approach normal distribution

Poisson distribution with large $\lambda$:

```{r, poisson-normal}
#| echo: false
#| fig.height: 2.5
#| fig.width: 11
purrr::map(c(1, 4, 16, 64, 256), \(lambda) {
  tibble::tibble(X = seq(0, 4 * lambda), Prob = dpois(X, lambda), lambda) |>
    dplyr::filter(Prob > 1e-5)
}) |>
  purrr::list_rbind() |>
  ggplot() +
  aes(X, Prob) +
  geom_col(width = 1) +
  facet_wrap(vars(lambda), nrow = 1L, scale = "free", labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
    axis.ticks = element_blank()
  )
```

Gamma distribution with large $k$:

```{r, gamma-normal}
#| echo: false
#| fig.height: 4
#| fig.width: 11
.guide = guide_legend(reverse = TRUE, label.position = "left", label.hjust = 1)
tidyr::crossing(x = seq(0, 25, length.out = 300), k = 10**seq.int(0, 3)) |>
  dplyr::mutate(Prob = dgamma(x, rate = k / 10, shape = k)) |>
  ggplot() +
  aes(x, Prob) +
  geom_area(aes(fill = k, group = k), position = "identity", alpha = 0.5) +
  scale_fill_viridis_c(trans = "log10", guide = .guide) +
  coord_cartesian(xlim = c(0, 20)) +
  theme_bw(base_size = 20) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank()
  )
```


---
## Major probability distributions and related phenomena

<figure style="float: right;">
<img src="../tokiomarine2021/math-model.drawio.svg" width="420"><br>
</figure>

Discrete uniform distribution
: tossing fair coins, rolling fair dice

Negative binomial distribution<br>(Geometric distribution if n = 1)
: failures before the n-th success in trials with p

Binomial distribution
: successes in n trials with p

Poisson distribution
: occurrences of a Poisson process with $\lambda$

Gamma distribution (Exponential distribution if k = 1)
: waiting time until k-th occurrence of a Poisson process with $\lambda$

Normal/Gaussian distribution
: sum of random variables, sample means, etc.


---
## Real data rarely follow theoretical distributions

Collect and sow 8 seeds from each of 100 plant individuals.<br>
The number of survived seeds per parent should follow $\text{Binomial}(n = 8, p)$.<br>
But extreme cases (all survived, all dead) were frequently observed.

<img `r src_alt_fig_chunk("overdispersion")`>

"Why? What other factors affect?" is the question of statistical modelling.<br>
It requires the understanding of the **null distribution**.


---
## Pseudo Random Number Generator

Algorithm to generate a sequence of random**-ish** numbers.<br>
Its computation is not stochastic, but **deterministic**.<br>
Exactly same values are generated if the same **seed** is set.

```r
set.seed(42)
runif(3L)
# 0.9148060 0.9370754 0.2861395
runif(3L)
# 0.8304476 0.6417455 0.5190959
set.seed(42)
runif(6L)
# 0.9148060 0.9370754 0.2861395 0.8304476 0.6417455 0.5190959
```

Reproducible results can be obtained by fixing a seed.<br>
Do NOT search for the seeds that produce favorable results.

Possible to generate various random numbers that follows probability distributions.

---
## Generate random numbers from various distributions

```r
n = 100
x = sample.int(6, n, replace = TRUE)
x = runif(n, min = 0, max = 1)
x = rgeom(n, prob = 0.5)
x = rbinom(n, size = 3, prob = 0.5)
x = rpois(n, lambda = 10)
x = rnorm(n, mean = 50, sd = 10)
print(x)

p1 = ggplot(data.frame(x)) + aes(x)
p1 + geom_histogram() # for continuous values
p1 + geom_bar()       # for discrete values
```

ğŸ”° Observe the effects of altering sample size `n`.

ğŸ”° Observe the effects of altering parameters for each distribution.

(Use Quarto effectively)

```{r, hidden-gacha}
#| include: false
x = rbinom(1000000, 10, 0.01)
p = ggplot(data.frame(x)) + aes(x) + geom_bar()
table(x)
```

---
## Fitting probability distributions to data

The number of seeds were counted for each of 50 plant individuals.<br>
Individual A has 2 seeds, B has 4 seeds, ...

```{r, df-poisson}
#| echo: false
set.seed(24601)
df_rpois = tibble::tibble(X = rpois(50L, 3))
max_x = 11L
df_dpois = purrr::map(c(1, 3, 5), \(lambda) {
  tibble::tibble(lambda, X = seq.int(0L, max_x), Prob = dpois(X, lambda))
}) |> purrr::list_rbind()
```
```{r, poisson-seed}
#| echo: false
#| fig.height: 4
#| fig.width: 4
ggplot(df_rpois) +
  aes(X) +
  geom_bar(width = 0.4) +
  coord_cartesian(xlim = c(0, max_x)) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
    axis.ticks = element_blank()
  )
```

This count data looks <span class="fragment custom blur">Poisson</span>-distributed.<br>
What is the optimal $\lambda$ value?



---
## Fitting probability distributions to data

The number of seeds were counted for each of 50 plant individuals.<br>
Individual A has 2 seeds, B has 4 seeds, ...

```{r, poisson-seed-lambda}
#| echo: false
#| fig.height: 4
#| fig.width: 11
p_pois = ggplot(df_rpois) +
  aes(X) +
  geom_bar(aes(y = after_stat(prop)), width = 0.4) +
  geom_col(data = df_dpois, aes(y = Prob), alpha = 0.5, fill = "#56B4E9") +
  facet_wrap(vars(lambda), nrow = 1L, labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(), axis.ticks = element_blank()
  )
p_pois
```

This count data looks Poisson-distributed.<br>
What is the optimal $\lambda$ value?

Observations in black.
<span style="color: #56B4E9;">Poisson distribution in blue</span>.
$\lambda = 3$ looks good.


---
## Likelihood: a measure for goodness-of-fit

The probability to observe the data $D$ given the model $M$.<br>
$\text{Prob}(D \mid M)$

**Likelihood function** is the same probability from different viewpoints:

- as a function of model $M$ given the data $D$,<br>
  $L(M \mid D)$<br>
- as a function of parameters $\theta$,<br>
  $L(\theta \mid D)$ or $L(\theta)$


---
## Example of likelihood calculation

Data $D$: 4 heads (H) and 1 tail (T) in tossing a coin 5 times

Assuming the probability of coming up head $p = 0.5$:
<div>\[\begin{split}
L(0.5 \mid D)
  &= \binom 5 1 \times \text{Prob}(H \mid 0.5) ^ 4 \times \text{Prob}(T \mid 0.5) ^ 1 \\
  &= 5 \times 0.5 ^ 4 \times 0.5 ^ 1 = 0.15625
\end{split}\]</div>

Assuming the probability of coming up head $p = 0.8$:
<div>\[\begin{split}
L(0.8 \mid D)
  &= \binom 5 1 \times \text{Prob}(H \mid 0.8) ^ 4 \times \text{Prob}(T \mid 0.8) ^ 1 \\
  &= 5 \times 0.8 ^ 4 \times 0.2 ^ 1 = 0.4096
\end{split}\]</div>

$L(0.8 \mid D) > L(0.5 \mid D)$

$p = 0.8$ is more likely.



---
## Likelihood in the example of Poisson distribution

The number of seeds were counted for each of 50 plant individuals.

<div>\[\begin{split}
L(\lambda \mid D)
  = \prod _i ^n \text{Prob}(X_i \mid \lambda)
  = \prod _i ^n \frac {\lambda ^ {X_i} e ^ {-\lambda}} {X_i !}
\end{split}\]</div>

```{r, poisson-seed-likelihood}
#| echo: false
#| fig.height: 4
#| fig.width: 11
df_likelihood = df_rpois |>
  dplyr::left_join(df_dpois, by = "X") |>
  dplyr::group_by(lambda) |>
  dplyr::summarize(L = prod(Prob)) |>
  dplyr::mutate(logL = log(L), label = sprintf("L(%.0f|D) = %.1e", lambda, L))
p_pois +
  geom_text(data = df_likelihood, aes(label = label), color = "#56B4E9",
            x = Inf, y = Inf, hjust = 1.1, vjust = 1.3, size = 6)
```

OK, $\lambda = 3$ is better than the other two. What is the best.

---
## <u>M</u>aximum <u>L</u>ikelihood <u>E</u>stimation

**Log likelihood** is often easier to handle.<br>
Solving the differential equation for $\lambda$ ...... finds **the sample mean**

<div>\[\begin{split}
\log L(\lambda \mid D)
  &= \sum _i ^n \left[ X_i \log (\lambda) - \lambda - \log (X_i !) \right] \\
\frac {\mathrm d \log L(\lambda \mid D)} {\mathrm d \lambda}
  &= \frac 1 \lambda \sum _i ^n X_i - n = 0 \\
\hat \lambda &= \frac 1 n \sum _i ^n X_i
\end{split}\]</div>


```{r, poisson-mle}
#| echo: false
#| fig.height: 3
#| fig.width: 10
count_rpois = df_rpois |> dplyr::count(X)
calc_likelihood_rpois = function(lambda) {
  prod(dpois(count_rpois[["X"]], lambda)**count_rpois[["n"]])
}
X_mle = mean(df_rpois[["X"]])
L_mle = calc_likelihood_rpois(X_mle)
p_mle = tibble::tibble(lambda = seq(1, 5, 0.1), L = purrr::map_dbl(lambda, calc_likelihood_rpois)) |>
  dplyr::mutate(logL = log(L)) |>
  ggplot() +
  aes(lambda, logL) +
  geom_line() +
  geom_vline(xintercept = X_mle, color = "#56B4E9") +
  annotate("point", x = X_mle, y = log(L_mle), size = 3, color = "#56B4E9") +
  annotate("text", label = sprintf("lambda = %.2f", X_mle), color = "#56B4E9",
           x = Inf, y = Inf, hjust = 1.1, vjust = 1.2, size = 5) +
  labs(y = "log L") +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank()
  )

df_dpois = tibble(lambda = X_mle, X = seq(0, 11), Prob = dpois(X, lambda))
p_pois = ggplot(df_rpois) +
  aes(X) +
  geom_bar(aes(y = after_stat(prop)), width = 0.4) +
  geom_col(data = df_dpois, aes(y = Prob), alpha = 0.5, fill = "#56B4E9") +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(), axis.ticks = element_blank()
  )

cowplot::plot_grid(p_mle, p_pois, nrow = 1L, rel_widths = c(2, 1))
```

---
## MLE does not give you â€œtrue Î»â€

The data was actually generated from â€œ$X \sim \text{Poisson}(\lambda = 3.0)$â€.

By replicating "sample 50 individuals â†’ MLE" 1,000 times,<br>
we find great variability in estimation and empirical distributions:

```{r, poisson-mle-repl}
#| echo: false
#| fig.height: 5
#| fig.width: 11
set.seed(19937)
nrep = 1000L
df_repl = tibble::tibble(X = rpois(50L * nrep, 3), repl = rep(seq_len(nrep), each = 50L))
df_sum = df_repl |>
  dplyr::group_by(repl) |>
  dplyr::summarize(lambda = mean(X))
df_minmax = dplyr::bind_rows(dplyr::slice_max(df_sum, lambda), dplyr::slice_min(df_sum, lambda))

p_repl = df_sum |>
  ggplot() +
  aes(lambda) +
  geom_histogram(bins = 25, center = 3) +
  annotate("point", x = df_minmax$lambda, y = 0, color = "#56B4E9", size = 8, alpha = 0.5) +
  labs(x = "estimated_lambda") +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank()
  )

df_dpois = tidyr::crossing(lambda = df_minmax$lambda, X = seq(0, 11)) |>
  dplyr::mutate(Prob = dpois(X, lambda))
p_minmax = df_repl |>
  dplyr::filter(repl %in% df_minmax$repl) |>
  dplyr::left_join(df_minmax, by = "repl") |>
  ggplot() +
  aes(X) +
  geom_bar(aes(y = after_stat(prop)), width = 0.4) +
  geom_col(data = df_dpois, aes(y = Prob), alpha = 0.5, fill = "#56B4E9") +
  facet_wrap(vars(lambda), ncol = 1L, labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(), axis.ticks = element_blank()
  )

cowplot::plot_grid(p_repl, p_minmax, nrow = 1L, rel_widths = c(2, 1))
```

Note: Fitting to each sample looks not bad!


---
## Alleviated by increasing sample size

1,000 replications of MLE with $n$ individuals from $X \sim \text{Poisson}(\lambda = 3.0)$:

```{r, poisson-mle-nsam}
#| echo: false
#| fig.height: 4
#| fig.width: 11
set.seed(19937)
nrep = 1000L
purrr::map(c(5, 50, 500, 5000), \(n) {
  tibble::tibble(X = rpois(n * nrep, 3), repl = rep(seq_len(nrep), each = n)) |>
    dplyr::group_by(repl) |>
    dplyr::summarize(estimated_lambda = mean(X)) |>
    dplyr::mutate(n = n)
}) |>
  purrr::list_rbind() |>
  ggplot() +
  aes(estimated_lambda) +
  geom_histogram(bins = 25, center = 3) +
  facet_wrap(vars(n), nrow = 1L, labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank()
  )
```

Q. How much is enough?<br>
A. Depends on what you estimate, acceptable error range, etc.


---
## Mathematical models in data science

> All models are wrong, but some are useful. --- George E. P. Box

<figure>
<img src="../tokiomarine2021/math-model.drawio.svg" width="900"><br>
<figcaption><cite>ã€Œãƒ‡ãƒ¼ã‚¿åˆ†æã®ãŸã‚ã®æ•°ç†ãƒ¢ãƒ‡ãƒ«å…¥é–€ã€æ±Ÿå´è²´è£• 2020 ã‚ˆã‚Šæ”¹å¤‰</cite></figcaption>
</figure>


---
## Toolbox of statistical modelling

- **Random variable** $X$
- **Probability distribution** $X \sim f(\theta)$
    - **parameters** $\theta$
- **Likelihood**
    - The probability to observe the data given the model: $\text{Prob}(D \mid M)$
    - as a function of model given the data<br>
      â†’ **likelihood function** $L(M \mid D),~L(\theta \mid D)$
    - **Maximum Likelihood Estimation** to fit parameters $\hat \theta$


---
## ğŸ”° Challenge: likelihood and MLE by hand

Rolling a dice 10 times, 3 sixes were observed.

1. Calculate likelihood assuming the probability to come up 6 $p = 1/6$.

1. Calculate likelihood assuming the probability to come up 6 $p = 0.2$.

1. **Draw a graph** with $p$ as horizontal axis, log likelihood as vertical axis.

1. Estimate $p$ with MLE.<br>
   Excellent, if solved with math; Good, if solved with R; OK, by eye or intuition.

Hint
: $\binom 5 2 = {}_5 \mathrm{C} _2 = 10$ can be achieved with `choose(5, 2)` in R.


```{r, get-likelihood}
#| include: false
get_likelihood = function(p, k = 3, n = 10) {
  choose(n, k) * p ** k * (1 - p) ** (n - k)
  # dbinom(k, n, p)
}

get_likelihood(0.2)
```

---
## å‚è€ƒæ–‡çŒ®

- [ãƒ‡ãƒ¼ã‚¿è§£æã®ãŸã‚ã®çµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€](https://amzn.to/33suMIZ) ä¹…ä¿æ‹“å¼¥ 2012
- [Stanã¨Rã§ãƒ™ã‚¤ã‚ºçµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°](https://amzn.to/3uwx7Pb) æ¾æµ¦å¥å¤ªéƒ 2016
- [Rã¨Stanã§ã¯ã˜ã‚ã‚‹ ãƒ™ã‚¤ã‚ºçµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†æå…¥é–€](https://amzn.to/3o1eCzP) é¦¬å ´çœŸå“‰ 2019
- [ãƒ‡ãƒ¼ã‚¿åˆ†æã®ãŸã‚ã®æ•°ç†ãƒ¢ãƒ‡ãƒ«å…¥é–€](https://amzn.to/3uCxTKo) æ±Ÿå´è²´è£• 2020
- [åˆ†æè€…ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿è§£é‡ˆå­¦å…¥é–€](https://amzn.to/3uznzCK) æ±Ÿå´è²´è£• 2020
- [çµ±è¨ˆå­¦ã‚’å“²å­¦ã™ã‚‹](https://amzn.to/3ty80Kv) å¤§å¡šæ·³ 2020
- [ç§‘å­¦ã¨ãƒ¢ãƒ‡ãƒ«---ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å“²å­¦ å…¥é–€](https://amzn.to/2Q0f6JQ) Michael Weisberg 2017<br>
  (åŸè‘—: [Simulation and Similarity](https://amzn.to/3bdvhuI) 2013)

`r .meta$next_link`
