```{r, setup-common}
#| file: "setup.R"
#| echo: false
#| results: "asis"
```
```{r, setup-local}
#| include: false
#| cache: false
wtl::knit_engines_set_cache_stan("stan/")
```


---
## ちょっとずつ線形モデルを発展させていく

<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="400" alt="データ解析のための統計モデリング入門 久保拓弥 2012">
</a>
</figure>

久保先生の"緑本"こと<br>
「[データ解析のための統計モデリング入門](https://kuboweb.github.io/-kubo/ce/IwanamiBook.html)」<br>
をベースに回帰分析の概要を紹介。

**線形モデル LM** (単純な直線あてはめ)

<span style="color: #888888;">&nbsp; &nbsp; ↓ いろんな<span style="font-weight: bold; color: #56B4E9;">確率分布</span>を扱いたい</span>

**一般化線形モデル GLM**

<span style="color: #888888;">&nbsp; &nbsp; ↓ <span style="font-weight: bold; color: #E69F00;">個体差</span>などの変量効果を扱いたい</span>

**一般化線形混合モデル GLMM**

<span style="color: #888888;">&nbsp; &nbsp; ↓ もっと<span style="font-weight: bold; color: #B2001D;">自由なモデリング</span>を！</span>

**階層ベイズモデル HBM**


---
## GLMMで登場した個体差を階層ベイズモデルで

植物100個体から8個ずつ種子を取って植えたら全体で半分ちょい発芽。<br>
親1個体あたりの生存数は<span style="color: #56B4E9;">n=8の二項分布</span>になるはずだけど、<br>
極端な値(全部死亡、全部生存)が多かった。個体差？

<img `r src_alt_fig_chunk("overdispersion")`>


---
## 個体差をモデルに組み込みたい

各個体の生存率$p_i$が能力値$z_i$のシグモイド関数で決まると仮定。<br>
その能力値は全個体共通の正規分布に従うと仮定:
$z_i \sim \mathcal{N}(\hat z, \sigma)$

<img `r src_alt_fig_chunk("sigmoid")`>

パラメータ2つで済む: 平均 $\hat z$, ばらつき $\sigma$ 。


---
## 個体能力のばらつき $\sigma$ が大きいと両端が増える

普通の二項分布は個体差無し $\sigma = 0$ を仮定してるのと同じ。

<img `r src_alt_fig_chunk("alter-sigma-z")`>

<img `r src_alt_fig_chunk("alter-sigma-z", number = 2)`>

---
## 階層ベイズモデルのイメージ図

事前分布のパラメータに、さらに事前分布を設定するので階層ベイズ

<figure>
<img src="../tokiomarine2022/hbm.drawio.svg" width="800">
</figure>


---
## さっきの図をStan言語で記述すると

`10` とか `3` とか、エイヤっと決めてるやつが超パラメータ。

```{stan, glmm}
#| cache_stan: true
data {
  int<lower=0> N;
  array[N] int<lower=0> y;
}

parameters {
  real z_hat;           // mean ability
  real<lower=0> sigma;  // sd of r
  vector[N] r;          // individual difference
}

transformed parameters {
  vector[N] z = z_hat + r;
  vector[N] p = inv_logit(z);
}

model {
  y ~ binomial(8, p);
  z_hat ~ normal(0, 10);
  r ~ normal(0, sigma);
  sigma ~ student_t(3, 0, 1);
}

generated quantities {
  array[N] int yrep = binomial_rng(8, p);
}
```

---
## 変量効果が入った推定結果

```{r, df-seeds-od-8}
#| echo: false
set.seed(24601)
sigmoid = function(x, gain = 1) {1 / (1 + exp(-gain * x))}
samplesize = 100L
df_seeds_od = tibble::tibble(
  z = rnorm(samplesize, 0.5, 3),
  p = sigmoid(z),
  y = rbinom(samplesize, 8L, p))
```
```{r, stan-glmm-prep}
#| cache_stan: "glmm"
#| stan_save_output_files: "fit"
#| results: "hide"
seeds_data = list(y = df_seeds_od$y, N = samplesize)
model = cmdstanr::cmdstan_model("stan/glmm.stan")
fit = model$sample(data = seeds_data, seed = 19937L, step_size = 0.1, refresh = 0)
draws = fit$draws(c("z_hat", "sigma", "r[1]", "r[2]"))
```

```{r, stan-glmm-print}
#| echo: false
#| cache.globals: "fit"
print(fit)
```

---
## 抜粋して作図。悪くない。

データ生成の真のパラメータ値は $\hat z = 0.5,~\sigma = 3.0$ だった。

```{r, stan-glmm}
#| echo: false
#| fig.width: 11
#| fig.height: 6
p_trace = bayesplot::mcmc_trace(draws, facet_args = list(nrow = 1L)) + coord_flip()
p_hist = bayesplot::mcmc_hist(draws, facet_args = list(nrow = 1L), bins = 30)
cowplot::plot_grid(p_trace, p_hist, ncol = 1L)
```


---
## 🔰 階層ベイズモデルの練習問題: 種の数

[`8-hbm.ipynb`](8-hbm.ipynb)
をJupyterで開き、スライド説明に沿って実行していこう。

100個体の植物から8つずつ種を取り、発芽した数を観察。

```{r, df-seeds-generate}
#| ref.label: "df-seeds-od-8"
#| echo: false
```

```{r, stan-glmm-ppc}
#| fig.width: 6
#| fig.height: 5
#| include: false
X = df_seeds_od[["y"]]
max_yrep = max(X)
y = wtl::tabulate0(X, nbins = max_yrep)
mrep = fit$draws("yrep", format = "matrix")
yrep = apply(mrep, 1, \(x) wtl::tabulate0(x, nbins = max_yrep)) |> t()
bayesplot::ppc_intervals(
  y = y, yrep = yrep, x = seq(0, max_yrep), alpha = 0.5, linewidth = 3
) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank()
  )
```

<img `r src_alt_fig_chunk("overdispersion")`>
<img `r src_alt_fig_chunk("stan-glmm-ppc")`>


---
## 🔰 階層ベイズモデルの練習問題: ビール注文数
<!-- TODO: 時間が余った場合の練習問題 -->

```{r, beer-overdispersion-data-re}
#| echo: -1
set.seed(24601)
samplesize = 300L
lambda = 3
overdisp = 4
.n = lambda / (overdisp - 1)
.p = 1 / overdisp
df_beer_od = tibble::tibble(
  X = rnbinom(samplesize, size = .n, prob = .p)
)
```

```{stan, beer-od}
#| cache_stan: true
#| echo: false
data {
  int<lower=0> N;
  array[N] int<lower=0> X;
}

parameters {
  vector<lower=0>[N] lambda;
  real<lower=0> alpha;
  real<lower=0> beta;
}

model {
  X ~ poisson(lambda);
  lambda ~ gamma(alpha, beta);
}

generated quantities {
  array[N] int yrep = poisson_rng(lambda);
}
```

```{r, stan-beer-od}
#| cache_stan: "beer-od"
#| stan_save_output_files: "fit_b"
#| include: false
beer_data = list(X = df_beer_od$X, N = samplesize)
model_b = cmdstanr::cmdstan_model("stan/beer-od.stan")
fit_b = model_b$sample(data = beer_data, seed = 19937L, step_size = 0.1, refresh = 0)
#draws = fit_b$draws(c("alpha", "beta"), format = "df") |> print()
#bayesplot::mcmc_trace(draws)
```

```{r, stan-beer-od-ppc}
#| fig.width: 6
#| fig.height: 4
#| include: false
X = beer_data[["X"]]
max_yrep = max(X) + 2L
y = wtl::tabulate0(X, nbins = max_yrep)
mrep = fit_b$draws("yrep", format = "matrix")
yrep = apply(mrep, 1, \(x) wtl::tabulate0(x, nbins = max_yrep)) |> t()
bayesplot::ppc_intervals(
  y = y, yrep = yrep, x = seq(0, max_yrep), alpha = 0.5, linewidth = 3
) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank()
  )
```

<img `r src_alt_fig_chunk("beer-overdispersion")`>
<img `r src_alt_fig_chunk("stan-beer-od-ppc")`>

---
## ベイズ推定まとめ

- 条件付き確率 $\text{Prob}(B \mid A)$ の理解が大事。
    - 事後分布 $\propto$ 尤度 ⨉ 事前分布
    - 確信度合いをデータで更新していく。
- 推定結果は分布そのもの。
    - そこから点推定も区間推定も可能。
- 解析的に解けない問題は計算機に乱数を振らせて解く。
    - MCMCサンプル $\sim$ 解きにくい事後分布
    - 理論・技術の進歩が目覚ましい。


---
## 回帰分析ふりかえり

より柔軟にモデルを記述できるようになった。計算方法も変化。

<figure>
<a href="https://kuboweb.github.io/-kubo/ce/LinksGlm.html">
<img src="../tokiomarine2021/image/kubo-p2.png" width="1100">
<figcaption class="url">久保さん https://kuboweb.github.io/-kubo/ce/LinksGlm.html</figcaption>
</a>
</figure>


---
## 全体まとめ

- 統計とは、データをうまくまとめ、それに基づいて推論するための手法。
- モデルには**理解志向**と**応用志向**があり、統計モデルは前者寄り。
    - どちらも多少は分かった上で使い分けたい。
    - どっちにしろ真の正しい何かではない。
- **確率分布**とその背後にある**確率過程**の理解が重要。
    - 乱数生成→作図を繰り返してイメージを掴もう。
    - MCMCサンプリングも事後分布からの乱数生成。
- 本講義で「統計モデリングを完全に理解した」とは言えない。
    - 理論も実践もほとんど説明していない。
    - 本を読む準備ができた、くらいの気持ち？



---
## 参考文献

- [データ解析のための統計モデリング入門](https://amzn.to/33suMIZ) 久保拓弥 2012
- [StanとRでベイズ統計モデリング](https://amzn.to/3uwx7Pb) 松浦健太郎 2016
- [RとStanではじめる ベイズ統計モデリングによるデータ分析入門](https://amzn.to/3o1eCzP) 馬場真哉 2019
- [データ分析のための数理モデル入門](https://amzn.to/3uCxTKo) 江崎貴裕 2020
- [分析者のためのデータ解釈学入門](https://amzn.to/3uznzCK) 江崎貴裕 2020
- [統計学を哲学する](https://amzn.to/3ty80Kv) 大塚淳 2020

<a href="." class="readmore">
目次に戻る
</a>
