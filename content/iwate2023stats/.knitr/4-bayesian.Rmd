```{r, setup-common}
#| file: "setup.R"
#| echo: false
#| results: "asis"
```
```{r, setup-local}
#| include: false
#| cache: false
wtl::knit_engines_set_cache_stan("stan/")
```


---
## コイントス4回、たまたま表が1回だったら

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0;">

最尤推定
: 推定結果は最も尤もらしい1点。
: データが少ないとき過剰適合気味。
: 表が出る確率 p = 0.25 のコインだろう。<br>
  (信じ難いけどデータはこう言っている)

<br>

ベイズ推定
: 推定結果は確率分布そのもの。
: データが少ないなりの不確実さも表現。
: p = 0.25 らへんである確率は高いが、<br>
  p = 0.6 とかである可能性もまあある。

  </div>
  <div class="column" style="flex-shrink: 1.4;">

```{r, freq-vs-bayes}
#| echo: false
#| fig.height: 4
#| fig.width: 4
tibble::tibble(p = seq(0, 1, 0.01), logLik = dbinom(1, 4, p, log = TRUE)) |>
  dplyr::filter(logLik > -1e6) |>
  ggplot() + aes(p, logLik) +
  geom_line() +
  geom_point(data = function(.x) {dplyr::slice_max(.x, logLik)}, shape = 16, size = 5, color = "#56B4E9") +
  geom_vline(data = function(.x) {dplyr::slice_max(.x, logLik)}, aes(xintercept = p), color = "#56B4E9") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())

tibble::tibble(p = seq(0, 1, 0.01), Density = dbeta(p, 2, 4)) |>
  ggplot() + aes(p, Density) +
  geom_area(fill = "#56B4E9", alpha = 0.5) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
```

  </div>
</div>


---
## コイントスの回数が増えていったら

**最尤推定**: 推定値が真の値に近づいていく

```{r, df-coin-beta}
#| echo: false
set.seed(24601)
coin_X = c(0L, 0L, 0L, 1L, rbinom(496L, 1L, 0.5))
coin_show = c(4, 20, 100, 500)
df_coin = purrr::map(c(0, 1, 2, 3, coin_show), \(i) {
  k = sum(head(coin_X, i))
  tibble::tibble(
    i = i,
    label = sprintf("%d / %d", k, i),
    p = seq(0, 1, 0.01),
    logLik = dbinom(k, i, p, log = TRUE),
    Density = dbeta(p, k + 1, i - k + 1))
}) |>
  purrr::list_rbind() |>
  dplyr::mutate(label = factor(label, levels = unique(label)))
```
```{r, coin-frequentist}
#| echo: false
#| fig.width: 11
#| fig.height: 3
df_coin |>
  dplyr::filter(i %in% coin_show) |>
  dplyr::filter(logLik > -20) |>
  dplyr::group_by(label) |>
  ggplot() + aes(p, logLik) +
  geom_line() +
  geom_point(data = function(.x) {dplyr::slice_max(.x, logLik)}, shape = 16, size = 5, color = "#56B4E9") +
  geom_vline(data = function(.x) {dplyr::slice_max(.x, logLik)}, aes(xintercept = p), color = "#56B4E9") +
  facet_wrap(vars(label), nrow = 1L) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
```

**ベイズ推定**: 確率分布がどんどん尖り、確信が強まる

```{r, coin-bayesian}
#| echo: false
#| fig.width: 11
#| fig.height: 3
df_coin |>
  dplyr::filter(i %in% coin_show) |>
  ggplot() + aes(p, Density) +
  geom_area(fill = "#56B4E9", alpha = 0.5) +
  facet_wrap(vars(label), nrow = 1L) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
```

---
## 確率おさらい

同時分布/結合確率: <span style="font-weight: normal;"> <span style="color: #E69F00;">A</span>かつ<span style="color: #0072B2;">B</span>の確率</span>
: $\text{Prob}(\textcolor{#E69F00}{A}, \textcolor{#0072B2}{B}) = \text{Prob}(\textcolor{#E69F00}{A} \cap \textcolor{#0072B2}{B}) = \text{Prob}(\textcolor{#E69F00}{A}) \text{Prob}(\textcolor{#0072B2}{B})$

周辺確率: <span style="font-weight: normal;"> <span style="color: #0072B2;">B</span>によらず<span style="color: #E69F00;">A</span>になる確率</span>
: $\text{Prob}(\textcolor{#E69F00}{A}) = \sum_{\textcolor{#0072B2}{B}} \text{Prob}(\textcolor{#E69F00}{A}, \textcolor{#0072B2}{B})$

条件付き確率: <span style="font-weight: normal;"> <span style="color: #0072B2;">B</span>である条件の下で<span style="color: #E69F00;">A</span>になる確率。重要。</span>
: $\text{Prob}(\textcolor{#E69F00}{A} \mid \textcolor{#0072B2}{B}) = \frac {\text{Prob}(\textcolor{#E69F00}{A}, \textcolor{#0072B2}{B})} {\text{Prob}(\textcolor{#0072B2}{B})}$

```{r, venn}
#| echo: false
#| fig.width: 6
#| fig.height: 3
make_circle = function(radius, center, n = 100L) {
  tibble::tibble(theta = seq(0, 2 * pi, length.out = n + 1L)[-1]) |>
    dplyr::mutate(x = radius * cos(theta) + center[1],
                  y = radius * sin(theta) + center[2])
}
ggplot() + aes(x, y) +
  geom_polygon(data = make_circle(5, c(0, 0)), fill = "#E69F00", alpha = 0.5) +
  geom_polygon(data = make_circle(1, c(4.3, 0)), fill = "#0072B2", alpha = 0.5) +
  coord_fixed(xlim = c(-7, 7)) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank())
```


---
## 条件付き確率がよくわかる具体例

<span style="color: #0072B2;">B Brewery</span>のビールが<span style="color: #E69F00;">Awesome</span>な確率
: $\text{Prob}(\textcolor{#E69F00}{\text{Awesome}} \mid \textcolor{#0072B2}{\text{B Brewery}}) = \frac {\text{Prob}(\textcolor{#E69F00}{\text{Awesome}},~\textcolor{#0072B2}{\text{B Brewery}})} {\text{Prob}(\textcolor{#0072B2}{\text{B Brewery}})}$
: かなり高い確率。良い醸造所。

<span style="color: #E69F00;">Awesome</span>なビールが<span style="color: #0072B2;">B Brewery</span>のものである確率
: $\text{Prob}(\textcolor{#0072B2}{\text{B Brewery}} \mid \textcolor{#E69F00}{\text{Awesome}}) = \frac {\text{Prob}(\textcolor{#E69F00}{\text{Awesome}},~\textcolor{#0072B2}{\text{B Brewery}})} {\text{Prob}(\textcolor{#E69F00}{\text{Awesome}})}$
: かなり低い確率。Awesomeなビールはほかにもたっくさんある。

<img `r src_alt_fig_chunk("venn")`>


---
## ベイズの定理

乗法定理
: $\text{Prob}(\textcolor{#E69F00}{A},~\textcolor{#0072B2}{B}) = \text{Prob}(\textcolor{#E69F00}{A} \mid \textcolor{#0072B2}{B})~\text{Prob}(\textcolor{#0072B2}{B}) = \text{Prob}(\textcolor{#0072B2}{B} \mid \textcolor{#E69F00}{A})~\text{Prob}(\textcolor{#E69F00}{A})$

<a href="https://en.wikipedia.org/wiki/Thomas_Bayes">
<img src="../tokiomarine2021/image/Thomas_Bayes.gif" height="200" align="right"></a>

移項するだけで**ベイズの定理**:
<img src="../tokiomarine2021/bayes.drawio.svg" style="padding-left: 1rem;">

宴会場にビールが運ばれてきた。これはどこのブルワリーの？

事前確率: $\text{Prob}(\textcolor{#0072B2}{B})$
: 飲む前、手元のビールが<span style="color: #0072B2;">B Brewery</span>のである確率。
: ↓ 🍻 飲んでみて更新

事後確率: $\text{Prob}(\textcolor{#0072B2}{B} \mid \textcolor{#E69F00}{A})$
: 飲んでみて<span style="color: #E69F00;">Awesome</span>だったビールが
  <span style="color: #0072B2;">B Brewery</span>のである確率。


---
## ベイズの定理 in 感染症検査

- 有病率 $\text{Prob}(I)$ : 0.3% (この地域の感染者の割合; 事前確率)
- 感度 $\text{Prob}(P \mid I)$ : 90% (感染してる人に陽性判定が出る)
- 特異度 $\text{Prob}(\lnot P \mid \lnot I)$: 99% (感染してない人に陰性判定が出る)

さて、陽性適中率(検査陽性の人が実際に感染者である確率)は？

<div>\[\begin{split}
\text{Prob}(I \mid P)
  &= \frac {\text{Prob}(P \mid I)~\text{Prob}(I)} {\text{Prob}(P)} \\
  &= \frac {\text{Prob}(P \mid I)~\text{Prob}(I)}
           {\text{Prob}(P \mid I)~\text{Prob}(I) + \text{Prob}(P \mid \lnot I)~\text{Prob}(\lnot I)} \\
  &= \frac {0.9 \times 0.003} {0.9 \times 0.003 + 0.01 \times 0.997} \approx 0.21
\end{split}\]</div>

感染者を隔離するスクリーニング目的では使いものにならない性能。

🔰 同様に $\text{Prob}(\lnot I \mid \lnot P)$ 陰性的中率を計算してみよう<br>
🔰 計算結果が検査性能だけでなく有病率にも依存することを確認しよう

```{r, infection-test}
#| eval: false
#| include: false
sensitivity = 0.9
specificity = 0.99
prevalence = 0.003
p_positive = (prevalence * sensitivity + (1 - prevalence) * (1 - specificity))
p_negative = ((1 - prevalence) * specificity + prevalence * (1 - sensitivity))
devnull = tibble::tibble(
  true_positive = prevalence * sensitivity / p_positive,
  false_positive = (1 - prevalence) * (1 - specificity) / p_positive,
  true_negative = (1 - prevalence) * specificity / p_negative,
  false_negative = prevalence * (1 - sensitivity) / p_negative
)
```


---
## ベイズの定理 in 統計モデリング

<p>
<img src="../tokiomarine2021/bayesian.drawio.svg" width="1200">
</p>

モデル$M$に対する確信度合いをデータ$D$に基づいて更新する。<br>
モデル$M$を仮説$H$やパラメータ$\theta$に置き換えてもいい。

**周辺尤度**は「確率分布の積分は1」を満たすための正規化定数とみなせる。<br>
比例関係だけ抜き出してこう書くことが多い:
<div>\[\begin{align}
\text{Prob}(M \mid D) &\propto \text{Prob}(D \mid M)~\text{Prob}(M) \tag{Model}\\
\text{Prob}(H \mid D) &\propto \text{Prob}(D \mid H)~\text{Prob}(H) \tag{Hypothesis} \\
\text{Prob}(\theta \mid D) &\propto \text{Prob}(D \mid \theta)~\text{Prob}(\theta) \tag{Parameter}
\end{align}\]</div>


---
## 表が出る確率のベイズ推定: 1. 事前分布

<div class="column-container">
  <div class="column" style="opacity: 0.2;">
<img `r src_alt_fig_chunk("posterior-beta")` style="vertical-align: middle;">
&ensp;$\propto$
  </div>
  <div class="column" style="opacity: 0.2;">
<img `r src_alt_fig_chunk("likelihood-binom")` style="vertical-align: middle;">
&ensp;⨉
  </div>
  <div class="column">
<img `r src_alt_fig_chunk("prior-beta")` style="vertical-align: middle;">
  </div>
</div>

コイントスを繰り返して、表が出る確率pをベイズ推定したい。

事前分布には**ベータ分布**を採用(理由は後で分かる):
<div>\[\begin{split}
\text{Beta}(p \mid a, b) =
   \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} p^{a-1} (1 - p)^{b-1}
\end{split}\]</div>

分布の形は $a,~b$ によって決まる。<br>
ガンマ関数の部分は厳つく見えるけどただの正規化定数。<br>
投げる前なのでとりあえず真っ平らを仮定 $\text{Beta}(p \mid a = 1, b = 1)$:

```{r, prior-beta}
#| include: false
#| fig.width: 3
#| fig.height: 3
prior_beta = df_coin |>
  dplyr::filter(i == 0) |>
  ggplot() + aes(p, Density) +
  geom_area(fill = "#56B4E9", alpha = 0.5) +
  coord_cartesian(ylim = c(0, 3)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
prior_beta + labs(title = "Prior")
```
```{r, likelihood-binom}
#| include: false
#| fig.width: 3
#| fig.height: 3
df_coin |>
  dplyr::filter(i == 4) |>
  dplyr::mutate(Lik = exp(logLik)) |>
  ggplot() + aes(p, Lik) +
  geom_line(color = "#56B4E9", linewidth = 2) +
  labs(title = "Likelihood") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
```
```{r, posterior-beta}
#| include: false
#| fig.width: 3
#| fig.height: 3
prior_beta %+%
  (df_coin |> dplyr::filter(i == 4)) +
  labs(title = "Posterior")
```


---
## 表が出る確率のベイズ推定: 2. 尤度関数

<div class="column-container">
  <div class="column" style="opacity: 0.2;">
<img `r src_alt_fig_chunk("posterior-beta")` style="vertical-align: middle;">
&ensp;$\propto$
  </div>
  <div class="column">
<img `r src_alt_fig_chunk("likelihood-binom")` style="vertical-align: middle;">
&ensp;⨉
  </div>
  <div class="column" style="opacity: 0.2;">
<img `r src_alt_fig_chunk("prior-beta")` style="vertical-align: middle;">
  </div>
</div>

4回投げて表が1回だった、というデータで**尤度**を計算(**二項分布**):
<div>\[\begin{split}
\text{Binom}(1 \mid 4,~p) = \binom {1} {4} p^{1} (1 - p)^{3}
\end{split}\]</div>

これに事前分布を掛けて正規化したら事後分布になるはず。


---
## 表が出る確率のベイズ推定: 3. 事後分布

<div class="column-container">
  <div class="column">
<img `r src_alt_fig_chunk("posterior-beta")` style="vertical-align: middle;">
&ensp;$\propto$
  </div>
  <div class="column" style="opacity: 0.66;">
<img `r src_alt_fig_chunk("likelihood-binom")` style="vertical-align: middle;">
&ensp;⨉
  </div>
  <div class="column" style="opacity: 0.66;">
<img `r src_alt_fig_chunk("prior-beta")` style="vertical-align: middle;">
  </div>
</div>

なんと、事後分布もベータ分布になる。

<div>\[\begin{split}
\text{Posterior}
  &\propto \text{Binom}(1 \mid 4,~p) \times \text{Beta}(p \mid  1, 1)\\
  &= \binom {1} {4} p^{1} (1 - p)^{3} \times
     \frac{\Gamma(1 + 1)}{\Gamma(1) \Gamma(1)} p^{1-1} (1 - p)^{1-1} \\
  &= C p^{2-1} (1 - p)^{4-1} \\
  &= \text{Beta}(p \mid 2, 4)
\end{split}\]</div>

ベータ分布の形パラメータ$a$が表、$b$が裏の回数分だけ増加。


---
## 表が出る確率のベイズ推定: 4. 逐次学習

さっきの事後分布を事前分布として、さらにデータを集める。

コイントス4回のうち表1回、に基づく**事前分布**: $\text{Beta}(p \mid 2,~4)$

さらに16回投げたら表が7回、の**尤度**: $\text{Binomial}(7 \mid 16,~p)$

**事後分布**はまた事前分布と同じ形になる:

<div>\[\begin{split}
\text{Beta}(p \mid 9, 13) \propto
  \text{Binomial}(7 \mid 16,~p) \times \text{Beta}(p \mid 2, 4)
\end{split}\]</div>

データを加えるたびに更新していける:

<img `r src_alt_fig_chunk("coin-bayesian")` width="85%">

---
## 共役事前分布

事後分布が事前分布と同じ形なので計算しやすい、という組み合わせ。

| 尤度関数 | 共役事前分布 |
| ---------- | ------------ |
| 二項分布 | ベータ分布 |
| ポアソン分布 | ガンマ分布 |
| 正規分布 | ガンマ分布 |
| 正規分布 (分散既知) | 正規分布 |

共役事前分布を使うことが常に最善とは限らない。<br>
計算コストがかかっても**無情報事前分布**を使う風潮。

---
## 事後分布を用いた推定

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0;">

区間推定
: 幅のある推定値を提示
: e.g., 95%ベイズ確信区間:<br>
  等裾事後確信区間 (<u>E</u>qual-<u>T</u>ailed <u>I</u>nterval)<br>
  最高密度区間 (<u>H</u>ighest <u>D</u>ensity <u>I</u>nterval)

点推定
: 値を1点だけ提示
: e.g.,<br>
  事後確率最大値 (<u>M</u>aximum <u>A</u> <u>P</u>osteriori)<br>
  事後中央値 (Posterior <u>Med</u>ian)<br>
  事後期待値 (<u>E</u>xpected <u>A</u> <u>P</u>osteriori)

  </div>
  <div class="column" style="flex-shrink: 1.3;">

```{r, integrate}
#| echo: false
#| fig.width: 5
#| fig.height: 8.2
#| cache-globals: false
hdi_lowerbound = function(density, ci = 0.95) {
  x = sort(density, decreasing = TRUE)
  cumsum_x = cumsum(x)
  sum_x = cumsum_x[length(x)]
  y = x[cumsum_x < (ci * sum_x)]
  y[length(y)]
}

n_head = 3
n_total = 12
df_density = tibble::tibble(
  p = seq(0, 1, 0.001),
  Density = dbeta(p, n_head, n_total - n_head))

.theme = theme_minimal(base_size = 18) + theme(
  panel.grid.minor = element_blank(),
  panel.grid.major.y = element_blank(),
  axis.text = element_blank())

bound = qbeta(c(0.025, 0.975), n_head, n_total - n_head)
.f = function(.x) {dplyr::filter(.x, dplyr::between(p, bound[1], bound[2]))}
.g = function(.x) {dplyr::mutate(.x, Density = ifelse(dplyr::between(p, bound[1], bound[2]), 0, Density))}
p_eti = df_density |>
  ggplot() + aes(p, Density) +
  geom_hline(yintercept = 0) +
  geom_line(color = "#999999", linewidth = 1) +
  geom_area(data = .f, fill = "#56B4E9", alpha = 0.5) +
  geom_area(data = .g, fill = "#333333", alpha = 0.8) +
  annotate("text", x = bound[1], y = 0.7, label = "2.5%", color = "#333333", hjust = 1) +
  annotate("text", x = bound[2], y = 0.7, label = "2.5%", color = "#333333", hjust = 0) +
  scale_x_continuous(breaks = bound) +
  labs(title = "Equal-Tailed Interval (ETI)", x = "Parameter") +
  .theme + theme(panel.grid.major = element_line(color = "#56B4E9"))

hdi_lb = hdi_lowerbound(df_density$Density)
df_hdi = df_density |> dplyr::filter(Density >= hdi_lb)
range_hdi = df_hdi |> dplyr::reframe(p = range(p)) |> dplyr::pull(p)
p_hdi = df_density |>
  ggplot() + aes(p, Density) +
  geom_hline(yintercept = 0) +
  geom_area(data = df_hdi, fill = "#56B4E9", alpha = 0.5) +
  geom_line(color = "#999999", linewidth = 1) +
  geom_hline(aes(yintercept = hdi_lb), color = "#444444", linewidth = 1, linetype = "dashed") +
  scale_x_continuous(breaks = range_hdi) +
  labs(title = "Highest Density Interval (HDI)", x = "Parameter") +
  .theme + theme(panel.grid.major = element_line(color = "#56B4E9"))

.map = function(.x) {dplyr::slice_max(.x, Density)}
.med = function(.x) {
  .x |>
    dplyr::mutate(cs = cumsum(Density)) |>
    dplyr::mutate(diff = abs(cs - 0.5 * max(cs))) |>
    dplyr::slice_min(diff)
}
.eap = function(.x) {
  .x |>
    dplyr::mutate(diff = abs(.data$p - sum(p * Density) / sum(Density))) |>
    dplyr::slice_min(diff)
}
.colors = setNames(palette()[2:4], c("map", "med", "eap"))
p_point = df_density |>
  ggplot() + aes(p, Density) +
  geom_hline(yintercept = 0) +
  geom_line(color = "#999999", linewidth = 1) +
  geom_vline(aes(xintercept = p), .map, color = .colors["map"], linewidth = 1) +
  geom_vline(aes(xintercept = p), .med, color = .colors["med"], linewidth = 1) +
  geom_vline(aes(xintercept = p), .eap, color = .colors["eap"], linewidth = 1) +
  geom_point(aes(y = 0), .map, color = .colors["map"], size = 4, shape = 16) +
  geom_point(aes(y = 0), .med, color = .colors["med"], size = 4, shape = 16) +
  geom_point(aes(y = 0), .eap, color = .colors["eap"], size = 4, shape = 16) +
  ggrepel::geom_text_repel(aes(y = 0.25, label = "MAP"), .map, color = .colors["map"], size = 5, nudge_x = -0.08) +
  ggrepel::geom_text_repel(aes(y = 0.45, label = "MED"), .med, color = .colors["med"], size = 5, nudge_x = 0.1) +
  ggrepel::geom_text_repel(aes(y = 0.25, label = "EAP"), .eap, color = .colors["eap"], size = 5, nudge_x = 0.2) +
  labs(title = "Point Estimation", x = "Parameter") +
  .theme + theme(panel.grid = element_blank())
cowplot::plot_grid(p_eti, p_hdi, p_point, ncol = 1L)
```

  </div>
</div>

???

ベイズ確信区間 (credible interval)
: 真の値が95%の確率で含まれる区間。

信頼区間 (confidence interval)
: 頻度主義の考え方に基づいていて、解釈が難しい。
「標本抽出を100回繰り返すとそのうちの95回はその区間に母平均が含まれる」
真の値は1つに定まっていて、不確実性は有限のサンプリングに由来する。



---
## ベイズ推定の中間まとめ

- 推定結果は**事後分布** ∝ 尤度関数。
    - 広がり具合によって不確実性も表現できる。
    - **逐次学習**で尖っていき、確信が強まる。

<img `r src_alt_fig_chunk("coin-bayesian")` width="70%">

<hr>

コイン投げモデルのベータ分布は美しい例。<br>
→ 解析的(数学的)に解ける。

実践的なモデル・事後分布はもっと複雑。<br>
→ コンピュータに頼って数値計算: MCMC


---
## MCMC: <u>M</u>arcov <u>C</u>hain <u>M</u>onte <u>C</u>arlo

<a href="https://en.wikipedia.org/wiki/Andrey_Markov">
<img src="../tokiomarine2021/image/AAMarkov.jpg" height="240" align="right"></a>

マルコフ連鎖
: 次の時点の挙動が現在の値だけで決定されるような確率過程。
: $\ldots \to X_{t - 2} \to X_{t - 1} \to X_{t} \to X_{t + 1}$
: $\text{Prob}(X_{t+1} \mid X_{t}, X_{t-1}, X_{t-2}, \ldots) = \text{Prob}(X_{t+1} \mid X_{t})$
: e.g., すごろく

モンテカルロ法
: 乱数を用いた計算方法の総称。
: <a href="https://en.wikipedia.org/wiki/Monte_Carlo_Casino">
  <img src="../tokiomarine2021/image/Real_Monte_Carlo_Casino.jpg" height="300"></a>
  <a href="https://en.wikipedia.org/wiki/Stanislaw_Ulam">
  <img src="../tokiomarine2021/image/Stanislaw_Ulam.jpg" height="300"></a>
  <a href="https://en.wikipedia.org/wiki/John_von_Neumann">
  <img src="../tokiomarine2021/image/John_von_Neumann.jpg" height="300"></a>
  <a href="https://en.wikipedia.org/wiki/Nicholas_Metropolis">
  <img src="../tokiomarine2021/image/Nicholas_Metropolis.png" height="300"></a>

???
スタニスワフ・ウラムがソリテアの成功率を考えてた時に思いついて、
同僚のジョン・フォン・ノイマンが計算機上での実用まで持ってった。
モナコ公国のモンテカルロ地区に国営カジノがあって、
ウラムの叔父がそこで負けて親戚から借金したことにちなんで
同僚のニコラス・メトロポリスが命名したらしい。

マルコフ連鎖
: マルコフ過程のうち、離散的な時間を考えるもの。

マルコフ過程
: マルコフ性を持つ確率過程

マルコフ性
: 次の時点の挙動が現在の値だけで決定され、過去の値と無関係


最適化ではなくサンプリング法


---
## モンテカルロ法は乱数を用いた計算方法

e.g., 半径1の円の面積

数学を知っていれば $\pi r ^ 2 \approx 3.14159$

面積4の正方形に400個の一様乱数を打ち込んだら318個が円に乗った:<br>
$4 \times \frac {318} {400} = 3.18$

```{r, circle}
#| echo: false
#| fig.width: 4
#| fig.height: 4
set.seed(19937)
.n = 200L
.theta = seq(0, 2 * pi, length.out = 100)
tibble(x = runif(.n, -1, 1), y = runif(.n, -1, 1)) |>
  dplyr::mutate(accepted = x ^ 2 + y ^ 2 < 1) |>
  ggplot() + aes(x, y) +
  annotate("polygon", x = cos(.theta), y = sin(.theta), fill = "#888888", alpha = 0.4) +
  geom_point(data = \(x) dplyr::filter(x, !accepted), shape = 16, alpha = 0.5, size = 2) +
  geom_point(data = \(x) dplyr::filter(x, accepted), color = "#56B4E9", shape = 16, size = 2) +
  coord_fixed(xlim = c(-1, 1), ylim = c(-1, 1), expand = FALSE)  +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
```

---
## 変な分布もモンテカルロ法で扱えそう

e.g., 確率密度分布に従って変数Xを集める(棄却サンプリング)。

```{r, mcpdf}
#| include: false
#| fig.width: 5
#| fig.height: 5
set.seed(24601)
n = 800
df_point = tibble::tibble(p = runif(n, -0.2, 1.2), Density = runif(n, 0, 5)) |>
  dplyr::mutate(accepted = Density < (dbeta(p, 4, 6) + dbeta(p, 70, 30)) / 2)
p_pdf = tibble::tibble(p = seq(0, 1, 0.01), Density = (dbeta(p, 4, 6) + dbeta(p, 70, 30)) / 2) |>
  ggplot() + aes(p, Density) +
  geom_hline(yintercept = 0) +
  geom_area(fill = "#000000", alpha = 0.33) +
  geom_point(data = df_point |> dplyr::filter(!accepted), shape = 16, alpha = 0.5) +
  geom_point(data = df_point |> dplyr::filter(accepted), color = "#56B4E9", shape = 16, size = 2, alpha = 1) +
  labs(x = "X") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank(),
        panel.grid.major.y = element_blank(), panel.border = element_blank(),
        axis.text = element_blank())

p_accepted = tibble::tibble(p = runif(n), th = dbeta(p, 4, 6) + dbeta(p, 70, 30)) |>
  dplyr::filter(runif(length(p), 0, max(th)) < th) |>
  ggplot() + aes(p) +
  geom_hline(yintercept = 0) +
  coord_cartesian(xlim = c(-0.2, 1.2)) +
  geom_histogram(bins = 20, center = 0.5, fill = "#56B4E9", alpha = 0.66) +
  labs(x = "X") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank(),
        panel.grid.major.y = element_blank(), panel.border = element_blank(),
        axis.text = element_blank())

print(p_pdf)
print(p_accepted)
```

<div>
<img `r src_alt_fig_chunk("mcpdf", number = 2)` style="vertical-align: middle;">
$\;\sim\;$
<img `r src_alt_fig_chunk("mcpdf")` style="vertical-align: middle;">
</div>

でも、ハズレの値もけっこう引いてしまう。


---
## 次元の呪い: 高次元になるほど当たりにくくなる

(N次元球の体積 / N次元の立方体) はゼロに近づいていく。

<img `r src_alt_fig_chunk("circle")` align="right">

- 2次元: $\frac {\pi r ^ 2} {(2r) ^ 2} = \frac \pi 4 \approx 0.79$
- 3次元: $\frac {\frac 4 3 \pi r ^ 3} {(2r) ^ 3} = \frac \pi 6 \approx 0.52$
- N次元: $\frac {\frac {\pi ^ {N/2}} {\Gamma (N/2 + 1)} r ^ N} {(2r) ^ N} = \frac {\pi ^ {N/2}} {2^N \Gamma (N/2 + 1)} \to 0$

パラメータが増えると計算量(≈乱数の無駄撃ち)が急増。

<hr>

密度の高い「当たり」付近を効率よく探索したい。<br>
「当たり」は「当たり」の近くにありがちだろう。<br>
→ マルコフ連鎖が使えそう


---
## Metropolis--Hastings法 (MH法)

0.  パラメータ $\theta$ の初期値を選ぶ
1.  $\theta$ をちょっと増減させて $\theta_\text{new}$ を作る
2.  それぞれ尤度を計算し、比較。
    - $L(\theta_\text{new}) \ge L(\theta)$ なら $\theta_\text{new}$ を即採択
    - $L(\theta_\text{new}) < L(\theta)$ でも
      確率 $r = \frac {L(\theta_\text{new})} {L(\theta)}$ で  $\theta_\text{new}$ を採択
3.  $\theta_\text{new}$ が採択されたら $\theta$ を更新。手順1に戻る。

```{r, metropolis-globals}
#| include: false
n_head = 12
n_total = 20
```
```{r, metropolis}
#| echo: false
#| fig.width: 11
#| fig.height: 3.5
.logLik = function(p) dbinom(n_head, n_total, p, log = TRUE)
.delta = 0.02
df_point = tibble::tibble(p = seq(0.48, 0.64, .delta), logLik = .logLik(p))
df_arrow = df_point |>
  dplyr::group_by(p) |>
  dplyr::mutate(data = purrr::map(p, \(.p) {
    pl = .p + 0.005
    pr = .p + .delta - 0.005
    ll = .logLik(pl)
    lr = .logLik(pr)
    if (ll < lr) {
      data.frame(pbegin = pl, pend = pr, lbegin = ll, lend = lr)
    } else {
      data.frame(pbegin = pr, pend = pl, lbegin = lr, lend = ll)
    }
  })) |>
  tidyr::unnest(data)

.ar = grid::arrow(angle = 20, length = grid::unit(0.11, "inches"), type = "closed")
.ar2 = grid::arrow(angle = 20, length = grid::unit(0.09, "inches"), type = "closed", ends = "first")
.aes = aes(pbegin, lbegin, xend = pend, yend = lend)
df_point |>
  ggplot() + aes(p, logLik) +
  geom_segment(.aes, df_arrow, linejoin = "mitre", linewidth = 1,
    position = position_nudge(0, +0.009), arrow = .ar) +
  geom_segment(.aes, df_arrow, linejoin = "mitre", linewidth = 1,
    position = position_nudge(0, -0.009), arrow = .ar2, alpha = 0.33) +
  geom_point(size = 6, alpha = 0.6, shape = 16) +
  labs(x = expression(theta)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
```

---
## 採択されたパラメータ値の軌跡

尤度が高い方にただ向かうだけでなく、結構うろつく。<br>
通ったパラメータ値を集めるといい感じの分布が得られる。

```{r, metropolis-trajectory}
#| echo: false
#| fig.show: "animate"
#| animation.hook: "gifski"
#| interval: 0.2
#| fig.width: 12
#| fig.height: 6
mh_sample = function(n, p0, n_head, n_total, delta) {
  p = p0
  lp = dbinom(n_head, n_total, p, log = FALSE)
  posterior = p
  for (i in seq.int(2L, n)) {
    p_new = p + sample(c(-delta, delta), 1)
    lp_new = suppressWarnings(dbinom(n_head, n_total, p_new, log = FALSE))
    if (is.nan(lp_new)) next
    if (lp_new > lp) {
      p = p_new
      lp = lp_new
    } else {
      if (runif(1L) < (lp_new / lp)) {
        p = p_new
        lp = lp_new
      }
    }
    posterior = c(posterior, p)
  }
  tibble::tibble(p = round(posterior, 6)) |> tibble::rowid_to_column("t")
}
.n = 800
.dp = 0.04
df_trace = tibble::tibble(init = c(0.06, 0.5, 0.94)) |>
  dplyr::mutate(data = purrr::map(init, ~{mh_sample(.n, .x, n_head, n_total, delta = .dp)})) |>
  tidyr::unnest(data)

df_traj = df_trace |> dplyr::filter(init == min(init)) |> dplyr::select(!init)

p_traj = df_traj |>
  ggplot() + aes(t, p) +
  geom_line(linewidth = 0.8, alpha = 0.66) +
  coord_cartesian(xlim = c(0, .n), ylim = c(0, 1)) +
  labs(title = "traceplot", y = expression(theta), x = "iterations") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())

p_hist = df_traj |>
  ggplot() + aes(p) +
  geom_bar() +
  coord_flip(xlim = c(0, 1)) +
  labs(title = "MCMC samples") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank(),
        axis.title.y = element_blank(), axis.text.y = element_blank())

ymax = max(ggplot_build(p_hist)$data[[1]]$count)

for (i in seq.int(40, .n, 40)) {
  df_traj_head = df_traj |> head(i)
  p1 = p_traj %+% df_traj_head
  p2 = p_hist %+% df_traj_head + ylim(c(0, ymax))
  p = cowplot::plot_grid(p1, p2, nrow = 1L, rel_widths = c(4, 1))
  print(p)
}
```


---
## 尤度に比例する事後分布からサンプルしたのと等価

全体にばら撒く棄却サンプリングよりも効率よく集められる。<br>
が、パラメータ1つの1次元ではご利益はわかりにくい。

```{r, propto-lik}
#| include: false
#| fig.width: 3.5
#| fig.height: 3.5
.theme = theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank(),
        panel.grid.major.y = element_blank(), panel.border = element_blank())

p_lik = tibble::tibble(p = seq(0, 1, 0.01), Lik = dbinom(n_head, n_total, p, log = FALSE)) |>
  ggplot() + aes(p, Lik) +
  geom_hline(yintercept = 0) +
  geom_line(linewidth = 2) +
  labs(title = "Likelihood", x = expression(theta)) +
  .theme

p_dens = tibble::tibble(p = seq(0, 1, 0.01), Density = dbeta(p, n_head, n_total - n_head)) |>
  ggplot() + aes(p, Density) +
  geom_hline(yintercept = 0) +
  geom_area(alpha = 0.5) +
  labs(title = "Posterior", x = expression(theta)) +
  .theme

p_hist = df_traj |> tail(-100) |>
  ggplot() + aes(p) +
  geom_hline(yintercept = 0) +
  geom_bar() +
  coord_cartesian(xlim = c(0, 1)) +
  labs(title = "MCMC samples", x = expression(theta), y = "Count") +
  .theme

print(p_hist)
print(p_dens)
print(p_lik)
```

<div>
<img `r src_alt_fig_chunk("propto-lik")` style="vertical-align: middle;">
$\;\sim\;$
<img `r src_alt_fig_chunk("propto-lik", number = 2)` style="vertical-align: middle;">
$\;\propto\;$
<img `r src_alt_fig_chunk("propto-lik", number = 3)` style="vertical-align: middle;">
</div>

パラメータが複数ある場合は？


---
## Gibbs Sampling

パラメータが複数の場合「ほかを固定してひとつ更新」を繰り返す。

e.g., 二次元正規分布。(-2, 2) からスタート。

```{r, gibbs}
#| echo: false
#| fig.show: "animate"
#| animation.hook: "gifski"
#| interval: 0.1
#| fig.width: 5
#| fig.height: 5
gibbs_sample = function(n, rho) {
  x = double(n)
  y = double(n)
  x[1] = -2
  y[1] = 2
  for (i in seq.int(2, n)) {
    x[i] = rnorm(1, rho * y[i - 1], sqrt(1 - rho ** 2))
    y[i] = rnorm(1, rho * x[i], sqrt(1 - rho ** 2))
  }
  x = rep(x, each = 2)
  y = rep(y, each = 2)
  tibble::tibble(x = x[-1], y = y[-2 * n])
}

set.seed(19937)
rho = 0.8
df_gibbs = gibbs_sample(40, rho)

p_tile = tidyr::crossing(x = seq(-3, 3, 0.05), y = x) |>
  dplyr::mutate(z = x^2 + y^2 - 2 * rho * x * y,
                d = (1 / (2 * pi * sqrt(1 - rho ** 2))) * exp(-0.5 * z / (1 - rho ** 2))) |>
  ggplot() + aes(x, y) +
  geom_tile(aes(fill = d)) +
  scale_fill_gradient(low = "#f8f8f8", high = "black") +
  coord_fixed(expand = FALSE) +
  theme_void() + theme(legend.position = "none")

for (i in seq.int(2L, nrow(df_gibbs), 2L)) {
  df_i = head(df_gibbs, i)
  p = p_tile +
    geom_path(data = df_i, color = "#56B4E9", alpha = 0.66, linewidth = 1) +
    geom_point(data = df_i, color = "#56B4E9", alpha = 0.66, shape = 16)
  print(p)
}
```


---
## 何回やっても似たような結果になってほしい

乱数や初期値によって偶々、じゃないことを確認したい。

e.g., `chains = 3` 。ほぼ同じところをうろうろ:

```{r, chains}
#| echo: false
#| fig.height: 5
#| fig.width: 12
t_warmup = 200
p_trace = ggplot(df_trace) + aes(t, p, group = init) +
  geom_path(aes(color = as.factor(init)), linewidth = 0.8) +
  scale_color_discrete(guide = NULL) +
  labs(title = "traceplot", y = expression(theta), x = "iterations") +
  coord_cartesian(ylim = c(0, 1)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())

p_hist = df_trace |>
  dplyr::mutate(init = as.factor(init)) |>
  ggplot() + aes(p) +
  geom_bar(aes(fill = init), position = position_stack(reverse = TRUE)) +
  coord_flip(xlim = c(0, 1)) +
  labs(title = "MCMC samples") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank(),
        axis.title.y = element_blank(), axis.text.y = element_blank(),
        legend.position = "none")
cowplot::plot_grid(p_trace, p_hist, nrow = 1L, rel_widths = c(4, 1))
```

収束(convergence)の判定については後ほど。

---
## 初期値の影響が消えるまでウォーミングアップ

定常分布の山に到達してからが本番。

e.g., `iter_warmup = 200, iter_sampling = 600` で灰色の部分を捨てる:

```{r, warmup}
#| echo: false
#| fig.height: 5
#| fig.width: 12
cowplot::plot_grid(
  p_trace + annotate("ribbon", x = c(0, t_warmup), ymin = Inf, ymax = -Inf, alpha = 0.3),
  p_hist %+% (df_trace |> dplyr::mutate(init = as.factor(ifelse(t < t_warmup, NA, init)))),
  nrow = 1L, rel_widths = c(4, 1))
```

どれくらい長く捨てるべきかは場合による。


---
## 適度に間引いて自己相関を軽減したい

直前の値と似すぎていたら独立サンプルとして扱えないので。

e.g., `thin = 5` で5回に1回だけサンプルする:

```{r, thin}
#| echo: false
#| fig.height: 5
#| fig.width: 7
p_trace +
  coord_cartesian(ylim = c(0, 1), xlim = c(200, 260)) +
  geom_point(data = function(.x) {dplyr::filter(.x, t %% 5 == 0)}, alpha = 0.6, size = 3, shape = 16)
```

間引かなくても大丈夫な場合も、間引いても解決しない場合もある。



---
## 収束判定

- 複数chainsで異なる初期値から実行し、軌跡を可視化(traceplot)
- Gelman-Rubin統計量 $\hat R < 1.05$
- Effective Sample Size (ESS) $N_\text{eff} > 100$ per chain

```{stan, converge-yes}
#| cache_stan: true
#| echo: false
data {
  int<lower=0> N;
  array[N] int x;
}

parameters {
  real<lower=0> lambda;
}

model {
  x ~ poisson(lambda);
}
```
```{stan, converge-no}
#| cache_stan: true
#| echo: false
data {
  int<lower=0> N;
  array[N] int x;
}

parameters {
  real lambda;
  real jammer;
}

model {
  x ~ poisson(lambda + jammer);
}
```

```{r, converge-template}
#| include: false
convdata = tibble::lst(N = 10L, x = rep(10L, N))
myplot_trace = function(fit) {
  s = fit$summary(variables = "lambda")
  stats = sprintf("Rhat = %.4f, N_eff = %.1f", s[["rhat"]], s[["ess_bulk"]])
  bayesplot::mcmc_trace(fit$draws("lambda")) +
    scale_color_discrete(guide = "none") +
    labs(title = ifelse(s[["rhat"]] < 1.05, "Good", "Bad"), subtitle = stats) +
    theme_bw(base_size = 18) +
    theme(panel.grid.minor = element_blank(), axis.ticks = element_blank(),
      axis.title.y = element_blank(), axis.text.y = element_blank())
}
```
```{r, converge-yes-plot}
#| include: false
#| cache_stan: "converge-yes"
#| stan_save_output_files: "fit_yes"
mod_yes = cmdstanr::cmdstan_model("stan/converge-yes.stan")
fit_yes = mod_yes$sample(data = convdata, seed = 24601L, refresh = 0)
p_yes = myplot_trace(fit_yes)
```
```{r, converge-no-plot}
#| include: false
#| cache_stan: "converge-no"
#| stan_save_output_files: "fit_no"
mod_no = cmdstanr::cmdstan_model("stan/converge-no.stan")
fit_no = mod_no$sample(data = convdata, seed = 24601L, refresh = 0, show_messages = FALSE)
p_no = myplot_trace(fit_no)
```
```{r, convergence}
#| echo: false
#| fig.height: 4
#| fig.width: 12
cowplot::plot_grid(p_yes, p_no, nrow = 1L)
```

[`diagnose()`](https://mc-stan.org/docs/cmdstan-guide/diagnose.html)
みたいな機能が提供されていれば利用する。

実行時に[警告してもらえること](https://mc-stan.org/misc/warnings.html)もある。

???
https://ill-identified.hatenablog.com/entry/2020/05/21/001158


---
## 収束・自己相関が悪い場合にどう改善するか

- 小手先の対処
    - iteration (warnup + sampling) をもっと長く
    - thinを大きくして間引く
- ちょっと大掛かり
    - プログラムの書き方を改善する
    - モデルの構造を見直す
    - アルゴリズム・ソフトウェアを変える


---
## 似て非なる: MCMCサンプル増やす vs データ増やす

<div>
<img `r src_alt_fig_chunk("propto-lik")` style="vertical-align: middle;">
$\;\sim\;$
<img `r src_alt_fig_chunk("propto-lik", number = 2)` style="vertical-align: middle;">
$\;\propto\;$
<img `r src_alt_fig_chunk("propto-lik", number = 3)` style="vertical-align: middle;">
</div>

- MCMCサンプルを増やす → 事後分布・尤度関数をより良く近似
- データを増やす → 分布の裾野が狭まり、確信が強まる

<img `r src_alt_fig_chunk("coin-bayesian")`>


---
##  MCMCの方法いろいろ

採択率を高め、早く収束するように改良されてきている。

- Metropolis--Hastings法
    - Gibbs Sampling
    - Hamiltonian Monte Carlo (HMC)
        - No-U-Turn Sampler (NUTS)


---
## MCMCソフトウェア

- [BUGS](https://www.mrc-bsu.cam.ac.uk/software/bugs/)
    - クローズドソースで、ほぼWindows専用。
- [JAGS](https://mcmc-jags.sourceforge.io/)
    - オープンソースで、さまざまなOS・言語から利用可能。
    - マニュアルや用例が不足。
- [**Stan**](https://mc-stan.org/) 👈
    - オープンソースで、さまざまなOS・言語から利用可能。
    - 開発も利用も活発。マニュアルや用例も充実。
    - HMC/NUTSにより早く収束。
- [PyMC3](https://docs.pymc.io/)
- [NumPyro](https://num.pyro.ai/)
- [TensorFlow Probability](https://www.tensorflow.org/probability/)


---
## Stan

<a href="https://mc-stan.org/">
<img src="/slides/image/stan/logo_name.png" width="180" align="right">
</a>

- Stan言語で**モデルを柔軟に記述**できる。
- C++で書かれていて**高速に動作**。
- RやPythonなどから呼び出して使うのが便利。

## R Interface

[RStan](http://mc-stan.org/rstan/)
: [Rcpp](https://heavywatal.github.io/rstats/rcpp.html)を介して[StanHeaders](https://github.com/stan-dev/rstan/tree/develop/StanHeaders)を取り込んだパッケージ。

[CmdStanR](https://mc-stan.org/cmdstanr/) 👈 今後の主流
: [CmdStan](https://mc-stan.org/users/interfaces/cmdstan)
  を呼び出し、書き出されたCSVを読み取る。


---
## CmdStanR: インストールがちょっと特殊

実行の前後にRを再起動してまっさらにすることを推奨。

1. C++言語を扱うためのツールを用意。
    - <img height=24 width=24 src="https://cdn.simpleicons.org/apple"></img>
      macOS: Command Line Tools (`xcode-select --install`)
    - <img height=24 width=24 src="https://cdn.simpleicons.org/windows"></img>
      Windows: [RTools](https://cran.r-project.org/bin/windows/Rtools/)
      (Rのバージョンに合わせる)
1. [CmdStanR](https://mc-stan.org/cmdstanr/)パッケージをインストール。
   (まだCRANに登録されていない):
    ```r
    install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
    ```
1. CmdStan本体と可視化パッケージのインストール:
    ```r
    library(cmdstanr)
    check_cmdstan_toolchain(fix = TRUE)
    install_cmdstan()
    install.packages("bayesplot")
    ```

<https://mc-stan.org/cmdstanr/articles/cmdstanr.html>

---
## 🔰 とりあえずStanを動かしてみよう

```r
library(cmdstanr)
library(bayesplot)
```
```{r, library-bayesplot}
#| message: !expr 'NA'
#| echo: false
withr::with_namespace("cmdstanr", {
  utils::capture.output(startup_messages(), type = "message") |> cat(sep = "\n")
})
withr::with_namespace("bayesplot", {
  utils::capture.output(.onAttach(), type = "message")[1] |> cat(sep = "\n")
})
```

おおまかな流れ:

1. データ準備
1. Stan言語でモデルを書く
1. モデルをコンパイルして機械語に翻訳 → 実行ファイル
1. 実行ファイルにデータを渡してMCMCサンプリング
1. 結果を見る


---
## 説明変数なしのベイズ推定: データ準備

表が出る確率 $p=0.7$ のイカサマコインをN回投げたデータを作る。<br>
この $p$ をStanで推定してみよう。

```{r, stan-binom}
#| echo: -1
set.seed(19937)
true_p = 0.7
N = 40L
coin_data = list(N = N, x = rbinom(N, 1, true_p))
print(coin_data)
```

Rならlist型、Pythonならdict型にまとめてStanに渡す。


---
## 説明変数なしのベイズ推定: Stan言語でモデル定義

別ファイルに書いておく。
e.g., `coin.stan`:

```{stan, coin}
#| cache_stan: true
data {
  int<lower=0> N;
  array[N] int x;
}
parameters {
  real<lower=0,upper=1> p;
}
model {
  x ~ binomial(1, p);
}
```

- いくつかのブロックに分けて記述する:<br>
  R/Pythonから受け取る `data`, 推定する `parameter`, 本体の `model`.
- [変数には型や制約を設定できる](https://mc-stan.org/docs/reference-manual/overview-of-data-types.html)
- [関数もたくさん用意されている](https://mc-stan.org/docs/functions-reference/)

---
## Stan言語の7種のブロック

順番厳守。よく使うのは**太字のやつ**。

1. `functions {...}`
1. **`data {...}`**
1. `transformed data {...}`
1. **`parameters {...}`**
1. `transformed parameters {...}`
1. **`model {...}`**
1. `generated quantities {...}`

<https://mc-stan.org/docs/reference-manual/overview-of-stans-program-blocks.html>


---
## 説明変数なしのベイズ推定: MCMCサンプル

予め実行速度の速い機械語に翻訳(コンパイル):
```{r, stan-binom-model}
#| cache_stan: "coin"
model = cmdstanr::cmdstan_model("stan/coin.stan")
```

モデルとデータを使ってMCMCサンプリング:

```{r, stan-binom-sample}
#| cache_stan: "coin"
#| results: "hide"
#| stan_save_output_files: "fit"
fit = model$sample(coin_data, seed = 24601L)
```

いろいろオプションはあるけど、ここではデフォルトに任せる:<br>
`chains`, `inits`, `iter_warmup`, `iter_samples`, `thin`, ...

問題があったら警告してくれるので**ちゃんと読む**。

---
## 説明変数なしのベイズ推定: 結果を眺める

`parameters` ブロックに書いた変数の情報が出てくる。<br>
乱数を使った計算なので(乱数シードを固定しない限り)毎回変わる。

```{r, stan-binom-fit}
#| echo: -1
summary_p = fit$summary("p") |> tibble::as_tibble()
print(fit)
```

真の値に近い $p \approx `r true_p`$ が得られた
(`r round(summary_p[["q5"]], 2)` から
`r round(summary_p[["q95"]], 2)` である確率が90%)。<br>
$\hat R$ もほぼ1で $N_\text{eff}$ も大きいのでよさそう。

`lp__` はlog posterior(対数事後確率)。後述。

念のため trace plot も確認しておこう→


---
## 説明変数なしのベイズ推定: trace plot 確認

どのchainも似た範囲を動いていて、しっかり毛虫っぽい:

```{r, stan-binom-traceplot}
#| fig.width: 11
#| fig.height: 4
draws = fit$draws()
params = names(model$variables()$parameters)
bayesplot::mcmc_trace(draws, pars = params)
```

---
## 説明変数なしのベイズ推定: 自己相関の確認

2--3ステップくらいで自己相関がほぼ消えるので問題なし:

```{r, stan-binom-ac}
#| fig.width: 6
#| fig.height: 5
bayesplot::mcmc_acf_bar(draws, pars = params)
```

---
## 説明変数なしのベイズ推定: 推定結果確認

サンプルサイズNが小さいせいか裾野の広い推定結果。<br>
真の$p$の値も含まれている:

```{r, stan-binom-hist}
#| fig.width: 4
#| fig.height: 4
bayesplot::mcmc_hist(draws, bins = 20, pars = params)
```


---
## `lp__`: log posterior とは?

`model` ブロックに次のように書いてあると:
```stan
model {
  mu ~ normal(0.0, 10.0);  // prior
  x ~ normal(mu, 1.0);     // likelihood
}
```

内部的には次のような処理が行われている:
```stan
target += normal_lpdf(theta | 0.0, 10.0)  // prior
target += normal_lpdf(x | theta, 1.0);    // likelihood
```

つまり、事前確率と尤度の対数の和を取っている。<br>
ベイズの定理により、事後確率はこれに比例する。<br>
`lp__` はこの `target` 変数を記録しておいたようなもの。


---
## Stanで回帰じゃないパラメータ推定、まとめ

別ファイルに書いておく。
e.g., `coin.stan`:

```{stan, binom}
#| cache_stan: true
data {
  int<lower=0> N;
  array[N] int x;
}
parameters {
  real<lower=0,upper=1> p;
}
model {
  x ~ binomial(1, p);
}
```

Rからデータを渡して走らせる:
```{r, stan-coin-binom}
#| cache_stan: "binom"
#| results: "hide"
#| stan_save_output_files: "coin_fit"
coin_data = tibble::lst(N = 50L, x = rbinom(N, 1, 0.7))
coin_model = cmdstanr::cmdstan_model("stan/binom.stan")
coin_fit = coin_model$sample(coin_data, seed = 24601L)
```


---
## 直線回帰するStanコードの例

受け渡しするデータや推定するパラメータがちょっと増えただけ。

```{stan, lm}
#| cache_stan: true
data {
  int<lower=0> N;
  vector<lower=0>[N] x;
  vector[N] y;
}

parameters {
  real intercept;
  real slope;
  real<lower=0> sigma;
}

model {
  y ~ normal(intercept + slope * x, sigma);
}
```

Rと同様、 `slope * x` のようなベクトル演算ができる。

---
## 直線回帰っぽいデータに当てはめてみる

```{r, df-lm}
#| echo: -1
set.seed(19937)
samplesize = 50L
df_lm = tibble::tibble(
  x = rnorm(samplesize, 1.70, 0.05),
  bmi = rnorm(samplesize, 22, 1),
  y = bmi * (x**2)
)
```

<img `r src_alt_fig_chunk("weight-lm")`>


---
## 操作は回帰じゃないモデルと同じ


```{r, stan-lm}
#| cache_stan: "lm"
#| stan_save_output_files: "lm_fit"
#| results: "hide"
# リストに入れて渡す:
lm_data = as.list(df_lm)
lm_data[["N"]] = samplesize
# モデルを実行速度の速い機械語に翻訳(コンパイル):
lm_model = cmdstanr::cmdstan_model("stan/lm.stan")
# モデルとデータを使ってMCMCサンプリング:
lm_fit = lm_model$sample(lm_data, seed = 19937L, refresh = 0)
```
```{r, print-stan-lm}
print(lm_fit)
```

切片と傾きはそれらしき値。
$\hat R$ や $N_{eff}$ も良さそう。
もう少し確認しよう。

---
## CmdStanによる診断

```{r, cmdstan-diagnose}
#| results: "hide"
lm_fit$cmdstan_diagnose()
```

satisfactory とか no problems ばかりであることを確認
```
Treedepth satisfactory for all transitions.

No divergent transitions found.

E-BFMI satisfactory.

Effective sample size satisfactory.

Split R-hat values satisfactory all parameters.

Processing complete, no problems detected.
```

---
## `draws`: 生のMCMCサンプル

```{r, draws-stan-lm}
lm_draws_array = lm_fit$draws()
dim(lm_draws_array)
print(lm_draws_array)
```

---
## `draws`: data.frameのほうが見やすいかも

```{r, draws-stan-lm-tibble}
lm_draws = lm_fit$draws(format = "df") |> print()
```

実体はCmdStanが書き出したCSVファイル:

```{r, fit-output-files}
#| results: "hide"
lm_fit$output_files()
```
```
[1] "/var/folders/**/***/T/Rtmp******/*-2023****-1-******.csv"
[2] "/var/folders/**/***/T/Rtmp******/*-2023****-2-******.csv"
[3] "/var/folders/**/***/T/Rtmp******/*-2023****-3-******.csv"
[4] "/var/folders/**/***/T/Rtmp******/*-2023****-4-******.csv"
```

---
## `traceplot`: サンプル順に `draws` を並べたもの

どの chain も同じところをうろうろしていればOK。

```{r, stan-lm-traceplot}
#| fig.width: 11
#| fig.height: 6
params = names(lm_model$variables()$parameters)
bayesplot::mcmc_trace(lm_draws, pars = params, facet_args = list(ncol = 1))
```

---
## 各パラメータの事後分布

```{r, stan-lm-hist}
#| fig.width: 11
#| fig.height: 5
bayesplot::mcmc_hist(lm_draws, pars = params, bins = 30)
```

---
## Posterior Predictive Checking (PPC)

サイズ $S$ のパラメータdrawsと $N$ 個の観察値から
$S \times N$ 行列の $y_{rep}$ を生成:

```{r, stan-lm-ppc}
#| echo: [2, 3, 4]
#| fig.width: 6
#| fig.height: 4.5
set.seed(19937)
mu_rep = lm_draws$intercept + lm_draws$slope %o% df_lm$x
yrep = mu_rep + rnorm(prod(dim(mu_rep)), 0, lm_draws$sigma)
bayesplot::ppc_intervals(y = df_lm[["y"]], yrep = yrep,
  x = df_lm[["x"]], prob = 0.5, prob_outer = 0.9)
bayesplot::ppc_ribbon(y = df_lm[["y"]], yrep = yrep,
  x = df_lm[["x"]], prob = 0.5, prob_outer = 0.9, y_draw = "points")
```

<http://mc-stan.org/bayesplot/reference/PPC-overview.html>


---
## 変数と[ブロック](6-bayesian.html#/37)をうまく使って可読性アップ

途中計算に名前をつけることでモデルが読みやすくなる:

```stan
model {
  vector[N] mu = intercept + slope * x;
  y ~ normal(mu, sigma);
}
```

`transformed parameters` ブロックに書くとさらに見通しがよくなる:

```stan
transformed parameters {
  vector[N] mu = intercept + slope * x;
}

model {
  y ~ normal(mu, sigma);
}
```

見た目が変わるだけでなくMCMCサンプルが記録されるようになる。

---
## drawsは嵩むが頭は使わずに済む

```{stan, lm-transformed}
#| cache_stan: true
#| echo: false
data {
  int<lower=0> N;
  vector<lower=0>[N] x;
  vector[N] y;
}

parameters {
  real intercept;
  real slope;
  real<lower=0> sigma;
}

transformed parameters {
  vector[N] mu = intercept + slope * x;
}

model {
  y ~ normal(mu, sigma);
}
```
```{r, stan-lmtr}
#| cache_stan: "lm-transformed"
#| stan_save_output_files: "lmtr_fit"
#| results: "hide"
lmtr_model = cmdstanr::cmdstan_model("stan/lm-transformed.stan")
lmtr_fit = lmtr_model$sample(lm_data, seed = 19937L, refresh = 0)
lmtr_draws = lmtr_fit$draws(format = "df") |> print()
```
```{r, stan-lmgr-draws}
#| echo: false
print(lmtr_draws, max_draws = 6, max_variables = 60, digits = 3)
```

この右側の `mu` 行列はさっき苦労して作った `mu_rep` と同じ。

ひょっとして `yrep` もStanで作れる？

---
## `generated quantities` ブロックで乱数生成

(`data` と `parameters` のブロックは同じなので省略)

```{stan, lm-generated}
#| cache_stan: true
#| echo: !expr -(1:12)
data {
  int<lower=0> N;
  vector<lower=0>[N] x;
  vector[N] y;
}

parameters {
  real intercept;
  real slope;
  real<lower=0> sigma;
}

transformed parameters {
  vector[N] mu = intercept + slope * x;
}

model {
  y ~ normal(mu, sigma);
}

generated quantities {
  array[N] real yrep = normal_rng(mu, sigma);
}
```

[`normal_rng()`](https://mc-stan.org/docs/functions-reference/normal-distribution.html)
のような乱数生成が使えるのは<br>
`generated quantities` ブロックだけ。

(`yrep` を `vector[N]` 型で作ろうとすると怒られる。)


---
## drawsはさらに嵩むがコードは美しくなった

```{r, stan-lm-generated}
#| cache_stan: "lm-generated"
#| stan_save_output_files: "lmgen_fit"
#| results: "hide"
lmgen_model = cmdstanr::cmdstan_model("stan/lm-generated.stan")
lmgen_fit = lmgen_model$sample(lm_data, seed = 19937L, refresh = 0)
lmgen_draws = lmgen_fit$draws(format = "df") |> print()
```
```{r, stan-lmgen-draws}
#| echo: false
print(lmgen_draws, max_draws = 6, max_variables = 120, digits = 3)
```

`yrep = lmgen_fit$draws("yrep", format = "matrix")`
を取り出したらあとは `bayesplot::ppc_*()` に渡すだけ。


---
## 観察値とは違うXを使ってPredictionすることも可能

観察値の外側とか、均等間隔とか `x_tilde` を好きに作って渡せる。

```stan
data {
  // ...
  int<lower=0> N_tilde
  vector[N_tilde] x_tilde;
}
// ...
generated quantities {
  array[N_tilde] real y_tilde = normal_rng(intercept + slope * x_tilde, sigma);
}
```

```{stan, lm-credible}
#| cache_stan: true
#| echo: false
data {
  int<lower=0> N;
  vector<lower=0>[N] x;
  vector[N] y;
  int<lower=0> N_tilde;
  vector[N_tilde] x_tilde;
}

parameters {
  real intercept;
  real slope;
  real<lower=0> sigma;
}

model {
  y ~ normal(intercept + slope * x, sigma);
}

generated quantities {
  array[N_tilde] real y_tilde = normal_rng(intercept + slope * x_tilde, sigma);
}
```

```{r, stan-lm-credible-prep}
#| include: false
#| cache_stan: "lm-credible"
#| stan_save_output_files: "fit_cred"
set.seed(24601)
n = 300L
a = 3
b = -3
df_pois = tibble::tibble(x = runif(n, 0.4, 1.7), y = rpois(n, exp(a * x + b)))
data_cred = as.list(df_pois)
data_cred[["N"]] = n
data_cred[["x_tilde"]] = seq(0.4, 1.7, 0.1)
data_cred[["N_tilde"]] = length(data_cred[["x_tilde"]])

model_cred = cmdstanr::cmdstan_model("stan/lm-credible.stan")
fit_cred = model_cred$sample(data_cred, seed = 19937L, refresh = 0, step_size = 0.1)
draws_cred = fit_cred$draws(format = "df") |> print()

.probs = c(lower = 0.025, median = 0.5, upper = 0.975)
df_quant = draws_cred |>
  dplyr::reframe(dplyr::across(starts_with("y_tilde"), quantile, probs = .probs)) |>
  dplyr::mutate(label = names(.probs)) |>
  tidyr::pivot_longer(!label, names_to = "x", values_to = "y") |>
  dplyr::mutate(x = data_cred[["x_tilde"]][readr::parse_number(x)]) |>
  tidyr::pivot_wider(names_from = label, values_from = y)
```

```{r, stan-lm-credible}
#| include: false
#| fig.height: 4.5
#| fig.width: 6
ggplot(df_pois) +
  aes(x) +
  geom_point(aes(y = y), shape = 16L, alpha = 0.5) +
  geom_ribbon(aes(ymin = lower, ymax = upper), data = df_quant, alpha = 0.3) +
  geom_line(aes(y = median), data = df_quant) +
  theme_bw(18)
```


---
## 変数の型: `vector` vs `array`

`vector`, `row_vector`, `matrix` は実数 `real` のみで、行列演算できる:

```stan
real x;
vector[3] v;
row_vector[3] r;
matrix[3, 3] m;

x * v  // vector[3]
r * v  // real
v * r  // matrix[3, 3]
m * v  // vector[3]
m * m  // matrix[3, 3]
m[1]   // row_vector[3]
```

`array` に型の制約は無いが、行列演算はできないので自力forループ:
```stan
array[3] int a;
array[3] int b;
for (i in 1:3) {
  b[i] = 2 * a[i] + 1
}
```

---
## パラメータの事前分布を明示的に設定できる

が、省略してもStanがデフォルトでうまくやってくれる。<br>
そのせいで収束が悪いかも、となってから考えても遅くない。

```stan
parameters {
  real intercept;
  real slope;
  real<lower=0> sigma;
}

model {
  y ~ normal(intercept + slope * x, sigma);
  intercept ~ normal(0, 100);
  slope ~ normal(0, 100);
  sigma ~ student_t(3, 0, 10);
}
```

設定したくなったら、どう選ぶか？

---
## 事前分布の選別

1.  とりあえず**無情報事前分布** $[-\infty, \infty]$。Stanのデフォルト。

1.  収束が悪かったら**弱情報事前分布**を試す。<br>
    事後分布を更新していったとき**事前分布っぽさが残らない**のが良い。

    - 取りうる値を逃すような狭すぎる分布はダメ。
    - 狭すぎるよりはマシだが、広すぎても良くない。
    - 一様分布 $[a, b]$ は一見無情報っぽくて良さそうだが、<br>
      事後分布に裾野が残ったり絶壁ができたりしがちなので微妙。

    おすすめ: [**正規分布**](https://mc-stan.org/docs/functions-reference/normal-distribution.html)
    or [**Student's t分布**](https://mc-stan.org/docs/functions-reference/student-t-distribution.html)

<cite><https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations></cite>


---
## Stanおすすめ弱情報事前分布: Student's t分布

Student's $t(\nu=\nu_0, \mu = 0, \sigma = \sigma_0)$

- 自由度 $3 \le \nu_0 \le 7 $ で適当に固定。
  - $\nu = 1$ でコーシー分布。裾野が広すぎて良くないらしい。
  - $\nu \to \infty$ で**正規分布**。だいたいこれでいいらしい。
- スケール $\sigma$: 「推定したい値は$[-\sigma_0, \sigma_0]$に収まるだろう」という値。

```{r, studentt}
#| echo: false
#| fig.height: 4
#| fig.width: 9
tidyr::crossing(x = seq(-5, 5, 0.05), df = c(1, 3, 7, Inf)) |>
  dplyr::mutate(nu = as.factor(df), Density = dt(x, df)) |>
  ggplot() + aes(x, Density, group = df) +
  geom_line(aes(color = nu), linewidth = 1, alpha = 0.7) +
  scale_color_viridis_d(end = 0.88, direction = 1, guide = guide_legend(reverse = TRUE)) +
  scale_x_continuous(breaks = c(-1, 0, 1), labels = c(expression(-sigma), 0, expression(sigma))) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank(),
        panel.grid.major.y = element_blank())
```


---
## 🔰 Stanで一般化線形モデル

[GLM回のデータ](4-glm.html#/15)をStanでモデリングしてみよう。

<div class="column-container">
  <div class="column" style="flex-shrink: 2.0;">

- 直線回帰
- ポアソン回帰
- ロジスティック回帰
- 重回帰
- 分散分析
- 共分散分析

  </div>
  <div class="column" style="flex-shrink: 1.0;">
<figure>
<img `r src_alt_fig_chunk("lm-bad")` height=240>
<img `r src_alt_fig_chunk("glm-poisson")` height=240>
<img `r src_alt_fig_chunk("glm-logistic")` height=240>
<img `r src_alt_fig_chunk("multiple-regression")` height=240>
<img `r src_alt_fig_chunk("glm-anova")` height=240>
<img `r src_alt_fig_chunk("glm-ancova")` height=240>
</figure>
  </div>
</div>


```{stan, poisson}
#| cache_stan: true
#| echo: false
data {
  int<lower=0> N;
  vector<lower=0>[N] x;
  array[N] int<lower=0> y;
}

parameters {
  real intercept;
  real slope;
}

model {
  vector[N] lambda = exp(intercept + slope * x);
  y ~ poisson(lambda);
}
```


```{stan, multiple}
#| cache_stan: true
#| echo: false
data {
  int<lower=0> N;
  vector[N] temperature;
  vector[N] humidity;
  array[N] int<lower=0> beer_sales;
}

parameters {
  real intercept;
  real coef_t;
  real coef_h;
}

model {
  vector[N] lambda = exp(intercept + coef_t * temperature + coef_h * humidity);
  beer_sales ~ poisson(lambda);
}
```


```{stan, logistic}
#| cache_stan: true
#| echo: false
data {
  int<lower=0> N;
  int<lower=0> n_trials;
  vector[N] temperature;
  array[N] int<lower=0,upper=n_trials> beer_sales;
}

parameters {
  real intercept;
  real slope;
}

model {
  vector[N] p = inv_logit(intercept + slope * temperature);
  beer_sales ~ binomial(n_trials, p);
}
```

```{stan, anova}
#| cache_stan: true
#| echo: false
data {
  int<lower=0> N;
  vector<lower=0,upper=1>[N] sunny;
  vector<lower=0,upper=1>[N] rainy;
  vector<lower=0>[N] beer_sales;
}

parameters {
  real intercept;
  real coef_s;
  real coef_r;
  real sigma;
}

model {
  vector[N] mu = intercept + coef_s * sunny + coef_r * rainy;
  beer_sales ~ normal(mu, sigma);
}
```

```{stan, ancova}
#| cache_stan: true
#| echo: false
data {
  int<lower=0> N;
  vector<lower=0,upper=1>[N] sunny;
  vector<lower=0,upper=1>[N] rainy;
  vector<lower=0>[N] temperature;
  vector<lower=0>[N] beer_sales;
}

parameters {
  real intercept;
  real coef_s;
  real coef_r;
  real coef_t;
  real sigma;
}

model {
  vector[N] mu = intercept +
                 coef_s * sunny +
                 coef_r * rainy +
                 coef_t * temperature;
  beer_sales ~ normal(mu, sigma);
}
```


---
## 🔰 Stanでpenguinsの回帰分析をしてみよう

<a href="https://allisonhorst.github.io/palmerpenguins/">
<cite>https://allisonhorst.github.io/palmerpenguins/</cite><br>
<img src="/slides/image/rstats/lter_penguins.png" width="45%">
<img src="/slides/image/rstats/culmen_depth.png" width="45%">
</a>

<img `r src_alt_fig_chunk("penguins-interaction")` height="300">

[第4回GLM回](4-glm.html#/32)を参照。

---
## 🔰 Stanでpenguinsの回帰分析をしてみよう

<a href="https://allisonhorst.github.io/palmerpenguins/">
<cite>https://allisonhorst.github.io/palmerpenguins/</cite><br>
<img src="/slides/image/rstats/lter_penguins.png" width="45%">
<img src="/slides/image/rstats/culmen_depth.png" width="45%">
</a>

`Stan does not support NA` と怒られるので欠損値を取り除いておく:



```{r, penguins-dropna-stan}
#| echo: 3
withr::local_package("palmerpenguins")
penguins_colors = c(Adelie = "darkorange", Chinstrap = "purple", Gentoo = "cyan4")
penguins_dropna = penguins |> tidyr::drop_na(body_mass_g)
```

```{stan, penguins-lm}
#| cache_stan: true
#| echo: false
data {
  int<lower=0> N;
  vector<lower=0>[N] body_mass_g;
  vector<lower=0>[N] flipper_length_mm;
}

parameters {
  real intercept;
  real slope;
  real<lower=0> sigma;
}

model {
  flipper_length_mm ~ normal(intercept + slope * body_mass_g, sigma);
}
```

```{stan, penguins-multiple}
#| cache_stan: true
#| echo: false
data {
  int<lower=0> N;
  vector<lower=0>[N] body_mass_g;
  vector<lower=0>[N] flipper_length_mm;
  array[N] int<lower=0,upper=1> sp_Chinstrap;
  array[N] int<lower=0,upper=1> sp_Gentoo;
}

parameters {
  real intercept;
  real slope;
  real b_chinstrap;
  real b_gentoo;
  real<lower=0> sigma;
}

model {
  array[N] real mu;
  for (i in 1:N) {
    mu[i] = intercept + slope * body_mass_g[i] + b_chinstrap * sp_Chinstrap[i] + b_gentoo * sp_Gentoo[i];
  }
  flipper_length_mm ~ normal(mu, sigma);
}
```




---
## ちょっとずつ線形モデルを発展させていく

<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="400" alt="データ解析のための統計モデリング入門 久保拓弥 2012">
</a>
</figure>

久保先生の"緑本"こと<br>
「[データ解析のための統計モデリング入門](https://kuboweb.github.io/-kubo/ce/IwanamiBook.html)」<br>
をベースに回帰分析の概要を紹介。

**線形モデル LM** (単純な直線あてはめ)

<span style="color: #888888;">&nbsp; &nbsp; ↓ いろんな<span style="font-weight: bold; color: #56B4E9;">確率分布</span>を扱いたい</span>

**一般化線形モデル GLM**

<span style="color: #888888;">&nbsp; &nbsp; ↓ <span style="font-weight: bold; color: #E69F00;">個体差</span>などの変量効果を扱いたい</span>

**一般化線形混合モデル GLMM**

<span style="color: #888888;">&nbsp; &nbsp; ↓ もっと<span style="font-weight: bold; color: #B2001D;">自由なモデリング</span>を！</span>

**階層ベイズモデル HBM**


---
## GLMMで登場した個体差を階層ベイズモデルで

植物100個体から8個ずつ種子を取って植えたら全体で半分ちょい発芽。<br>
親1個体あたりの生存数は<span style="color: #56B4E9;">n=8の二項分布</span>になるはずだけど、<br>
極端な値(全部死亡、全部生存)が多かった。個体差？

<img `r src_alt_fig_chunk("overdispersion")`>


---
## 個体差をモデルに組み込みたい

各個体の生存率$p_i$が能力値$z_i$のシグモイド関数で決まると仮定。<br>
その能力値は全個体共通の正規分布に従うと仮定:
$z_i \sim \mathcal{N}(\hat z, \sigma)$

<img `r src_alt_fig_chunk("sigmoid")`>

パラメータ2つで済む: 平均 $\hat z$, ばらつき $\sigma$ 。


---
## 個体能力のばらつき $\sigma$ が大きいと両端が増える

普通の二項分布は個体差無し $\sigma = 0$ を仮定してるのと同じ。

<img `r src_alt_fig_chunk("alter-sigma-z")`>

<img `r src_alt_fig_chunk("alter-sigma-z", number = 2)`>

---
## 階層ベイズモデルのイメージ図

事前分布のパラメータに、さらに事前分布を設定するので階層ベイズ

<figure>
<img src="../tokiomarine2022/hbm.drawio.svg" width="800">
</figure>


---
## さっきの図をStan言語で記述すると

`10` とか `3` とか、エイヤっと決めてるやつが超パラメータ。

```{stan, glmm}
#| cache_stan: true
data {
  int<lower=0> N;
  array[N] int<lower=0> y;
}

parameters {
  real z_hat;           // mean ability
  real<lower=0> sigma;  // sd of r
  vector[N] r;          // individual difference
}

transformed parameters {
  vector[N] z = z_hat + r;
  vector[N] p = inv_logit(z);
}

model {
  y ~ binomial(8, p);
  z_hat ~ normal(0, 10);
  r ~ normal(0, sigma);
  sigma ~ student_t(3, 0, 1);
}
```

---
## 変量効果が入った推定結果

```{r, df-seeds-od-8}
#| echo: false
set.seed(24601)
sigmoid = function(x, gain = 1) {1 / (1 + exp(-gain * x))}
samplesize = 100L
df_seeds_od = tibble::tibble(
  z = rnorm(samplesize, 0.5, 3),
  p = sigmoid(z),
  y = rbinom(samplesize, 8L, p))
```
```{r, stan-glmm-prep}
#| cache_stan: "glmm"
#| stan_save_output_files: "fit"
#| results: "hide"
seeds_data = list(y = df_seeds_od$y, N = samplesize)
model = cmdstanr::cmdstan_model("stan/glmm.stan")
fit = model$sample(data = seeds_data, seed = 19937L, step_size = 0.1, refresh = 0)
draws = fit$draws(c("z_hat", "sigma", "r[1]", "r[2]"))
```

```{r, stan-glmm-print}
#| echo: false
#| cache.globals: "fit"
print(fit)
```

---
## 抜粋して作図。悪くない。

データ生成の真のパラメータ値は $\hat z = 0.5,~\sigma = 3.0$ だった。

```{r, stan-glmm}
#| echo: false
#| fig.width: 11
#| fig.height: 6
p_trace = bayesplot::mcmc_trace(draws, facet_args = list(nrow = 1L)) + coord_flip()
p_hist = bayesplot::mcmc_hist(draws, facet_args = list(nrow = 1L), bins = 30)
cowplot::plot_grid(p_trace, p_hist, ncol = 1L)
```


---
## 🔰 階層ベイズモデルの練習問題: 種の数

100個体の植物から8つずつ種を取った、のデータでやってみよう。

```{r, df-seeds-generate}
#| ref.label: "df-seeds-od-8"
#| echo: -1
```

<img `r src_alt_fig_chunk("overdispersion")`>


---
## 🔰 階層ベイズモデルの練習問題: ビール注文数
<!-- TODO: 時間が余った場合の練習問題 -->

```{r, beer-overdispersion-data-re}
#| echo: -1
set.seed(24601)
samplesize = 300L
lambda = 3
overdisp = 4
.n = lambda / (overdisp - 1)
.p = 1 / overdisp
df_beer_od = tibble::tibble(
  X = rnbinom(samplesize, size = .n, prob = .p)
)
```

<img `r src_alt_fig_chunk("beer-overdispersion")`>


---
## ベイズ推定まとめ

- 条件付き確率 $\text{Prob}(B \mid A)$ の理解が大事。
    - 事後分布 $\propto$ 尤度 ⨉ 事前分布
    - 確信度合いをデータで更新していく。
- 推定結果は分布そのもの。
    - そこから点推定も区間推定も可能。
- 解析的に解けない問題は計算機に乱数を振らせて解く。
    - MCMCサンプル $\sim$ 解きにくい事後分布
    - 理論・技術の進歩が目覚ましい。


---
## 回帰分析ふりかえり

より柔軟にモデルを記述できるようになった。計算方法も変化。

<figure>
<a href="https://kuboweb.github.io/-kubo/ce/LinksGlm.html">
<img src="../tokiomarine2021/image/kubo-p2.png" width="1100">
<figcaption class="url">久保さん https://kuboweb.github.io/-kubo/ce/LinksGlm.html</figcaption>
</a>
</figure>


---
## 全体まとめ

- 統計とは、データをうまくまとめ、それに基づいて推論するための手法。
- モデルには**理解志向**と**応用志向**があり、統計モデルは前者寄り。
    - どちらも多少は分かった上で使い分けたい。
    - どっちにしろ真の正しい何かではない。
- **確率分布**とその背後にある**確率過程**の理解が重要。
    - 乱数生成→作図を繰り返してイメージを掴もう。
    - MCMCサンプリングも事後分布からの乱数生成。
- 本講義で「統計モデリングを完全に理解した」とは言えない。
    - 理論も実践もほとんど説明していない。
    - 本を読む準備ができた、くらいの気持ち？


---
## 参考文献

- [データ解析のための統計モデリング入門](https://amzn.to/33suMIZ) 久保拓弥 2012
- [StanとRでベイズ統計モデリング](https://amzn.to/3uwx7Pb) 松浦健太郎 2016
- [RとStanではじめる ベイズ統計モデリングによるデータ分析入門](https://amzn.to/3o1eCzP) 馬場真哉 2019
- [データ分析のための数理モデル入門](https://amzn.to/3uCxTKo) 江崎貴裕 2020
- [分析者のためのデータ解釈学入門](https://amzn.to/3uznzCK) 江崎貴裕 2020
- [統計学を哲学する](https://amzn.to/3ty80Kv) 大塚淳 2020

<a href="." class="readmore">
目次に戻る
</a>
