+++
url = "tmd2022stats/3-likelihood.html"
title = "3. 尤度、最尤推定 — 統計モデリング実習 2022 TMDU"
linktitle = "尤度、最尤推定"
date = 2023-03-18T13:00:00+09:00
type = "reveal"
draft = false
+++

<link rel="stylesheet" href="style.css">

# [統計モデリング実習 2022 TMDU](.)

<div class="author">
岩嵜 航 (Watal M. Iwasaki, PhD)
</div>

<div class="affiliation">
東北大学 生命科学研究科 進化ゲノミクス分野 特任助教<br>
(Graduate School of Life Sciences, Tohoku University)
</div>

<ol>
<li><a href="1-introduction.html">導入、直線回帰</a>
<li><a href="2-distribution.html">確率分布、擬似乱数生成</a>
<li class="current-deck"><a href="3-likelihood.html">尤度、最尤推定</a>
<li><a href="4-glm.html">一般化線形モデル (GLM)</a>
<li><a href="5-glmm.html">個体差、一般化線形混合モデル (GLMM)</a>
<li><a href="6-bayesian.html">ベイズの定理、事後分布、MCMC</a>
<li><a href="7-stan.html">StanでGLM</a>
<li><a href="8-hbm.html">階層ベイズモデル (HBM)</a>
</ol>

<div class="footnote">
2023-03-18 東京医科歯科大学
<a href="https://heavywatal.github.io/slides/tmd2022stats/">https://heavywatal.github.io/slides/tmd2022stats/</a>
</div>

```{r, setup-common}
#| file: "setup.R"
#| include: false
```

```{r, setup-local}
#| include: false
```


---
## 本講義のお品書き

<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="300" alt="データ解析のための統計モデリング入門 久保拓弥 2012">
</a>
</figure>

久保先生の"緑本"こと<br>
「データ解析のための統計モデリング入門」<br>
をベースに回帰分析の概要を紹介。

1. イントロ
1. Rの基礎を駆け足で
1. 統計モデルの基本
    - 直線回帰
    - 確率変数・**確率分布** 👈 ここまでやった
    - 尤度・最尤推定
1. 一般化線形モデル、混合モデル
1. ベイズ統計、階層ベイズモデル

回帰のキモは**線ではなく分布**


---
## 有名な確率分布対応関係ふりかえり

<figure style="float: right;">
<img src="../tokiomarine2021/math-model.drawio.svg" width="300"><br>
</figure>

離散一様分布
: コインの表裏、サイコロの出目1–6

幾何分布
: 成功率pの試行が初めて成功するまでの失敗回数

二項分布
: 成功率p、試行回数nのうちの成功回数

ポアソン分布
: 単位時間あたり平均$\lambda$回起こる事象の発生回数

ガンマ分布
: ポアソン過程でk回起こるまでの待ち時間
: (k = 1のとき**指数分布**と呼ばれる)

正規分布
: 確率変数の和、平均値。使い勝手が良く、よく登場する。


---
## データに分布をあてはめたい

ある植物を50個体調べて、それぞれの種子数Xを数えた。<br>
個体Aは種2個、個体Bは種4個、、、サンプルサイズ n = 50 のデータ。

```{r, df-poisson}
#| echo: false
set.seed(24601)
df_rpois = tibble::tibble(X = rpois(50L, 3))
max_x = 11L
df_dpois = purrr::map_dfr(c(1, 3, 5), ~ {
  tibble(lambda = .x, X = seq.int(0L, max_x), Prob = dpois(X, lambda))
})
```
```{r, poisson-seed}
#| echo: false
#| fig.height: 4
#| fig.width: 4
ggplot(df_rpois) +
  aes(X) +
  geom_bar(width = 0.4) +
  coord_cartesian(xlim = c(0, max_x)) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
    axis.ticks = element_blank()
  )
```

カウントデータだから<span class="fragment custom blur">ポアソン</span>分布っぽい。<br>
分布のパラメータ $\lambda$ はどれくらいがいいだろう？


---
## データに分布をあてはめたい

ある植物を50個体調べて、それぞれの種子数Xを数えた。<br>
個体Aは種2個、個体Bは種4個、、、サンプルサイズ n = 50 のデータ。

```{r, poisson-seed-lambda}
#| echo: false
#| fig.height: 4
#| fig.width: 11
p_pois = ggplot(df_rpois) +
  aes(X) +
  geom_bar(aes(y = after_stat(prop)), width = 0.4) +
  geom_col(data = df_dpois, aes(y = Prob), alpha = 0.5, fill = "#3366ff") +
  facet_wrap(vars(lambda), nrow = 1L, labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(), axis.ticks = element_blank()
  )
p_pois
```

カウントデータだからポアソン分布っぽい。<br>
分布のパラメータ $\lambda$ はどれくらいがいいだろう？

黒が観察データ。<span style="color: #3366ff;">青がポアソン分布</span>。
よく重なるのは $\lambda \approx 3$ くらいか。


---
## <ruby>尤<rt>ゆう</rt>度</ruby> (likelihood)

<ruby>尤<rt>もっと</rt></ruby>もらしさ。
モデルのあてはまりの良さの尺度のひとつ。

**あるモデル$M$の下でそのデータ$D$が観察される確率**。<br>
定義通り素直に書くと<br>
$\text{Prob}(D \mid M)$

データ$D$を固定し、モデル$M$の関数とみなしたものが**尤度関数**:<br>
$L(M \mid D)$

モデルの構造も固定してパラメータ$\theta$だけ動かす場合はこう書く:<br>
$L(\theta \mid D)$ とか $L(\theta)$ とか


---
## 尤度を手計算できる例

コインを5枚投げた結果 $D$: 表 4, 裏 1

表が出る確率 $p = 0.5$ と仮定:
<div>\[\begin{split}
L(0.5 \mid D)
  &= \binom 5 1 \times \text{Prob}(表 \mid 0.5) ^ 4 \times \text{Prob}(裏 \mid 0.5) ^ 1 \\
  &= 5 \times 0.5 ^ 4 \times 0.5 ^ 1 = 0.15625
\end{split}\]</div>

表が出る確率 $p = 0.8$ と仮定:
<div>\[\begin{split}
L(0.8 \mid D)
  &= \binom 5 1 \times \text{Prob}(表 \mid 0.8) ^ 4 \times \text{Prob}(裏 \mid 0.8) ^ 1 \\
  &= 5 \times 0.8 ^ 4 \times 0.2 ^ 1 = 0.4096
\end{split}\]</div>

$L(0.8 \mid D) > L(0.5 \mid D)$

$p = 0.8$ のほうがより尤もらしい。



---
## 種子数ポアソン分布の例でも尤度を計算してみる

ある植物が作った種子を数える。$n = 50$個体ぶん。

<div>\[\begin{split}
L(\lambda \mid D)
  = \prod _i ^n \text{Prob}(X_i \mid \lambda)
  = \prod _i ^n \frac {\lambda ^ {X_i} e ^ {-\lambda}} {X_i !}
\end{split}\]</div>

```{r, poisson-seed-likelihood}
#| echo: false
#| fig.height: 4
#| fig.width: 11
df_likelihood = df_rpois |>
  dplyr::left_join(df_dpois, by = "X") |>
  dplyr::group_by(lambda) |>
  dplyr::summarize(L = prod(Prob)) |>
  dplyr::mutate(logL = log(L), label = sprintf("L(%.0f|D) = %.1e", lambda, L))
p_pois +
  geom_text(data = df_likelihood, aes(label = label), color = "#3366ff",
            x = Inf, y = Inf, hjust = 1.1, vjust = 1.3, size = 6)
```

この中では $\lambda = 3$ がいいけど、より尤もらしい値を求めたい。

---
## 最尤推定 <u>M</u>aximum <u>L</u>ikelihood <u>E</u>stimation

扱いやすい **対数尤度** (log likelihood) にしてから計算する。<br>
一階微分が0になる $\lambda$ を求めると...**標本平均**と一致。

<div>\[\begin{split}
\log L(\lambda \mid D)
  &= \sum _i ^n \left[ X_i \log (\lambda) - \lambda - \log (X_i !) \right] \\
\frac {\mathrm d \log L(\lambda \mid D)} {\mathrm d \lambda}
  &= \frac 1 \lambda \sum _i ^n X_i - n = 0 \\
\hat \lambda &= \frac 1 n \sum _i ^n X_i
\end{split}\]</div>


```{r, poisson-mle}
#| echo: false
#| fig.height: 3
#| fig.width: 10
count_rpois = df_rpois |> dplyr::count(X)
calc_likelihood_rpois = function(lambda) {
  prod(dpois(count_rpois[["X"]], lambda)**count_rpois[["n"]])
}
X_mle = mean(df_rpois[["X"]])
L_mle = calc_likelihood_rpois(X_mle)
p_mle = tibble::tibble(lambda = seq(1, 5, 0.1), L = purrr::map_dbl(lambda, calc_likelihood_rpois)) |>
  dplyr::mutate(logL = log(L)) |>
  ggplot() +
  aes(lambda, logL) +
  geom_line() +
  geom_vline(xintercept = X_mle, color = "#3366ff") +
  annotate("point", x = X_mle, y = log(L_mle), size = 3, color = "#3366ff") +
  annotate("text", label = sprintf("lambda = %.2f", X_mle), color = "#3366ff",
           x = Inf, y = Inf, hjust = 1.1, vjust = 1.2, size = 5) +
  labs(y = "log L") +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank()
  )

df_dpois = tibble(lambda = X_mle, X = seq(0, 11), Prob = dpois(X, lambda))
p_pois = ggplot(df_rpois) +
  aes(X) +
  geom_bar(aes(y = after_stat(prop)), width = 0.4) +
  geom_col(data = df_dpois, aes(y = Prob), alpha = 0.5, fill = "#3366ff") +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(), axis.ticks = element_blank()
  )

cowplot::plot_grid(p_mle, p_pois, nrow = 1L, rel_widths = c(2, 1))
```

---
## 最尤推定を使っても“真のλ”は得られない

今回のデータは真の生成ルール“$X \sim \text{Poisson}(\lambda = 3.0)$”で作った。<br>
「50個体サンプル→最尤推定」を1,000回繰り返してみると:

```{r, poisson-mle-repl}
#| echo: false
#| fig.height: 5
#| fig.width: 11
nrep = 1000L
df_repl = tibble::tibble(X = rpois(50L * nrep, 3), repl = rep(seq_len(nrep), each = 50L))
df_sum = df_repl |>
  dplyr::group_by(repl) |>
  dplyr::summarize(lambda = mean(X))
df_minmax = dplyr::bind_rows(dplyr::slice_max(df_sum, lambda), dplyr::slice_min(df_sum, lambda))

p_repl = df_sum |>
  ggplot() +
  aes(lambda) +
  geom_histogram(bins = 25, center = 3) +
  annotate("point", x = df_minmax$lambda, y = 0, color = "#3366ff", size = 8, alpha = 0.5) +
  labs(x = "estimated_lambda") +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank()
  )

df_dpois = tidyr::crossing(lambda = df_minmax$lambda, X = seq(0, 11)) |>
  dplyr::mutate(Prob = dpois(X, lambda))
p_minmax = df_repl |>
  dplyr::filter(repl %in% df_minmax$repl) |>
  dplyr::left_join(df_minmax, by = "repl") |>
  ggplot() +
  aes(X) +
  geom_bar(aes(y = after_stat(prop)), width = 0.4) +
  geom_col(data = df_dpois, aes(y = Prob), alpha = 0.5, fill = "#3366ff") +
  facet_wrap(vars(lambda), ncol = 1L, labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(), axis.ticks = element_blank()
  )

cowplot::plot_grid(p_repl, p_minmax, nrow = 1L, rel_widths = c(2, 1))
```

サンプルの取れ方によってはかなりズレた推定をしてしまう。<br>
(標本データへのあてはまりはかなり良く見えるのに！)


---
## サンプルサイズを増やすほどマシにはなる

“$X \sim \text{Poisson}(\lambda = 3.0)$”からnサンプル→最尤推定を1,000回繰り返す:

```{r, poisson-mle-nsam}
#| echo: false
#| fig.height: 4
#| fig.width: 11
nrep = 1000L
purrr::map_dfr(c(5, 50, 500, 5000), ~ {
  tibble::tibble(X = rpois(.x * nrep, 3), repl = rep(seq_len(nrep), each = .x)) |>
    dplyr::group_by(repl) |>
    dplyr::summarize(estimated_lambda = mean(X)) |>
    dplyr::mutate(n = .x)
}) |>
  ggplot() +
  aes(estimated_lambda) +
  geom_histogram(bins = 25, center = 3) +
  facet_wrap(vars(n), nrow = 1L, labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank()
  )
```

Q. じゃあどれくらいのサンプル数nを確保すればいいのか？<br>
A. 推定したい統計量とか、許容できる誤差とかによる。


---
## すべてのモデルは間違っている

確率分布がいい感じに最尤推定できたとしても、<br>
それはあくまでモデル。仮定。近似。

> All models are wrong, but some are useful. --- George E. P. Box

<figure>
<img src="../tokiomarine2021/math-model.drawio.svg" width="600"><br>
<figcaption><cite>「データ分析のための数理モデル入門」江崎貴裕 2020 より改変</cite></figcaption>
</figure>


---
## 統計モデリングの道具 --- まとめ

- **確率変数** $X$
- **確率分布** $X \sim f(\theta)$
    - **少ないパラメータ** $\theta$ でばらつきの様子を表現
    - **この現象はこの分布を作りがち(〜に従う)** という知見がある
- **尤度**
    - あるモデルでこのデータになる確率 $\text{Prob}(D \mid M)$
    - データ固定でモデル探索 → **尤度関数** $L(M \mid D),~L(\theta \mid D)$
    - 対数を取ったほうが扱いやすい → **対数尤度** $\log L(M \mid D)$
    - これを最大化するようなパラメータ $\hat \theta$ 探し ＝ **最尤法**


---
## 🔰 尤度の練習問題

サイコロを10回振ったら6の目が3回出た。

1. 6の目の出る確率が1/6だとした場合の尤度は?
1. 6の目の出る確率が0.2だとした場合の尤度は?
1. 横軸を6の目の出る確率、縦軸を対数尤度とするグラフを描こう。
1. このサイコロで6の目が出る確率を最尤推定しよう。<br>
   数学で解ければ**優**。Rで見つければ**良**。目分量・勘で**可**。

ヒント
: 確率pで当たるクジをn回引いてk回当たる確率、と同じ計算を使う。


```{r, get-likelihood}
#| include: false
get_likelihood = function(p, k = 3, n = 10) {
  choose(n, k) * p**k * (1 - p)**(n - k)
  # dbinom(k, n, p)
}

get_likelihood(0.2)
```

---
## 🔰 分布を当てはめる練習問題

1. データの分布を描いてみる
1. 理論分布のどれが当てはまりそうか検討する
1. 理論分布を適当なパラメータで描いてみる
1. 尤度を計算しつつ擦り寄せる

```{r, distribution-datasets}
#| include: false
fs::dir_create("../data")

set.seed(19937)
n = 600L
rate = 2.5
df = tibble::tibble(
  time = cumsum(rexp(n, rate)),
  label = sample(LETTERS[1:3], n, replace = TRUE)
)
readr::write_tsv(df, "../data/radiation.tsv")

set.seed(19937)
n = 600L
trials = 10
p = 0.1
df = tibble::tibble(
  trials,
  hit = rbinom(n, trials, p)
)
readr::write_csv(df, "../data/gacha.csv")
```

<div class="column-container">
  <div class="column">

[`radiation.tsv`](data/radiation.tsv)
```{r, radiation-tsv}
#| echo: false
readr::read_tsv("../data/radiation.tsv")
```

  </div>
  <div class="column">

[`gacha.csv`](data/gacha.csv)
```{r, gacha-csv}
#| echo: false
readr::read_csv("../data/gacha.csv")
```

  </div>
</div>



---
## 参考文献

- [データ解析のための統計モデリング入門](https://amzn.to/33suMIZ) 久保拓弥 2012
- [StanとRでベイズ統計モデリング](https://amzn.to/3uwx7Pb) 松浦健太郎 2016
- [RとStanではじめる ベイズ統計モデリングによるデータ分析入門](https://amzn.to/3o1eCzP) 馬場真哉 2019
- [データ分析のための数理モデル入門](https://amzn.to/3uCxTKo) 江崎貴裕 2020
- [分析者のためのデータ解釈学入門](https://amzn.to/3uznzCK) 江崎貴裕 2020
- [統計学を哲学する](https://amzn.to/3ty80Kv) 大塚淳 2020

<a href="4-glm.html" rel="next" class="readmore">
4. 一般化線形モデルGLM
</a>
