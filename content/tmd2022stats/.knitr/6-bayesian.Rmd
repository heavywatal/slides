+++
url = "tmd2022stats/6-bayesian.html"
title = "6. ベイズ推定とMCMC — 統計モデリング実習 2022 TMDU"
linktitle = "ベイズ推定とMCMC"
date = 2023-03-25T14:40:00+09:00
type = "reveal"
draft = false
+++

<link rel="stylesheet" href="style.css">

# [統計モデリング実習 2022 TMDU](.)

<div class="author">
岩嵜 航 (Watal M. Iwasaki, PhD)
</div>

<div class="affiliation">
東北大学 生命科学研究科 進化ゲノミクス分野 特任助教<br>
(Graduate School of Life Sciences, Tohoku University)
</div>

<ol>
<li><a href="1-introduction.html">導入、直線回帰</a>
<li><a href="2-distribution.html">確率分布、擬似乱数生成</a>
<li><a href="3-likelihood.html">尤度、最尤推定</a>
<li><a href="4-glm.html">一般化線形モデル (GLM)</a>
<li><a href="5-glmm.html">個体差、一般化線形混合モデル (GLMM)</a>
<li class="current-deck"><a href="6-bayesian.html">ベイズ推定とMCMC</a>
<li><a href="7-stan.html">StanでGLM</a>
<li><a href="8-hbm.html">階層ベイズモデル (HBM)</a>
</ol>

<div class="footnote">
2023-03-25 東京医科歯科大学
<a href="https://heavywatal.github.io/slides/tmd2022stats/">https://heavywatal.github.io/slides/tmd2022stats/</a>
</div>

```{r, setup-global}
#| file: "setup.R"
#| message: !expr 'NA'
#| include: false
```

```{r, setup-local}
#| include: false
library(ggplot2)
# to suppress "warning: jobserver unavailable: using -j1" from cmdstanr
Sys.setenv(MAKEFLAGS = "-j1")
library(cmdstanr)
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE)
```

```{r}
#| label: label
ls()
```

---
## コイントス4回、たまたま表が1回だったら

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0;">

最尤推定
: 推定結果は最も尤もらしい1点。
: データが少ないとき過剰適合気味。
: 表が出る確率 p = 0.25 のコインだろう。<br>
  (信じ難いけどデータはこう言っている)

<br>

ベイズ推定
: 推定結果は確率分布そのもの。
: データが少ないなりの不確実さも表現。
: p = 0.25 らへんである確率は高いが、<br>
  p = 0.6 とかである可能性もまあある。

  </div>
  <div class="column" style="flex-shrink: 1.4;">

```{r, freq-vs-bayes}
#| echo: false
#| fig.height: 4
#| fig.width: 4
tibble::tibble(p = seq(0, 1, 0.01), logLik = dbinom(1, 4, p, log = TRUE)) |>
  dplyr::filter(logLik > -1e6) |>
  ggplot() + aes(p, logLik) +
  geom_line() +
  geom_point(data = function(.x) {dplyr::slice_max(.x, logLik)}, shape = 16, size = 5, color = palette()[3]) +
  geom_vline(data = function(.x) {dplyr::slice_max(.x, logLik)}, aes(xintercept = p), color = palette()[3]) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())

tibble::tibble(p = seq(0, 1, 0.01), Density = dbeta(p, 2, 4)) |>
  ggplot() + aes(p, Density) +
  geom_area(fill = palette()[3], alpha = 0.5) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
```

  </div>
</div>


---
## コイントスの回数が増えていったら

**最尤推定**: 推定値が真の値に近づいていく

```{r, df-coin-beta}
#| echo: false
set.seed(24601)
coin_X = c(0L, 0L, 0L, 1L, rbinom(496L, 1L, 0.5))
coin_show = c(4, 20, 100, 500)
df_coin = purrr::map_dfr(c(0, 1, 2, 3, coin_show), ~{
  k = sum(head(coin_X, .x))
  tibble::tibble(
    i = .x,
    label = sprintf("%d / %d", k, .x),
    p = seq(0, 1, 0.01),
    logLik = dbinom(k, .x, p, log = TRUE),
    Density = dbeta(p, k + 1, .x - k + 1))
}) |>
  dplyr::mutate(label = factor(label, levels = unique(label)))
```
```{r, coin-frequentist}
#| echo: false
#| fig.width: 11
#| fig.height: 3
df_coin |>
  dplyr::filter(i %in% coin_show) |>
  dplyr::filter(logLik > -20) |>
  group_by(label) |>
  ggplot() + aes(p, logLik) +
  geom_line() +
  geom_point(data = function(.x) {dplyr::slice_max(.x, logLik)}, shape = 16, size = 5, color = palette()[3]) +
  geom_vline(data = function(.x) {dplyr::slice_max(.x, logLik)}, aes(xintercept = p), color = palette()[3]) +
  facet_wrap(vars(label), nrow = 1L) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
```

**ベイズ推定**: 確率分布がどんどん尖り、確信が強まる

```{r, coin-bayesian}
#| echo: false
#| fig.width: 11
#| fig.height: 3
df_coin |>
  dplyr::filter(i %in% coin_show) |>
  ggplot() + aes(p, Density) +
  geom_area(fill = palette()[3], alpha = 0.5) +
  facet_wrap(vars(label), nrow = 1L) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
```

---
## 確率おさらい

同時分布/結合確率: <span style="font-weight: normal;"> <span style="color: #E69F00;">A</span>かつ<span style="color: #0072B2;">B</span>の確率</span>
: $\text{Prob}(\textcolor{#E69F00}{A}, \textcolor{#0072B2}{B}) = \text{Prob}(\textcolor{#E69F00}{A} \cap \textcolor{#0072B2}{B}) = \text{Prob}(\textcolor{#E69F00}{A}) \text{Prob}(\textcolor{#0072B2}{B})$

周辺確率: <span style="font-weight: normal;"> <span style="color: #0072B2;">B</span>によらず<span style="color: #E69F00;">A</span>になる確率</span>
: $\text{Prob}(\textcolor{#E69F00}{A}) = \sum_{\textcolor{#0072B2}{B}} \text{Prob}(\textcolor{#E69F00}{A}, \textcolor{#0072B2}{B})$

条件付き確率: <span style="font-weight: normal;"> <span style="color: #0072B2;">B</span>である条件の下で<span style="color: #E69F00;">A</span>になる確率。重要。</span>
: $\text{Prob}(\textcolor{#E69F00}{A} \mid \textcolor{#0072B2}{B}) = \frac {\text{Prob}(\textcolor{#E69F00}{A}, \textcolor{#0072B2}{B})} {\text{Prob}(\textcolor{#0072B2}{B})}$

```{r, venn}
#| echo: false
#| fig.width: 6
#| fig.height: 3
make_circle = function(radius, center, n = 100L) {
  tibble::tibble(theta = seq(0, 2 * pi, length.out = n + 1L)[-1]) |>
    dplyr::mutate(x = radius * cos(theta) + center[1],
                  y = radius * sin(theta) + center[2])
}
ggplot() + aes(x, y) +
  geom_polygon(data = make_circle(5, c(0, 0)), fill = "#E69F00", alpha = 0.5) +
  geom_polygon(data = make_circle(1, c(4.3, 0)), fill = "#0072B2", alpha = 0.5) +
  coord_fixed(xlim = c(-7, 7)) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank())
```


---
## 条件付き確率がよくわかる具体例

<span style="color: #0072B2;">B Brewery</span>のビールが<span style="color: #E69F00;">Awesome</span>な確率
: $\text{Prob}(\textcolor{#E69F00}{\text{Awesome}} \mid \textcolor{#0072B2}{\text{B Brewery}}) = \frac {\text{Prob}(\textcolor{#E69F00}{\text{Awesome}},~\textcolor{#0072B2}{\text{B Brewery}})} {\text{Prob}(\textcolor{#0072B2}{\text{B Brewery}})}$
: かなり高い確率。良い醸造所。

<span style="color: #E69F00;">Awesome</span>なビールが<span style="color: #0072B2;">B Brewery</span>のものである確率
: $\text{Prob}(\textcolor{#0072B2}{\text{B Brewery}} \mid \textcolor{#E69F00}{\text{Awesome}}) = \frac {\text{Prob}(\textcolor{#E69F00}{\text{Awesome}},~\textcolor{#0072B2}{\text{B Brewery}})} {\text{Prob}(\textcolor{#E69F00}{\text{Awesome}})}$
: かなり低い確率。Awesomeなビールはほかにもたっくさんある。

<img src="./figure/venn-1.png" alt="plot of chunk venn" width="432">


---
## ベイズの定理

乗法定理
: $\text{Prob}(\textcolor{#E69F00}{A},~\textcolor{#0072B2}{B}) = \text{Prob}(\textcolor{#E69F00}{A} \mid \textcolor{#0072B2}{B})~\text{Prob}(\textcolor{#0072B2}{B}) = \text{Prob}(\textcolor{#0072B2}{B} \mid \textcolor{#E69F00}{A})~\text{Prob}(\textcolor{#E69F00}{A})$

<a href="https://en.wikipedia.org/wiki/Thomas_Bayes">
<img src="../tokiomarine2021/image/Thomas_Bayes.gif" height="150" align="right"></a>

移項するだけで**ベイズの定理**:
<img src="../tokiomarine2021/bayes.drawio.svg" style="padding-left: 1rem;">

宴会場にビールが運ばれてきた。これはどこのブルワリーの？

事前確率: $\text{Prob}(\textcolor{#0072B2}{B})$
: 飲む前、手元のビールが<span style="color: #0072B2;">B Brewery</span>のである確率。
: ↓ 🍻 飲んでみて更新

事後確率: $\text{Prob}(\textcolor{#0072B2}{B} \mid \textcolor{#E69F00}{A})$
: 飲んでみて<span style="color: #E69F00;">Awesome</span>だったビールが
  <span style="color: #0072B2;">B Brewery</span>のである確率。


---
## ベイズの定理 in 感染症検査

- 有病率 $\text{Prob}(I)$ : 0.3% (この地域の感染者の割合; 事前確率)
- 感度 $\text{Prob}(P \mid I)$ : 90% (感染してる人に陽性判定が出る)
- 特異度 $\text{Prob}(\lnot P \mid \lnot I)$: 99% (感染してない人に陰性判定が出る)

さて、陽性適中率(検査陽性の人が実際に感染者である確率)は？

<div>\[\begin{split}
\text{Prob}(I \mid P)
  &= \frac {\text{Prob}(P \mid I)~\text{Prob}(I)} {\text{Prob}(P)} \\
  &= \frac {\text{Prob}(P \mid I)~\text{Prob}(I)}
           {\text{Prob}(P \mid I)~\text{Prob}(I) + \text{Prob}(P \mid \lnot I)~\text{Prob}(\lnot I)} \\
  &= \frac {0.9 \times 0.003} {0.9 \times 0.003 + 0.01 \times 0.997} \approx 0.21
\end{split}\]</div>

感染者を隔離するスクリーニング目的では使いものにならない性能。

🔰 同様に $\text{Prob}(\lnot I \mid \lnot P)$ 陰性的中率を計算してみよう<br>
🔰 計算結果が検査性能だけでなく有病率にも依存することを確認しよう

```{r, infection-test}
#| eval: false
#| include: false
sensitivity = 0.9
specificity = 0.99
prevalence = 0.003
p_positive = (prevalence * sensitivity + (1 - prevalence) * (1 - specificity))
p_negative = ((1 - prevalence) * specificity + prevalence * (1 - sensitivity))
devnull = tibble::tibble(
  true_positive = prevalence * sensitivity / p_positive,
  false_positive = (1 - prevalence) * (1 - specificity) / p_positive,
  true_negative = (1 - prevalence) * specificity / p_negative,
  false_negative = prevalence * (1 - sensitivity) / p_negative
)
```


---
## ベイズの定理 in 統計モデリング

<p>
<img src="../tokiomarine2021/bayesian.drawio.svg">
</p>

モデル$M$に対する確信度合いをデータ$D$に基づいて更新する。<br>
モデル$M$を仮説$H$やパラメータ$\theta$に置き換えてもいい。

**周辺尤度**は「確率分布の積分は1」を満たすための正規化定数とみなせる。<br>
比例関係だけ抜き出してこう書くことが多い:
<div>\[\begin{align}
\text{Prob}(M \mid D) &\propto \text{Prob}(D \mid M)~\text{Prob}(M) \tag{Model}\\
\text{Prob}(H \mid D) &\propto \text{Prob}(D \mid H)~\text{Prob}(H) \tag{Hypothesis} \\
\text{Prob}(\theta \mid D) &\propto \text{Prob}(D \mid \theta)~\text{Prob}(\theta) \tag{Parameter}
\end{align}\]</div>


---
## 表が出る確率のベイズ推定: 1. 事前分布

コイントスを繰り返して、表が出る確率pをベイズ推定したい。

事前分布には**ベータ分布**を採用(理由は後で分かる):
<div>\[\begin{split}
\text{Beta}(p \mid a, b) =
   \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} p^{a-1} (1 - p)^{b-1}
\end{split}\]</div>

分布の形は $a,~b$ によって決まる。<br>
ガンマ関数の部分は厳つく見えるけどただの正規化定数。<br>
投げる前なのでとりあえず真っ平らを仮定 $\text{Beta}(p \mid a = 1, b = 1)$:

```{r, prior-beta}
#| include: false
#| fig.width: 3
#| fig.height: 3
prior_beta = df_coin |>
  dplyr::filter(i == 0) |>
  ggplot() + aes(p, Density) +
  geom_area(fill = palette()[3], alpha = 0.5) +
  coord_cartesian(ylim = c(0, 3)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
prior_beta + labs(title = "Prior")
```
```{r, likelihood-binom}
#| include: false
#| fig.width: 3
#| fig.height: 3
df_coin |>
  dplyr::filter(i == 4) |>
  dplyr::mutate(Lik = exp(logLik)) |>
  ggplot() + aes(p, Lik) +
  geom_line(color = palette()[3], linewidth = 2) +
  labs(title = "Likelihood") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
```
```{r, posterior-beta}
#| include: false
#| fig.width: 3
#| fig.height: 3
prior_beta %+%
  (df_coin |> dplyr::filter(i == 4)) +
  labs(title = "Posterior")
```


<div class="column-container">
  <div class="column"></div>
  <div class="column"></div>
  <div class="column">
<img src="./figure/prior-beta-1.png" alt="plot of chunk prior-beta" width="216" style="vertical-align: middle;">
  </div>
</div>


---
## 表が出る確率のベイズ推定: 2. 尤度関数

4回投げて表が1回だった、というデータで**尤度**を計算(**二項分布**):
<div>\[\begin{split}
\text{Binom}(1 \mid 4,~p) = \binom {1} {4} p^{1} (1 - p)^{3}
\end{split}\]</div>

これに事前分布を掛けて正規化したら事後分布になるはず。

<div class="column-container">
  <div class="column"></div>
  <div class="column">
<img src="./figure/likelihood-binom-1.png" alt="plot of chunk likelihood-binom" width="216" style="vertical-align: middle;">
&ensp;⨉
  </div>
  <div class="column">
<img src="./figure/prior-beta-1.png" alt="plot of chunk prior-beta" width="216" style="vertical-align: middle;">
  </div>
</div>


---
## 表が出る確率のベイズ推定: 3. 事後分布

なんと、事後分布もベータ分布になる。

<div>\[\begin{split}
\text{Posterior}
  &\propto \text{Binom}(1 \mid 4,~p) \times \text{Beta}(p \mid  1, 1)\\
  &= \binom {1} {4} p^{1} (1 - p)^{3} \times
     \frac{\Gamma(1 + 1)}{\Gamma(1) \Gamma(1)} p^{1-1} (1 - p)^{1-1} \\
  &= C p^{2-1} (1 - p)^{4-1} \\
  &= \text{Beta}(p \mid 2, 4)
\end{split}\]</div>

ベータ分布の形パラメータ$a$が表、$b$が裏の回数分だけ増加。

<div class="column-container">
  <div class="column">
<img src="./figure/posterior-beta-1.png" alt="plot of chunk posterior-beta" width="216" style="vertical-align: middle;">
&ensp;$\propto$
  </div>
  <div class="column">
<img src="./figure/likelihood-binom-1.png" alt="plot of chunk likelihood-binom" width="216" style="vertical-align: middle;">
&ensp;⨉
  </div>
  <div class="column">
<img src="./figure/prior-beta-1.png" alt="plot of chunk prior-beta" width="216" style="vertical-align: middle;">
  </div>
</div>


---
## 表が出る確率のベイズ推定: 4. 逐次学習

さっきの事後分布を事前分布として、さらにデータを集める。

コイントス4回のうち表1回、に基づく**事前分布**: $\text{Beta}(p \mid 2,~4)$

さらに16回投げたら表が7回、の**尤度**: $\text{Binomial}(7 \mid 16,~p)$

**事後分布**はまた事前分布と同じ形になる:

<div>\[\begin{split}
\text{Beta}(p \mid 9, 13) \propto
  \text{Binomial}(7 \mid 16,~p) \times \text{Beta}(p \mid 2, 4)
\end{split}\]</div>

データを加えるたびに更新していける:

<img src="figure/coin-bayesian-1.png" alt="plot of chunk coin-bayesian" width="85%">


---
## 共役事前分布

事後分布が事前分布と同じ形なので計算しやすい、という組み合わせ。

| 尤度関数 | 共役事前分布 |
| ---------- | ------------ |
| 二項分布 | ベータ分布 |
| ポアソン分布 | ガンマ分布 |
| 正規分布 | ガンマ分布 |
| 正規分布 (分散既知) | 正規分布 |

共役事前分布を使うことが常に最善とは限らない。<br>
計算コストがかかっても**無情報事前分布**を使う風潮。

---
## 事後分布を用いた推定

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0;">

区間推定
: 幅のある推定値を提示
: e.g., 95%ベイズ確信区間:<br>
  等裾事後確信区間 (<u>E</u>qual-<u>T</u>ailed <u>I</u>nterval)<br>
  最高密度区間 (<u>H</u>ighest <u>D</u>ensity <u>I</u>nterval)

点推定
: 値を1点だけ提示
: e.g.,<br>
  事後確率最大値 (<u>M</u>aximum <u>A</u> <u>P</u>osteriori)<br>
  事後中央値 (Posterior <u>Med</u>ian)<br>
  事後期待値 (<u>E</u>xpected <u>A</u> <u>P</u>osteriori)

  </div>
  <div class="column" style="flex-shrink: 1.3;">

```{r, integrate}
#| echo: false
#| fig.width: 5
#| fig.height: 8.2
#| cache-globals: false
hdi_lowerbound = function(density, ci = 0.95) {
  x = sort(density, decreasing = TRUE)
  cumsum_x = cumsum(x)
  sum_x = cumsum_x[length(x)]
  y = x[cumsum_x < (ci * sum_x)]
  y[length(y)]
}

n_head = 3
n_total = 12
df_density = tibble::tibble(
  p = seq(0, 1, 0.001),
  Density = dbeta(p, n_head, n_total - n_head))

.theme = theme_minimal(base_size = 18) + theme(
  panel.grid.minor = element_blank(),
  panel.grid.major.y = element_blank(),
  axis.text = element_blank())

bound = qbeta(c(0.025, 0.975), n_head, n_total - n_head)
.f = function(.x) {dplyr::filter(.x, dplyr::between(p, bound[1], bound[2]))}
.g = function(.x) {dplyr::mutate(.x, Density = ifelse(dplyr::between(p, bound[1], bound[2]), 0, Density))}
p_eti = df_density |>
  ggplot() + aes(p, Density) +
  geom_hline(yintercept = 0) +
  geom_line(color = "#999999", linewidth = 1) +
  geom_area(data = .f, fill = palette()[3], alpha = 0.5) +
  geom_area(data = .g, fill = "#333333", alpha = 0.8) +
  annotate("text", x = bound[1], y = 0.7, label = "2.5%", color = "#333333", hjust = 1) +
  annotate("text", x = bound[2], y = 0.7, label = "2.5%", color = "#333333", hjust = 0) +
  scale_x_continuous(breaks = bound) +
  labs(title = "Equal-Tailed Interval (ETI)", x = "Parameter") +
  .theme + theme(panel.grid.major = element_line(color = palette()[3]))

hdi_lb = hdi_lowerbound(df_density$Density)
df_hdi = df_density |> dplyr::filter(Density >= hdi_lb)
range_hdi = df_hdi |> dplyr::reframe(p = range(p)) |> dplyr::pull(p)
p_hdi = df_density |>
  ggplot() + aes(p, Density) +
  geom_hline(yintercept = 0) +
  geom_area(data = df_hdi, fill = palette()[3], alpha = 0.5) +
  geom_line(color = "#999999", linewidth = 1) +
  geom_hline(aes(yintercept = hdi_lb), color = "#444444", linewidth = 1, linetype = "dashed") +
  scale_x_continuous(breaks = range_hdi) +
  labs(title = "Highest Density Interval (HDI)", x = "Parameter") +
  .theme + theme(panel.grid.major = element_line(color = palette()[3]))

.map = function(.x) {dplyr::slice_max(.x, Density)}
.med = function(.x) {
  .x |>
    dplyr::mutate(cs = cumsum(Density)) |>
    dplyr::mutate(diff = abs(cs - 0.5 * max(cs))) |>
    dplyr::slice_min(diff)
}
.eap = function(.x) {
  .x |>
    dplyr::mutate(diff = abs(.data$p - sum(p * Density) / sum(Density))) |>
    dplyr::slice_min(diff)
}
.colors = setNames(palette()[2:4], c("map", "med", "eap"))
p_point = df_density |>
  ggplot() + aes(p, Density) +
  geom_hline(yintercept = 0) +
  geom_line(color = "#999999", linewidth = 1) +
  geom_vline(aes(xintercept = p), .map, color = .colors["map"], linewidth = 1) +
  geom_vline(aes(xintercept = p), .med, color = .colors["med"], linewidth = 1) +
  geom_vline(aes(xintercept = p), .eap, color = .colors["eap"], linewidth = 1) +
  geom_point(aes(y = 0), .map, color = .colors["map"], size = 4, shape = 16) +
  geom_point(aes(y = 0), .med, color = .colors["med"], size = 4, shape = 16) +
  geom_point(aes(y = 0), .eap, color = .colors["eap"], size = 4, shape = 16) +
  ggrepel::geom_text_repel(aes(y = 0.25, label = "MAP"), .map, color = .colors["map"], size = 5, nudge_x = -0.08) +
  ggrepel::geom_text_repel(aes(y = 0.45, label = "MED"), .med, color = .colors["med"], size = 5, nudge_x = 0.1) +
  ggrepel::geom_text_repel(aes(y = 0.25, label = "EAP"), .eap, color = .colors["eap"], size = 5, nudge_x = 0.2) +
  labs(title = "Point Estimation", x = "Parameter") +
  .theme + theme(panel.grid = element_blank())
cowplot::plot_grid(p_eti, p_hdi, p_point, ncol = 1L)
```

  </div>
</div>

???

ベイズ確信区間 (credible interval)
: 真の値が95%の確率で含まれる区間。

信頼区間 (confidence interval)
: 頻度主義の考え方に基づいていて、解釈が難しい。
「標本抽出を100回繰り返すとそのうちの95回はその区間に母平均が含まれる」
真の値は1つに定まっていて、不確実性は有限のサンプリングに由来する。



---
## ベイズ推定の中間まとめ

- 推定結果は**事後分布** ∝ 尤度関数。
    - 広がり具合によって不確実性も表現できる。
    - **逐次学習**で尖っていき、確信が強まる。

<img src="figure/coin-bayesian-1.png" alt="plot of chunk coin-bayesian" width="70%">

<hr>

コイン投げモデルのベータ分布は美しい例。<br>
→ 解析的(数学的)に解ける。

実践的なモデル・事後分布はもっと複雑。<br>
→ コンピュータに頼って数値計算: MCMC


---
## MCMC: <u>M</u>arcov <u>C</u>hain <u>M</u>onte <u>C</u>arlo

<a href="https://en.wikipedia.org/wiki/Andrey_Markov">
<img src="../tokiomarine2021/image/AAMarkov.jpg" height="180" align="right"></a>

マルコフ連鎖
: 次の時点の挙動が現在の値だけで決定されるような確率過程。
: $\ldots \to X_{t - 2} \to X_{t - 1} \to X_{t} \to X_{t + 1}$
: $\text{Prob}(X_{t+1} \mid X_{t}, X_{t-1}, X_{t-2}, \ldots) = \text{Prob}(X_{t+1} \mid X_{t})$
: e.g., すごろく

モンテカルロ法
: 乱数を用いた計算方法の総称。
: <a href="https://en.wikipedia.org/wiki/Monte_Carlo_Casino">
  <img src="../tokiomarine2021/image/Real_Monte_Carlo_Casino.jpg" height="200"></a>
  <a href="https://en.wikipedia.org/wiki/Stanislaw_Ulam">
  <img src="../tokiomarine2021/image/Stanislaw_Ulam.jpg" height="200"></a>
  <a href="https://en.wikipedia.org/wiki/John_von_Neumann">
  <img src="../tokiomarine2021/image/John_von_Neumann.jpg" height="200"></a>
  <a href="https://en.wikipedia.org/wiki/Nicholas_Metropolis">
  <img src="../tokiomarine2021/image/Nicholas_Metropolis.png" height="200"></a>

???
スタニスワフ・ウラムがソリテアの成功率を考えてた時に思いついて、
同僚のジョン・フォン・ノイマンが計算機上での実用まで持ってった。
モナコ公国のモンテカルロ地区に国営カジノがあって、
ウラムの叔父がそこで負けて親戚から借金したことにちなんで
同僚のニコラス・メトロポリスが命名したらしい。

マルコフ連鎖
: マルコフ過程のうち、離散的な時間を考えるもの。

マルコフ過程
: マルコフ性を持つ確率過程

マルコフ性
: 次の時点の挙動が現在の値だけで決定され、過去の値と無関係


最適化ではなくサンプリング法


---
## モンテカルロ法は乱数を用いた計算方法

e.g., 半径1の円の面積

数学を知っていれば $\pi r ^ 2 \approx 3.14159$

面積4の正方形に400個の一様乱数を打ち込んだら318個が円に乗った:<br>
$4 \times \frac {318} {400} = 3.18$

```{r, circle}
#| echo: false
#| fig.width: 4
#| fig.height: 4
set.seed(19937)
.n = 200L
.theta = seq(0, 2 * pi, length.out = 100)
tibble(x = runif(.n, -1, 1), y = runif(.n, -1, 1)) |>
  dplyr::mutate(accepted = x ^ 2 + y ^ 2 < 1) |>
  ggplot() + aes(x, y) +
  annotate("polygon", x = cos(.theta), y = sin(.theta), fill = "#888888", alpha = 0.4) +
  geom_point(data = \(x) dplyr::filter(x, !accepted), shape = 16, alpha = 0.5, size = 2) +
  geom_point(data = \(x) dplyr::filter(x, accepted), color = palette()[3], shape = 16, size = 2) +
  coord_fixed(xlim = c(-1, 1), ylim = c(-1, 1), expand = FALSE)  +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
```

---
## 変な分布もモンテカルロ法で扱えそう

e.g., 確率密度分布に従って変数Xを集める(棄却サンプリング)。

```{r, mcpdf}
#| include: false
#| fig.width: 5
#| fig.height: 5
set.seed(24601)
n = 800
df_point = tibble::tibble(p = runif(n, -0.2, 1.2), Density = runif(n, 0, 5)) |>
  dplyr::mutate(accepted = Density < (dbeta(p, 4, 6) + dbeta(p, 70, 30)) / 2)
p_pdf = tibble::tibble(p = seq(0, 1, 0.01), Density = (dbeta(p, 4, 6) + dbeta(p, 70, 30)) / 2) |>
  ggplot() + aes(p, Density) +
  geom_hline(yintercept = 0) +
  geom_area(fill = "#000000", alpha = 0.33) +
  geom_point(data = df_point |> dplyr::filter(!accepted), shape = 16, alpha = 0.5) +
  geom_point(data = df_point |> dplyr::filter(accepted), color = palette()[3], shape = 16, size = 2, alpha = 1) +
  labs(x = "X") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank(),
        panel.grid.major.y = element_blank(), panel.border = element_blank(),
        axis.text = element_blank())

p_accepted = tibble::tibble(p = runif(n), th = dbeta(p, 4, 6) + dbeta(p, 70, 30)) |>
  dplyr::filter(runif(length(p), 0, max(th)) < th) |>
  ggplot() + aes(p) +
  geom_hline(yintercept = 0) +
  coord_cartesian(xlim = c(-0.2, 1.2)) +
  geom_histogram(bins = 20, center = 0.5, fill = palette()[3], alpha = 0.66) +
  labs(x = "X") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank(),
        panel.grid.major.y = element_blank(), panel.border = element_blank(),
        axis.text = element_blank())

print(p_pdf)
print(p_accepted)
```

<div>
<img src="./figure/mcpdf-2.png" alt="plot of chunk mcpdf" width="360" style="vertical-align: middle;">
$\;\sim\;$
<img src="./figure/mcpdf-1.png" alt="plot of chunk mcpdf" width="360" style="vertical-align: middle;">
</div>

でも、ハズレの値もけっこう引いてしまう。


---
## 次元の呪い: 高次元になるほど当たりにくくなる

(N次元球の体積 / N次元の立方体) はゼロに近づいていく。

<img src="figure/circle-1.png" width="210" align="right">

- 2次元: $\frac {\pi r ^ 2} {(2r) ^ 2} = \frac \pi 4 \approx 0.79$
- 3次元: $\frac {\frac 4 3 \pi r ^ 3} {(2r) ^ 3} = \frac \pi 6 \approx 0.52$
- N次元: $\frac {\frac {\pi ^ {N/2}} {\Gamma (N/2 + 1)} r ^ N} {(2r) ^ N} = \frac {\pi ^ {N/2}} {2^N \Gamma (N/2 + 1)} \to 0$

パラメータが増えると計算量(≈乱数の無駄撃ち)が急増。

<hr>

密度の高い「当たり」付近を効率よく探索したい。<br>
「当たり」は「当たり」の近くにありがちだろう。<br>
→ マルコフ連鎖が使えそう


---
## Metropolis--Hastings法 (MH法)

0.  パラメータ $\theta$ の初期値を選ぶ
1.  $\theta$ をちょっと増減させて $\theta_\text{new}$ を作る
2.  それぞれ尤度を計算し、比較。
    - $L(\theta_\text{new}) \ge L(\theta)$ なら $\theta_\text{new}$ を即採択
    - $L(\theta_\text{new}) < L(\theta)$ でも
      確率 $r = \frac {L(\theta_\text{new})} {L(\theta)}$ で  $\theta_\text{new}$ を採択
3.  $\theta_\text{new}$ が採択されたら $\theta$ を更新。手順1に戻る。

```{r, metropolis-globals}
#| include: false
n_head = 12
n_total = 20
```
```{r, metropolis}
#| echo: false
#| fig.width: 11
#| fig.height: 3.5
.logLik = function(p) dbinom(n_head, n_total, p, log = TRUE)
.delta = 0.02
df_point = tibble::tibble(p = seq(0.48, 0.64, .delta), logLik = .logLik(p))
df_arrow = df_point |>
  dplyr::group_by(p) |>
  dplyr::mutate(data = purrr::map_dfr(p, ~{
    pl = p + 0.005
    pr = p + .delta - 0.005
    ll = .logLik(pl)
    lr = .logLik(pr)
    if (ll < lr) {
      data.frame(pbegin = pl, pend = pr, lbegin = ll, lend = lr)
    } else {
      data.frame(pbegin = pr, pend = pl, lbegin = lr, lend = ll)
    }
  })) |>
  tidyr::unpack(data)

.ar = grid::arrow(angle = 20, length = grid::unit(0.11, "inches"), type = "closed")
.ar2 = grid::arrow(angle = 20, length = grid::unit(0.09, "inches"), type = "closed", ends = "first")
.aes = aes(pbegin, lbegin, xend = pend, yend = lend)
df_point |>
  ggplot() + aes(p, logLik) +
  geom_segment(.aes, df_arrow, linejoin = "mitre", linewidth = 1,
    position = position_nudge(0, +0.009), arrow = .ar) +
  geom_segment(.aes, df_arrow, linejoin = "mitre", linewidth = 1,
    position = position_nudge(0, -0.009), arrow = .ar2, alpha = 0.33) +
  geom_point(size = 6, alpha = 0.6, shape = 16) +
  labs(x = expression(theta)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())
```

---
## 採択されたパラメータ値の軌跡

尤度が高い方にただ向かうだけでなく、結構うろつく。<br>
通ったパラメータ値を集めるといい感じの分布が得られる。

```{r, metropolis-trajectory}
#| echo: false
#| fig.show: "animate"
#| animation.hook: "gifski"
#| interval: 0.2
#| fig.width: 12
#| fig.height: 6
mh_sample = function(n, p0, n_head, n_total, delta) {
  p = p0
  lp = dbinom(n_head, n_total, p, log = FALSE)
  posterior = p
  for (i in seq.int(2L, n)) {
    p_new = p + sample(c(-delta, delta), 1)
    lp_new = suppressWarnings(dbinom(n_head, n_total, p_new, log = FALSE))
    if (is.nan(lp_new)) next
    if (lp_new > lp) {
      p = p_new
      lp = lp_new
    } else {
      if (runif(1L) < (lp_new / lp)) {
        p = p_new
        lp = lp_new
      }
    }
    posterior = c(posterior, p)
  }
  tibble::tibble(p = round(posterior, 6)) |> tibble::rowid_to_column("t")
}
.n = 800
.dp = 0.04
df_trace = tibble::tibble(init = c(0.06, 0.5, 0.94)) |>
  dplyr::mutate(data = purrr::map(init, ~{mh_sample(.n, .x, n_head, n_total, delta = .dp)})) |>
  tidyr::unnest(data)

df_traj = df_trace |> dplyr::filter(init == min(init)) |> dplyr::select(!init)

p_traj = df_traj |>
  ggplot() + aes(t, p) +
  geom_line(linewidth = 0.8, alpha = 0.66) +
  coord_cartesian(xlim = c(0, .n), ylim = c(0, 1)) +
  labs(title = "traceplot", y = expression(theta), x = "iterations") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())

p_hist = df_traj |>
  ggplot() + aes(p) +
  geom_bar() +
  coord_flip(xlim = c(0, 1)) +
  labs(title = "MCMC samples") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank(),
        axis.title.y = element_blank(), axis.text.y = element_blank())

ymax = max(ggplot_build(p_hist)$data[[1]]$count)

for (i in seq.int(40, .n, 40)) {
  df_traj_head = df_traj |> head(i)
  p1 = p_traj %+% df_traj_head
  p2 = p_hist %+% df_traj_head + ylim(c(0, ymax))
  p = cowplot::plot_grid(p1, p2, nrow = 1L, rel_widths = c(4, 1))
  print(p)
}
```


---
## 尤度に比例する事後分布からサンプルしたのと等価

全体にばら撒く棄却サンプリングよりも効率よく集められる。<br>
が、パラメータ1つの1次元ではご利益はわかりにくい。

```{r, propto-lik}
#| include: false
#| fig.width: 3.5
#| fig.height: 3.5
.theme = theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank(),
        panel.grid.major.y = element_blank(), panel.border = element_blank())

p_lik = tibble::tibble(p = seq(0, 1, 0.01), Lik = dbinom(n_head, n_total, p, log = FALSE)) |>
  ggplot() + aes(p, Lik) +
  geom_hline(yintercept = 0) +
  geom_line(linewidth = 2) +
  labs(title = "Likelihood", x = expression(theta)) +
  .theme

p_dens = tibble::tibble(p = seq(0, 1, 0.01), Density = dbeta(p, n_head, n_total - n_head)) |>
  ggplot() + aes(p, Density) +
  geom_hline(yintercept = 0) +
  geom_area(alpha = 0.5) +
  labs(title = "Posterior", x = expression(theta)) +
  .theme

p_hist = df_traj |> tail(-100) |>
  ggplot() + aes(p) +
  geom_hline(yintercept = 0) +
  geom_bar() +
  coord_cartesian(xlim = c(0, 1)) +
  labs(title = "MCMC samples", x = expression(theta), y = "Count") +
  .theme

print(p_hist)
print(p_dens)
print(p_lik)
```

<div>
<img src="./figure/propto-lik-1.png" alt="plot of chunk propto-lik" width="240" style="vertical-align: middle;">
$\;\sim\;$
<img src="./figure/propto-lik-2.png" alt="plot of chunk propto-lik" width="240" style="vertical-align: middle;">
$\;\propto\;$
<img src="./figure/propto-lik-3.png" alt="plot of chunk propto-lik" width="240" style="vertical-align: middle;">
</div>

パラメータが複数ある場合は？


---
## Gibbs Sampling

パラメータが複数の場合「ほかを固定してひとつ更新」を繰り返す。

e.g., 二次元正規分布。(-2, 2) からスタート。

```{r, gibbs}
#| echo: false
#| fig.show: "animate"
#| animation.hook: "gifski"
#| interval: 0.1
#| fig.width: 5
#| fig.height: 5
gibbs_sample = function(n, rho) {
  x = double(n)
  y = double(n)
  x[1] = -2
  y[1] = 2
  for (i in seq.int(2, n)) {
    x[i] = rnorm(1, rho * y[i - 1], sqrt(1 - rho ** 2))
    y[i] = rnorm(1, rho * x[i], sqrt(1 - rho ** 2))
  }
  x = rep(x, each = 2)
  y = rep(y, each = 2)
  tibble::tibble(x = x[-1], y = y[-2 * n])
}

set.seed(19937)
rho = 0.8
df_gibbs = gibbs_sample(40, rho)

p_tile = tidyr::crossing(x = seq(-3, 3, 0.05), y = x) |>
  dplyr::mutate(z = x^2 + y^2 - 2 * rho * x * y,
                d = (1 / (2 * pi * sqrt(1 - rho ** 2))) * exp(-0.5 * z / (1 - rho ** 2))) |>
  ggplot() + aes(x, y) +
  geom_tile(aes(fill = d)) +
  scale_fill_gradient(low = "#f8f8f8", high = "black") +
  coord_fixed(expand = FALSE) +
  theme_void() + theme(legend.position = "none")

for (i in seq.int(2L, nrow(df_gibbs), 2L)) {
  df_i = head(df_gibbs, i)
  p = p_tile +
    geom_path(data = df_i, color = palette()[3], alpha = 0.66, linewidth = 1) +
    geom_point(data = df_i, color = palette()[3], alpha = 0.66, shape = 16)
  print(p)
}
```


---
## 何回やっても似たような結果になってほしい

乱数や初期値によって偶々、じゃないことを確認したい。

e.g., `chains = 3` 。ほぼ同じところをうろうろ:

```{r, chains}
#| echo: false
#| fig.height: 5
#| fig.width: 12
t_warmup = 200
p_trace = ggplot(df_trace) + aes(t, p, group = init) +
  geom_path(aes(color = as.factor(init)), linewidth = 0.8) +
  scale_color_discrete(guide = NULL) +
  labs(title = "traceplot", y = expression(theta), x = "iterations") +
  coord_cartesian(ylim = c(0, 1)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank())

p_hist = df_trace |>
  dplyr::mutate(init = as.factor(init)) |>
  ggplot() + aes(p) +
  geom_bar(aes(fill = init), position = position_stack(reverse = TRUE)) +
  coord_flip(xlim = c(0, 1)) +
  labs(title = "MCMC samples") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), axis.ticks = element_blank(),
        axis.title.y = element_blank(), axis.text.y = element_blank(),
        legend.position = "none")
cowplot::plot_grid(p_trace, p_hist, nrow = 1L, rel_widths = c(4, 1))
```

収束(convergence)の判定については後ほど。

---
## 初期値の影響が消えるまでウォーミングアップ

定常分布の山に到達してからが本番。

e.g., `iter_warmup = 200, iter_sampling = 600` で灰色の部分を捨てる:

```{r, warmup}
#| echo: false
#| fig.height: 5
#| fig.width: 12
cowplot::plot_grid(
  p_trace + annotate("ribbon", x = c(0, t_warmup), ymin = Inf, ymax = -Inf, alpha = 0.3),
  p_hist %+% (df_trace |> dplyr::mutate(init = as.factor(ifelse(t < t_warmup, NA, init)))),
  nrow = 1L, rel_widths = c(4, 1))
```

どれくらい長く捨てるべきかは場合による。


---
## 適度に間引いて自己相関を軽減したい

直前の値と似すぎていたら独立サンプルとして扱えないので。

e.g., `thin = 5` で5回に1回だけサンプルする:

```{r, thin}
#| echo: false
#| fig.height: 5
#| fig.width: 7
#| message: false
p_trace +
  coord_cartesian(ylim = c(0, 1), xlim = c(200, 260)) +
  geom_point(data = function(.x) {dplyr::filter(.x, t %% 5 == 0)}, alpha = 0.6, size = 3, shape = 16)
```

間引かなくても大丈夫な場合も、間引いても解決しない場合もある。



---
## 収束判定

- 複数chainsで異なる初期値から実行し、軌跡を可視化(traceplot)
- Gelman-Rubin統計量 $\hat R < 1.05$
- Effective Sample Size (ESS) $N_\text{eff} > 100$ per chain

```{cat, engine.opts = list(file = "stan/converge-yes.stan"), cache.rebuild = !file.exists("stan/converge-yes.stan")}
data {
  int<lower=0> N;
  array[N] int x;
}

parameters {
  real<lower=0> lambda;
}

model {
  x ~ poisson(lambda);
}
```
```{cat, engine.opts = list(file = "stan/converge-no.stan"), cache.rebuild = !file.exists("stan/converge-no.stan")}
data {
  int<lower=0> N;
  array[N] int x;
}

parameters {
  real lambda;
  real jammer;
}

model {
  x ~ poisson(lambda + jammer);
}
```

```{r, converge-template}
#| include: false
convdata = tibble::lst(N = 10L, x = rep(10L, N))
myplot_trace = function(stan_file, data) {
  model = cmdstan_model(stan_file)
  fit = model$sample(data = data, seed = 24601L, show_messages = FALSE, refresh = 0)
  s = fit$summary(variables = "lambda")
  stats = sprintf("Rhat = %.4f, N_eff = %.1f", s[["rhat"]], s[["ess_bulk"]])
  bayesplot::mcmc_trace(fit$draws("lambda")) +
    scale_color_discrete(guide = "none") +
    labs(title = ifelse(s[["rhat"]] < 1.05, "Good", "Bad"), subtitle = stats) +
    theme_bw(base_size = 16) +
    theme(panel.grid.minor = element_blank(), axis.ticks = element_blank(),
      axis.title.y = element_blank(), axis.text.y = element_blank())
}
```
```{r, converge-yes}
#| echo: false
#| cache.extra: !expr 'tools::md5sum("stan/converge-yes.stan")'
#| message: false
#| warning: false
#| results: false
p_yes = myplot_trace("stan/converge-yes.stan", convdata)
```
```{r, converge-no}
#| echo: false
#| cache.extra: !expr 'tools::md5sum("stan/converge-no.stan")'
#| message: false
#| warning: false
#| results: false
p_no = myplot_trace("stan/converge-no.stan", convdata)
```
```{r, convergence}
#| echo: false
#| fig.height: 4
#| fig.width: 12
cowplot::plot_grid(p_yes, p_no, nrow = 1L)
```

[`diagnose()`](https://mc-stan.org/docs/cmdstan-guide/diagnose.html)
みたいな機能が提供されていれば利用する。

実行時に[警告してもらえること](https://mc-stan.org/misc/warnings.html)もある。

???
https://ill-identified.hatenablog.com/entry/2020/05/21/001158


---
## 収束・自己相関が悪い場合にどう改善するか

- 小手先の対処
    - iteration (warnup + sampling) をもっと長く
    - thinを大きくして間引く
- ちょっと大掛かり
    - プログラムの書き方を改善する
    - モデルの構造を見直す
    - アルゴリズム・ソフトウェアを変える

---
##  MCMCの方法いろいろ

採択率を高め、早く収束するように改良されてきている。

- Metropolis--Hastings法
    - Gibbs Sampling
    - Hamiltonian Monte Carlo (HMC)
        - No-U-Turn Sampler (NUTS)


---
## MCMCソフトウェア

- [BUGS](https://www.mrc-bsu.cam.ac.uk/software/bugs/)
    - クローズドソースで、ほぼWindows専用。
- [JAGS](https://mcmc-jags.sourceforge.io/)
    - オープンソースで、さまざまなOS・言語から利用可能。
    - マニュアルや用例が不足。
- [**Stan**](https://mc-stan.org/) 👈
    - オープンソースで、さまざまなOS・言語から利用可能。
    - 開発も利用も活発。マニュアルや用例も充実。
    - HMC/NUTSにより早く収束。
- [PyMC3](https://docs.pymc.io/)
- [NumPyro](https://num.pyro.ai/)
- [TensorFlow Probability](https://www.tensorflow.org/probability/)


---
## ベイズ推定とMCMCまとめ

- 推定結果は**事後分布** ∝ 尤度関数。
    - 広がり具合によって不確実性も表現できる。
    - **逐次学習**で尖っていき、確信が強まる。
- コンピュータに頼った計算方法としてのモンテカルロ法。
    - 高次元・変な形の分布でも効率よくサンプルできるかが肝。
    - ソフトウェアの代表は**Stan** → これから見ていく


---
## Stan

<a href="https://mc-stan.org/">
<img src="/slides/image/stan/logo_name.png" width="120" align="right">
</a>

- Stan言語で**モデルを柔軟に記述**できる。
- C++で書かれていて**高速に動作**。
- RやPythonなどから呼び出して使うのが便利。

## R Interface

[RStan](http://mc-stan.org/rstan/)
: [Rcpp](https://heavywatal.github.io/rstats/rcpp.html)を介して[StanHeaders](https://github.com/stan-dev/rstan/tree/develop/StanHeaders)を取り込んだパッケージ。

[CmdStanR](https://mc-stan.org/cmdstanr/) 👈 今後の主流
: [CmdStan](https://mc-stan.org/users/interfaces/cmdstan)
  を呼び出し、書き出されたCSVを読み取る。


---
## CmdStanR: インストールがちょっと特殊

実行の前後にRを再起動してまっさらにすることを推奨。

1. C++言語を扱うためのツールを用意。
    - <img height=22 width=22 src="https://cdn.simpleicons.org/apple"></img>
      macOS: Command Line Tools (`xcode-select --install`)
    - <img height=22 width=22 src="https://cdn.simpleicons.org/windows"></img>
      Windows: [RTools](https://cran.r-project.org/bin/windows/Rtools/)
      (Rのバージョンに合わせる)
1. [CmdStanR](https://mc-stan.org/cmdstanr/)パッケージをインストール。
   (まだCRANに登録されていない):
    ```r
    install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
    ```
1. CmdStan本体と可視化パッケージのインストール:
    ```r
    library(cmdstanr)
    check_cmdstan_toolchain(fix = TRUE)
    install_cmdstan()
    install.packages("bayesplot")
    ```

<https://mc-stan.org/cmdstanr/articles/cmdstanr.html>

---
## 🔰 とりあえずStanを動かしてみよう

```r
library(cmdstanr)
library(bayesplot)
```
```{r, library-bayesplot, message = NA, echo = FALSE}
cmdstanr_msg = utils::capture.output(cmdstanr:::startup_messages(), type = "message")
cat(cmdstanr_msg, sep = "\n")
bayesplot_msg = utils::capture.output(bayesplot:::.onAttach(), type = "message")
cat(bayesplot_msg[1], sep = "\n")
```

おおまかな流れ:

1. データ準備
1. Stan言語でモデルを書く
1. モデルをコンパイルして機械語に翻訳 → 実行ファイル
1. 実行ファイルにデータを渡してMCMCサンプリング
1. 結果を見る


---
## 説明変数なしのベイズ推定: データ準備

表が出る確率 $p=0.7$ のイカサマコインをN回投げたデータを作る。<br>
この $p$ をStanで推定してみよう。

```{r, stan-binom}
#| echo: -1
set.seed(19937)
true_p = 0.7
N = 40L
coin_data = list(N = N, x = rbinom(N, 1, true_p))
print(coin_data)
```

Rならlist型、Pythonならdict型にまとめてStanに渡す。


---
## 説明変数なしのベイズ推定: Stan言語でモデル定義

別ファイルに書いておく。
e.g., `coin.stan`:

```{cat, engine.opts = list(file = "stan/coin.stan"), class.source = "stan"}
data {
  int<lower=0> N;
  array[N] int x;
}
parameters {
  real<lower=0,upper=1> p;
}
model {
  x ~ binomial(1, p);
}
```

- いくつかのブロックに分けて記述する:<br>
  R/Pythonから受け取る `data`, 推定する `parameter`, 本体の `model`.
- [変数には型や制約を設定できる](https://mc-stan.org/docs/reference-manual/overview-of-data-types.html)
- [関数もたくさん用意されている](https://mc-stan.org/docs/functions-reference/)

---
## Stan言語の7種のブロック

順番厳守。よく使うのは**太字のやつ**。

1. `functions {...}`
1. **`data {...}`**
1. `transformed data {...}`
1. **`parameters {...}`**
1. `transformed parameters {...}`
1. **`model {...}`**
1. `generated quantities {...}`

<https://mc-stan.org/docs/reference-manual/overview-of-stans-program-blocks.html>


---
## 説明変数なしのベイズ推定: MCMCサンプル

予め実行速度の速い機械語に翻訳(コンパイル):
```{r, stan-binom-model}
#| cache.extra: !expr 'tools::md5sum("stan/coin.stan")'
model = cmdstan_model("stan/coin.stan")
```

モデルとデータを使ってMCMCサンプリング:

```{r, stan-binom-sample}
#| results: false
fit = model$sample(coin_data, seed = 24601L)
```

いろいろオプションはあるけど、ここではデフォルトに任せる:<br>
`chains`, `inits`, `iter_warmup`, `iter_samples`, `thin`, ...

問題があったら警告してくれるので**ちゃんと読む**。

---
## 説明変数なしのベイズ推定: 結果を眺める

`parameters` ブロックに書いた変数の情報が出てくる。<br>
乱数を使った計算なので(乱数シードを固定しない限り)毎回変わる。

```{r, stan-binom-fit}
#| echo: -1
summary_p = fit$summary("p") |> tibble::as_tibble()
print(fit)
```

真の値に近い $p \approx `r true_p`$ が得られた
(`r round(summary_p[["q5"]], 2)` から
`r round(summary_p[["q95"]], 2)` である確率が90%)。<br>
$\hat R$ もほぼ1で $N_\text{eff}$ も大きいのでよさそう。

`lp__` はlog posterior(対数事後確率)。後述。

念のため trace plot も確認しておこう→


---
## 説明変数なしのベイズ推定: trace plot 確認

どのchainも似た範囲を動いていて、しっかり毛虫っぽい:

```{r, stan-binom-traceplot}
#| fig.width: 11
#| fig.height: 4
draws = fit$draws()
params = names(model$variables()$parameters)
bayesplot::mcmc_trace(draws, pars = params)
```

---
## 説明変数なしのベイズ推定: 自己相関の確認

2--3ステップくらいで自己相関がほぼ消えるので問題なし:

```{r, stan-binom-ac}
#| fig.width: 6
#| fig.height: 5
bayesplot::mcmc_acf_bar(draws, pars = params)
```

---
## 説明変数なしのベイズ推定: 推定結果確認

サンプルサイズNが小さいせいか裾野の広い推定結果。<br>
真の$p$の値も含まれている:

```{r, stan-binom-hist}
#| fig.width: 4
#| fig.height: 4
bayesplot::mcmc_hist(draws, bins = 20, pars = params)
```


---
## `lp__`: log posterior とは?

`model` ブロックに次のように書いてあると:
```stan
model {
  mu ~ normal(0.0, 10.0);  // prior
  x ~ normal(mu, 1.0);     // likelihood
}
```

内部的には次のような処理が行われている:
```stan
target += normal_lpdf(theta | 0.0, 10.0)  // prior
target += normal_lpdf(x | theta, 1.0);    // likelihood
```

つまり、事前確率と尤度の対数の和を取っている。<br>
ベイズの定理により、事後確率はこれに比例する。<br>
`lp__` はこの `target` 変数を記録しておいたようなもの。


---
## ベイズ推定とMCMC

<p>
<img src="../tokiomarine2021/bayesian.drawio.svg">
</p>

- 推定結果は**事後分布** ∝ 尤度関数。
    - 広がり具合によって不確実性も表現できる。
    - **逐次学習**で尖っていき、確信が強まる。
- コンピュータに頼った計算方法としてのモンテカルロ法。
    - 高次元・変な形の分布でも効率よくサンプルできるかが肝。
    - ソフトウェアの代表は**Stan**。
    - Stan言語でモデルを書き、RやPythonから動かす。


---
## 参考文献

- [データ解析のための統計モデリング入門](https://amzn.to/33suMIZ) 久保拓弥 2012
- [StanとRでベイズ統計モデリング](https://amzn.to/3uwx7Pb) 松浦健太郎 2016
- [RとStanではじめる ベイズ統計モデリングによるデータ分析入門](https://amzn.to/3o1eCzP) 馬場真哉 2019
- [データ分析のための数理モデル入門](https://amzn.to/3uCxTKo) 江崎貴裕 2020
- [分析者のためのデータ解釈学入門](https://amzn.to/3uznzCK) 江崎貴裕 2020
- [統計学を哲学する](https://amzn.to/3ty80Kv) 大塚淳 2020


<a href="7-stan.html" rel="next" class="readmore">
7. StanでGLM
</a>