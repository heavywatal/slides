```{r, setup-common}
#| file: "setup.R"
#| echo: false
#| results: "asis"
```
```{r, setup-local}
#| include: false
#| cache: false
```

---
## データを使ってやりたいこと

- 現象を**理解**したい
- 将来を**予測**したい
- ものを**分類・判別**したい
- 挙動を**制御**したい
- 新しい何かを**生成**したい

そのために解析は必要？
未加工の生データこそ宝？

???
よりよい職場に転職したい

---
## データ解析って必要？ 生データ見ればいいべ？

往々にして複雑過ぎ、情報多すぎ、そのままでは手に負えない

```{r, diamonds}
print(ggplot2::diamonds)
```

ダイヤモンド53,940個について10項目の値を持つデータセット

---
## 要約統計量を見てみよう

各列の**平均**とか**標準偏差**とか:

```{r, summary-diamonds}
#| echo: false
dia_cols = c("carat", "depth", "table", "price")
diamonds |>
  dplyr::summarize(dplyr::across(all_of(dia_cols), list(mean = mean, sd = sd, max = max, min = min))) |>
  tidyr::pivot_longer(everything(), names_to = c(".value", "stat"), names_sep = "_") |>
  dplyr::mutate(dplyr::across(where(is.numeric), \(x) round(x, digits = 2)))
```

大きさ `carat` と価格 `price` の**相関係数**はかなり高い:
```{r, cov-diamonds}
#| echo: false
diamonds |>
  dplyr::select(all_of(dia_cols)) |>
  scale() |>
  cov() |>
  wtl::printmat("%.2f", upper = FALSE)
```

<hr>

**生のままよりは把握しやすい**かも。

---
## 分布を特徴づける代表値 central tendency

<div class="column-container">
  <div class="column" style="flex-shrink: 1.6;">

平均値 mean
: 和を観察数で割る

中央値 median
: 順に並べて真ん中

最頻値 mode
: 最も頻度が高い値

  </div>
  <div class="column">
  <a href="https://www.mhlw.go.jp/toukei/list/20-21.html">
  <img src="../tohoku2022r/image/hist-income-japan-2019.png" width="840">
  <figcaption><small><cite>所得金額階級別世帯数の頻度分布</cite> 厚生労働省 国民生活基礎調査 2019</small></figcaption>
  </a>
  </div>
</div>

外れ値に対する応答
: もし総資産額20兆円の大富豪が鳥取県に引っ越してきたら<br>
  → 県民の**平均**資産は4000万円上昇。**中央値**・**最頻値**はほぼそのまま。

目的や状況に応じて使い分けよう。

---
## ばらつきを捉える記述統計量

分散 variance
: 平均値からの差の自乗の平均。 $\frac 1 n \sum _i ^n (X_i - \bar X)^2$
: これの平方根が**標準偏差 (standard deviation)**。

Percentile, Quantile (四分位)
: 小さい順にならべて上位何%にあるか。
: 中央値 = 50th percentile = 第二四分位(Q2)

```{r, quantile}
#| fig.width: 12
#| fig.height: 4
#| echo: false
df = tibble::tibble(x = rnorm(200)) |>
  dplyr::filter(abs(x) < 2) |>
  dplyr::mutate(x = x + 3)
probs = c(min = 0, Q1 = 0.25, Q2 = 0.5, Q3 = 0.75, max = 1)
dfq = df |>
  dplyr::reframe(x = quantile(x, probs)) |>
  dplyr::mutate(name = names(probs), percent = sprintf("%d%%", 100 * probs))

p_hist = ggplot(df) + aes(x) +
  geom_histogram(bins = 30) +
  theme_void()
built = ggplot_build(p_hist)
bdata = built$data[[1]]
xlim = bdata |> dplyr::select(xmin, xmax) |> range()
p_box = ggplot(df) + aes(x) +
  geom_boxplot() +
  geom_rug(sides = "t", length = unit(0.06, "npc"), alpha = 0.66) +
  geom_point(data = dfq, aes(y = -0.6), shape = 17, size = 4) +
  geom_text(data = dfq, aes(y = -0.78, label = name), size = 6) +
  geom_text(data = dfq, aes(y = -1.0, label = percent), size = 6) +
  coord_cartesian(xlim = xlim, ylim = c(-1.2, 0.5)) +
  theme_void()
cowplot::plot_grid(p_hist, p_box, ncol = 1L)
```

---
## ばらつきの様子は大小の判断にも重要

<div class="column-container" style="padding-left: 10px;">
<div class="column" style="flex-shrink: 1;">
観測値1つずつ。<br>
たまたまかも。
</div>
<div class="column" style="flex-shrink: 1;">
群内のばらつき大きい。<br>
群間の差は微妙か。
</div>
<div class="column" style="flex-shrink: 1;">
ばらつき小さい。<br>
AとBには差がありそう。
</div>
</div>

```{r, comparison}
#| fig.width: 12
#| fig.height: 4
#| echo: false
set.seed(19937)
n = 20
df1 = tibble::tibble(x = c("A", "B"), y = c(42, 51))
df2 = dplyr::bind_rows(
  tibble::tibble(x = "A", y = runif(n, 42 - 20, 42 + 20)),
  tibble::tibble(x = "B", y = runif(n, 51 - 20, 51 + 20)))
df3 = dplyr::bind_rows(
  tibble::tibble(x = "A", y = rnorm(n, 42, 1)),
  tibble::tibble(x = "B", y = rnorm(n, 51, 1)))
.lim = c(0, max(df2$y, df3$y))
.th = list(coord_cartesian(ylim = .lim),
  theme_classic(base_size = 20),
  theme(legend.position = "none", axis.title = element_blank()))

p1 = ggplot(df1) + aes(x, y, color = x) +
  geom_point(shape = 16, size = 5) +
  .th
p2 = ggplot(df2) + aes(x, y, color = x) +
  geom_jitter(height = 0, width = 0.2, shape = 16, size = 4, alpha = 0.66) +
  .th
cowplot::plot_grid(p1, p2, p2 %+% df3, nrow = 1)
```

「こんなことがたまたま起こる確率はすごく低いです！」<br>
をちゃんと示す手続きが**統計的仮説検定**。

---
## 記述統計量に頼りすぎず分布を可視化する

同じデータでも見せ方で印象・情報量が変わる。

```{r, visualize-distribution}
#| echo: false
#| fig.width: 11
#| fig.height: 7
df = mpg |> dplyr::mutate(y = hwy)
df_mean = df |> dplyr::summarize(y = mean(y))
p0 = ggplot(df) + aes(y = y) +
  theme_classic() +
  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank())

ylim = c(0, max(df$y))
coord_y = coord_cartesian(ylim = ylim)
coord_one = coord_cartesian(ylim = ylim, xlim = c(-1, 1))

pcol = p0 + aes(x = 0) +
  geom_col(data = df_mean, fill = "#999999") +
  stat_summary(fun.data = wtl::mean_sd, geom = "linerange")

set.seed(1)
cowplot::plot_grid(nrow = 2L,
  pcol + coord_one,
  p0 + geom_boxplot() + coord_one,
  p0 + geom_density(fill = "#999999", color = NA) + coord_y,
  p0 + geom_histogram(fill = "#999999", bins = 30L) + coord_y,
  p0 + geom_jitter(aes(x = 0), height = 0, shape = 16, alpha = 0.3, stroke = 0) + coord_one,
  p0 + geom_violin(aes(x = 0), fill = "#999999", color = NA) + coord_one,
  p0 + geom_dotplot(aes(x = 0), binaxis = "y", binwidth = 1, stackratio = 0.8,
    stroke = 0, alpha = 0.66, stackdir = "center") + coord_y,
  p0 + geom_dotplot(aes(x = 0), binaxis = "y", binwidth = 1, stackratio = 0.8,
    stroke = 0, alpha = 0.66) + coord_y
)
```


---
## 代表値ばかり見て可視化を怠ると構造を見逃す

<figure style="position: relative;">
<a href="https://www.research.autodesk.com/publications/same-stats-different-graphs/">
<img src="/slides/image/rstats/datasaurus.png" width="95%">
<figcaption><small>https://www.research.autodesk.com/publications/same-stats-different-graphs/</small></figcaption>
</a>
<img src="/slides/image/rstats/DataDino-600x455.gif" width="22%"
     style="position: absolute; left: 0; top: 0; z-index: 255;">
</figure>


---
## データ可視化は理解の第一歩

情報をうまく絞って整理 → **直感的にわかる**

```{r, simplify-diamonds}
#| echo: false
#| fig.height: 6
#| fig.width: 7
diamonds |>
  dplyr::filter(clarity %in% c("I1", "SI2", "IF")) |>
  ggplot(aes(carat, price, color = clarity)) +
  geom_point(alpha = 0.4, size = 3) +
  scale_color_viridis_d(
    guide = guide_legend(reverse = TRUE, override.aes = list(alpha = 1))
  ) +
  labs(title = "Diamonds") +
  theme_gray(base_size = 22) +
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#bbbbbb"),
    legend.key = element_rect(fill = "#bbbbbb"),
    axis.ticks = element_blank()
  )
```

`carat` が大きいほど `price` も高いらしい。<br>
その度合いは `clarity` によって異なるらしい。

---
## 統計とは

データをうまくまとめ、それに基づいて推論するための手法。

- **記述統計**: データそのものを要約する
    - 要約統計量 (e.g., 平均、標準偏差、etc.)
    - 作図、作表
- **推測統計**: データの背後にある母集団・生成過程を考える
    - 数理モデル
    - 確率分布
    - パラメータ(母数)

「グラフを眺めてなんとなく分かる」以上の分析には**モデル**が必要

---
## モデルとは

対象システムを単純化・理想化して扱いやすくしたもの

Mathematical Model 数理モデル<img src="../tokiomarine2021/image/hill-equation.png" width="210" style="float: right; margin: 0 -5px;">
: 数学的な方程式として記述されるもの。
: e.g., Lotka-Volterra eq., <span style="color: #888;">Hill eq.</span>
: <br>

Computational Model 数値計算モデル<img src="/slides/image/tumopp/Chex_Lconst.gif" width="210" style="float: right;">
: 数値計算の手続きとして記述されるもの。
: e.g., Schelling’s Segregation Model, <span style="color: #888;"><em>tumopp</em></span>
: <br>

Concrete Model 具象モデル<img src="../tokiomarine2021/image/weisberg-sfbay.jpg" width="330" style="float: right;">
: 具体的な事物で作られるもの。
: e.g., San Francisco Bay-Delta Model

<small>
<a href="https://amzn.to/3bdvhuI">Weisberg 2012 <cite>Simulation and Similarity (科学とモデル)</cite></a>
</small>

???
数理モデルが決定論的、数値計算が確率論的、になる場合が多いけど必ずしもそうではない。
解析的に解くことを諦めて計算機にやらせるという点で実装方法は異なるが、
数理的に記述して解釈するという大枠では同じとみなしたほうがいいかもしれない。

プラモデル: 車や飛行機の重さ・材質は無視して色や形を模倣

---
## データ科学における数理モデル

データ生成をうまく真似できそうな仮定の数式表現。<br>
&nbsp;

<figure>
<img src="../tokiomarine2021/math-model.drawio.svg" width="1200"><br>
<figcaption><small>「<a href="https://amzn.to/3uCxTKo"><cite>データ分析のための数理モデル入門</cite></a>」江崎貴裕 2020 より改変</small></figcaption>
</figure>


???

確率モデル: 決定論的なモデルじゃなくて確率論的なゆらぎを導入したもの。
ただし、大塚淳さんの定義は異なる。
帰納推論を可能にする枠組みとして自然の斉一性(ヒューム)を仮定した上で、
データを生成している真の現象を確率用語で記述したものが確率モデルだ、という感じ。
そこからさらに強い仮定としてパラメトリックな確率分布を生成元としたのが統計モデル。

---
## データ科学における数理モデル

データ生成をうまく真似できそうな仮定の数式表現。<br>
e.g., 大きいほど高く売れる: $\text{price} = A \times \text{carat} + B + \epsilon$

```{r, lm-diamonds}
#| echo: false
#| fig.height: 5
#| fig.width: 6
diamonds |>
  dplyr::filter(clarity %in% c("I1", "SI2", "IF")) |>
  ggplot(aes(carat, price)) +
  geom_point(alpha = 0.3, size = 3) +
  stat_smooth(formula = y ~ x, method = lm, se = FALSE) +
  coord_cartesian(ylim = c(0, 20000)) +
  labs(title = "Diamonds") +
  theme_classic(base_size = 22)
```

新しく採れたダイヤモンドの価格予想とかにも使える。

このように「YをXの関数として表す」ようなモデルを**回帰**と呼ぶ。


---
## 本講義の主題: 回帰

単純な直線あてはめから出発し、ちょっとずつ統計モデリング。

<figure>
<a href="https://kuboweb.github.io/-kubo/ce/LinksGlm.html">
<img src="../tokiomarine2021/image/kubo-p2.png" width="1200">
<figcaption><small>久保さん https://kuboweb.github.io/-kubo/ce/LinksGlm.html</small></figcaption>
</a>
</figure>

---
## 何でもかんでも直線あてはめではよろしくない

<img `r src_alt_fig_chunk("glm-better")`>

- 観察データは常に**正の値**なのに予測が負に突入してない？
- **縦軸は整数**。しかもの**ばらつき**が横軸に応じて変化？
- **データに合わせた統計モデルを使うとマシ**


---
## 回帰は教師あり機械学習の一種とも言える

<figure>
<img src="../tokiomarine2021/regression-in-ml.drawio.svg" width="1200">
</figure>

でも統計モデリングはいわゆる“機械学習”とは違う気もする...?


---
## モデリングにおける2つのアプローチ

<figure>
<img src="../tokiomarine2021/model-approaches.drawio.svg" width="1200"><br>
<figcaption><small>「<a href="https://amzn.to/3uCxTKo"><cite>データ分析のための数理モデル入門</cite></a>」江崎貴裕 2020 より改変</small></figcaption>
</figure>

---
## どっちも知っておいて使い分けたい

項目       | 統計モデリング | 近年の機械学習
---------- | -------------- | ----------------
モデル構造 | 単純化したい | 性能のためなら複雑化
モデル解釈 | **ここが強み** | 難しい。重視しない。途上。
予測・生成 | うまくすれば頑健 | **主目的**。強力。高精度
データ量   | 少なくてもそれなり | 大量に必要
計算量     | 場合による  | 場合による
例 | 一般化線形モデル<br>階層ベイズモデル | ランダムフォレスト<br>ニューラルネットワーク

教科書的には概ねこんな感じとして、実際の仕事ではどうだろう？

---
## 現役データサイエンティスト2人に聞きました

- 統計モデルはデータ加工など事前の手続きが多め。<br>
  機械学習は事前の決定が少ないので楽ちん。
- 「要因の効果はどれくらい？」<br>
  意思決定をするのは結局人間。物事を分かった上で判断したい。<br>
  **実務の人への説明**や**意思決定**の場面で解析の解釈性が重要。<br>
- **仮説があるなら**、それに基づいて統計モデリング。<br>
  何もないところからまず機械学習で要因抽出・仮説生成するのもあり。
- 統計モデル縛り・実行環境縛りなどの案件もある。
- 分析方針を決める立場の上級職になるつもりなら統計モデルも必要。

協力: [`@kato_kohaku`](https://twitter.com/kato_kohaku)さん、[`@teuder`](https://twitter.com/teuder)さん


---
## 本講義のお品書き

<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="400" alt="データ解析のための統計モデリング入門 久保拓弥 2012">
</a>
</figure>

久保先生の"緑本"こと<br>
「データ解析のための統計モデリング入門」<br>
をベースに回帰分析の概要を紹介。

1. イントロ 👈 いまここ
1. 統計モデルの基本
    - 直線回帰
    - 確率変数・**確率分布** 👈 本日の主役
    - 尤度・最尤推定
1. 一般化線形モデル、混合モデル
1. ベイズ統計（、階層ベイズモデル）

回帰のキモは**線ではなく分布**


---
## 余裕があれば、手元でRを動かして体感しよう！

1.  こういう枠が出てきたら、自分のRスクリプトに**コピペして保存**:
    ```r
    head(iris)
    ```
1.  **実行してコンソールを確認**。思ったとおりの出力？<br>
    `Error` や `Warning` があったらよく読んで対処する。<br>
1.  🔰若葉マークの練習問題があれば解いてみる。<br>
    そこまでのコードの**コピペ＋改変**でできるはず。

<br>
<hr>

Rの基礎、データ前処理、データ可視化などについては別資料を参照:<br>
<https://heavywatal.github.io/slides/tohoku2024r/1-introduction.html>



---
## Rとは

統計解析と作図の機能が充実したプログラミング言語・環境

<figure style="float: right;">
<a href="https://cran.r-project.org/">
<img src="/slides/image/rstats/Rlogo.svg" height="200">
<figcaption><small>https://cran.r-project.org/</small></figcaption>
</a>
</figure>

クロスプラットフォーム
: Linux, Mac, Windows で動く

オープンソース
: 永久に無償で、すべての機能を使える。
: 集合知によって常に進化している。

コミュニティ
: 相談できる人や参考になるウェブサイトがたくさん見つかる。

他のプログラミング言語でもいいよ:
[Python](https://www.python.org/)とか[Julia](https://julialang.org/)とか。


---
## ちょっとずつ線形モデルを発展させていく

**線形モデル LM** (単純な直線あてはめ)

<span style="color: #888888;">&nbsp; &nbsp; ↓ いろんな確率分布を扱いたい</span>

**一般化線形モデル GLM**

<span style="color: #888888;">&nbsp; &nbsp; ↓ 個体差などの変量効果を扱いたい</span>

**一般化線形混合モデル GLMM**

<span style="color: #888888;">&nbsp; &nbsp; ↓ もっと自由なモデリングを！</span>

**階層ベイズモデル HBM**

<small>「<cite>[データ解析のための統計モデリング入門](https://amzn.to/33suMIZ)</cite>」久保拓弥 2012 より改変</small>



---
## 直線あてはめ: 統計モデルの出発点

<img `r src_alt_fig_chunk("weight-lm")`>

- 身長が高いほど体重も重い。いい感じ。

(説明のために作った架空のデータ。今後もほぼそうです)


---
## 回帰モデルの2段階

1. Define a **family of models**: だいたいどんな形か、式をたてる
    - 直線: $y = a_1 + a_2 x$
    - 対数: $\log(y) = a_1 + a_2 x$
    - 二次曲線: $y = a_1 + a_2 x^2$

2. Generate a **fitted model**: データに合うようにパラメータを調整
    - $y = 3x + 7$
    - $y = 9x^2$

<small><https://r4ds.had.co.nz/model-basics.html></small>


---
## たぶん身長が高いほど体重も重い

なんとなく $y = a x + b$ でいい線が引けそう<br>
&nbsp;

```{r, df-linear}
#| echo: false
set.seed(19937)
n = 50
df_weight = tibble::tibble(
  height = rnorm(n, 1.70, 0.05),
  bmi = rnorm(n, 22, 1),
  weight = bmi * (height**2)
)
```
```{r, weight-height}
#| echo: false
#| fig.height: 5.5
#| fig.width: 5.5
p_weight = ggplot(df_weight) +
  aes(height, weight) +
  geom_point(size = 3, alpha = 0.5, shape = 16) +
  theme_bw(base_size = 22) +
  theme(panel.grid = element_blank())
p_weight
```


---
## たぶん身長が高いほど体重も重い

なんとなく $y = a x + b$ でいい線が引けそう<br>
じゃあ傾き *a* と切片 *b*、どう決める？

```{r, weight-lines}
#| echo: false
#| fig.height: 5.5
#| fig.width: 5.5
set.seed(19937)
df_ab = tibble(intercept = runif(n, -50, 50), slope = runif(n, -200, 200)) |>
  dplyr::mutate(intercept = intercept - 1.7 * slope + 50)
p_weight +
  geom_abline(data = df_ab, aes(intercept = intercept, slope = slope), color = "#3366ff", linewidth = 1, alpha = 0.5)
```


---
## 最小二乗法 (Ordinary Least Square: OLS)

<span style="color: #3366ff">回帰直線</span>からの<strong style="color: #E69F00">残差</strong>平方和(RSS)を最小化する。

```{r, weight-residual}
#| echo: false
#| fig.height: 5.5
#| fig.width: 11
predict_weight = function(parameters, data) {
  parameters[1] + parameters[2] * data$height
}

add_pred_weight = function(data, parameters) {
  dplyr::mutate(data, pred = predict_weight(param1, data = data))
}

rss_weight = function(parameters, data) {
  pred = predict_weight(parameters, data)
  sqdev = (data[["weight"]] - pred)**2
  sum(sqdev)
}
# rss_weight(c(40, -5), df_weight)

param1 = c(10, 30)
lm_weight = lm(weight ~ height, data = df_weight)
rss1 = rss_weight(param1, df_weight)
rss2 = rss_weight(lm_weight$coefficients, df_weight)
stopifnot(abs(sum(lm_weight$residuals**2) - rss2) < 1e-6)

p1 = p_weight %+%
  add_pred_weight(df_weight, param1) +
  geom_line(aes(y = pred), color = "#3366ff", linewidth = 1) +
  geom_linerange(aes(ymin = weight, ymax = pred), color = "#E69F00", linewidth = 1, alpha = 0.8)
p2 = p1 %+%
  (df_weight |> modelr::add_predictions(lm_weight))
cowplot::plot_grid(nrow = 1L,
  p1 + annotate("text", x = -Inf, y = Inf, label = sprintf("RSS = %.1f", rss1), size = 8, hjust = -0.1, vjust = 2),
  p2 + annotate("text", x = -Inf, y = Inf, label = sprintf("RSS = %.1f", rss2), size = 8, hjust = -0.1, vjust = 2))
```


---
## 残差平方和(RSS)が最小となるパラメータを探せ

ランダムに試してみて、上位のものを採用。<br>
この程度の試行回数では足りなそう。

```{r, weight-goodlines}
#| echo: false
#| fig.height: 5.5
#| fig.width: 11
set.seed(19937)
n = 200
df_ab_random = tibble::tibble(intercept = runif(n, -200, 100), slope = runif(n, 0, 150)) |>
  dplyr::mutate(rss = purrr::map2_dbl(intercept, slope, ~ rss_weight(c(.x, .y), data = df_weight)))
p_ab = ggplot(df_ab_random) +
  aes(intercept, slope) +
  geom_point(data = function(x) {
    dplyr::filter(x, rank(rss) < 6)
  }, shape = 1, size = 6) +
  geom_point(aes(color = log10(rss)), shape = 16, size = 3) +
  theme_bw(base_size = 20) +
  theme(panel.grid = element_blank(), legend.position = c(0.99, 0.99), legend.justification = c(1, 1))

df_randmin = df_ab_random |> dplyr::slice_min(rss, n = 5)
p2 = p_weight + geom_abline(
  aes(slope = slope, intercept = intercept),
  data = df_randmin, color = "#3366ff", alpha = 0.5, linewidth = 1
)
cowplot::plot_grid(p_ab, p2, nrow = 1L)
```

---
## 残差平方和(RSS)が最小となるパラメータを探せ

**グリッドサーチ**: パラメータ空間の一定範囲内を均等に試す。<br>
さっきのランダムよりはちょっとマシか。

```{r, weight-grid}
#| echo: false
#| fig.height: 5.5
#| fig.width: 11
df_ab_grid = tidyr::crossing(intercept = seq(-100, -30, 4), slope = seq(50, 100, 4)) |>
  dplyr::mutate(rss = purrr::map2_dbl(intercept, slope, ~ rss_weight(c(.x, .y), data = df_weight)))

p1 = p_ab %+% df_ab_grid

df_gridmin = df_ab_grid |> dplyr::slice_min(rss, n = 5)
p2 = p_weight + geom_abline(
  aes(slope = slope, intercept = intercept),
  data = df_gridmin, color = "#3366ff", alpha = 0.5, linewidth = 1
)
cowplot::plot_grid(p1, p2, nrow = 1L)
```

こうした**最適化**の手法はいろいろあるけど、ここでは扱わない。


---
## これくらいなら一瞬で計算してもらえる

```{r, ols}
par_init = c(intercept = 0, slope = 0)
result = optim(par_init, fn = rss_weight, data = df_weight)
result$par
```

```{r, weight-lm}
#| echo: false
#| fig.height: 4
#| fig.width: 4
label = sprintf("y = %.1f x + %.1f", result$par["slope"], result$par["intercept"])
p_weight +
  stat_smooth(formula = y ~ x, method = lm, se = FALSE, color = "#3366ff", alpha = 0.5) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = label, size = 6, color = "#3366ff")
```

上記コードは最適化一般の書き方。<br>
回帰が目的なら次ページのようにするのが楽 →


---
## `lm()` で直線あてはめしてみる

架空のデータを作る(乱数生成については後述):
```{r, generate-df-linear}
#| ref.label: "df-linear"
#| echo: -1
#| eval: false
```

データと関係式(`Y ~ X` の形)を `lm()` に渡して係数を読む:
```{r, lm-demo}
fit = lm(data = df_weight, formula = weight ~ height)
coef(fit)
```

せっかくなので作図もやってみる→

---
## `lm()` の結果をggplotする

```{r, lm-ggplot}
#| fig.height: 3
#| fig.width: 3
df = modelr::add_predictions(df_weight, fit, type = "response")
head(df, 2L)
ggplot(df) +
  aes(height, weight) +
  geom_point() +
  geom_line(aes(y = pred), linewidth = 1, color = "#3366ff")
```

---
## 🔰 ほかのデータでも `lm()` を試してみよう

```{r, lm-mpg}
#| fig.height: 3
#| fig.width: 3
fit = lm(data = mpg, formula = hwy ~ displ)
coef(fit)

mpg_added = modelr::add_predictions(mpg, fit, type = "response")
ggplot(mpg_added) + aes(displ, hwy) + geom_point() +
  geom_line(aes(y = pred), linewidth = 1, color = "#3366ff")
```

🔰 `diamonds` などほかのデータでも `lm()` を試してみよう。


---
## 何でもかんでも直線あてはめではよろしくない

```{r, df-pois}
#| echo: false
set.seed(24601)
n = 300L
a = 3
b = -3
df_pois = tibble::tibble(
  body_mass = runif(n, 0.4, 1.7),
  num_seeds = rpois(n, exp(a * body_mass + b))
)
```
```{r, lm-bad}
#| echo: false
#| fig.height: 5.5
#| fig.width: 5.5
x_breaks = c(0.5, 1.0, 1.5)
coeff = lm(num_seeds ~ body_mass, data = df_pois)$coefficients
df_lm = tidyr::crossing(body_mass = x_breaks, num_seeds = seq(-5, 20, 0.1)) |>
  dplyr::mutate(density = dnorm(num_seeds, coeff[1] + coeff[2] * body_mass, 1.4)) |>
  dplyr::filter(density > 1e-4)

p_pois = ggplot(df_pois) +
  aes(body_mass, num_seeds) +
  ggridges::geom_vridgeline(data = df_lm, aes(width = density * 0.4, group = body_mass), linetype = 0, alpha = 0) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  scale_x_continuous(breaks = x_breaks) +
  coord_cartesian(ylim = c(-5, 15)) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())
p_pois + stat_smooth(formula = y ~ x, method = lm, se = FALSE)
```

- 観察データは常に**正の値**なのに予測が負に突入してない？
- **縦軸は整数**。しかもの**ばらつき**が横軸に応じて変化？


---
## 何でもかんでも直線あてはめではよろしくない

```{r, glm-better}
#| echo: false
#| fig.height: 5.5
#| fig.width: 11
p_lm = p_pois +
  stat_smooth(formula = y ~ x, method = lm, se = FALSE) +
  ggridges::geom_vridgeline(aes(width = density * 0.4, group = body_mass), df_lm,
                            fill = "#56B4E9AA", linetype = 0)
# p_lm

df_ridges = tidyr::crossing(body_mass = x_breaks, num_seeds = seq_len(30L) - 1L) |>
  dplyr::mutate(density = dpois(num_seeds, exp(a * body_mass + b))) |>
  dplyr::filter(density > 1e-4)
df_bars = df_ridges |> wtl::ridges2bars(num_seeds, density)

p_poisson = p_pois +
  stat_smooth(formula = y ~ x, method = glm, method.args = list(family = poisson), se = FALSE) +
  ggridges::geom_vridgeline(aes(width = density * 0.5, group = body_mass), df_bars,
                            fill = "#56B4E9AA", linetype = 0)
# p_poisson

cowplot::plot_grid(p_lm, p_poisson, nrow = 1L)
```

- 観察データは常に**正の値**なのに予測が負に突入してない？
- **縦軸は整数**。しかもの**ばらつき**が横軸に応じて変化？
- **データに合わせた統計モデルを使うとマシ**


---
## ちょっとずつ線形モデルを発展させていく

**線形モデル LM** (単純な直線あてはめ)

<span style="color: #888888;">&nbsp; &nbsp; ↓ いろんな<span class="fragment highlight-blue custom bold">確率分布</span>を扱いたい</span>

**一般化線形モデル GLM**

<span style="color: #888888;">&nbsp; &nbsp; ↓ 個体差などの変量効果を扱いたい</span>

**一般化線形混合モデル GLMM**

<span style="color: #888888;">&nbsp; &nbsp; ↓ もっと自由なモデリングを！</span>

**階層ベイズモデル HBM**

<small>「<cite>[データ解析のための統計モデリング入門](https://amzn.to/33suMIZ)</cite>」久保拓弥 2012 より改変</small>


---
## 確率分布

発生する事象(値)と頻度の関係。

手元のデータを数えて作るのが**経験分布**<br>
e.g., サイコロを12回投げた結果、学生1000人の身長

```{r, distribution}
#| echo: false
#| fig.height: 4.4
#| fig.width: 8.8
set.seed(19937)
p1 = tibble::tibble(face = sample.int(6, 12, replace = TRUE)) |>
  ggplot() +
  aes(face) +
  geom_bar(aes(y = after_stat(prop))) +
  scale_x_continuous(breaks = seq_len(6L)) +
  theme_bw(base_size = 20) +
  theme(
    panel.grid.minor.y = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
    axis.ticks = element_blank()
  )

p2 = tibble::tibble(height = rnorm(1000L, c(160, 170), 5.5)) |>
  ggplot() +
  aes(height) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 1, boundary = 0) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor.x = element_blank(), axis.ticks = element_blank())
cowplot::plot_grid(p1, p2, nrow = 1L)
```

一方、少数のパラメータと数式で作るのが**理論分布**。<br>
(こちらを単に「確率分布」と呼ぶことが多い印象）

---
## 確率変数$X$はパラメータ$\theta$の確率分布$f$に従う...?

$X \sim f(\theta)$

e.g.,<br>
コインを3枚投げたうち表の出る枚数 $X$ は**二項分布に従う**。<br>
$X \sim \text{Binomial}(n = 3, p = 0.5)$

<div class="column-container">
  <div class="column" style="flex-shrink: 2.0;">

```{r, dbinom}
#| echo: false
#| fig.height: 3.5
#| fig.width: 3.5
size = 3L
p = 0.5
tibble(X = seq(0, 3), Prob = dbinom(X, size, p), obs = Inf) |>
  ggplot() +
  aes(X, Prob) +
  geom_col() +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  theme_bw(base_size = 22) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

  </div>
  <div class="column" style="padding-top: 10px;">
\[\begin{split}
\Pr(X = k) &= \binom n k p^k (1 - p)^{n - k} \\
k &\in \{0, 1, 2, \ldots, n\}
\end{split}\]
  </div>
</div>

一緒に実験してみよう。

---
## 試行を繰り返して記録してみる

コインを3枚投げたうち表の出た枚数 $X$

試行1: **表** 裏 **表** → $X = 2$<br>
試行2: 裏 裏 裏 → $X = 0$<br>
試行3: **表** 裏 裏 → $X = 1$ 続けて $2, 1, 3, 0, 2, \ldots$

```{r, rbinom}
#| echo: false
#| fig.height: 3
#| fig.width: 12
set.seed(19937)
size = 3L
p = 0.5
X = c(2L, 0L, 1L, 2L, 1L, 3L, 0L, 2L, rbinom(92L, size, p))
df_rbinom = purrr::map(c(1, 2, 3, 10, 100), \(n) {
  tibble::tibble(X = head(X, n)) |>
    dplyr::count(X, name = "k") |>
    dplyr::mutate(Freq = k / n, Repl = n)
}) |>
  purrr::list_rbind() |>
  dplyr::bind_rows(tibble(X = seq(0, 3), Freq = dbinom(X, size, p), Repl = Inf))
df_rbinom |>
  ggplot() +
  aes(X, Freq) +
  geom_col() +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  facet_wrap(vars(Repl), nrow = 1L, labeller = label_both) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

<div style="text-align: right;">
試行回数を増やすほど<b>二項分布</b>の形に近づく。<br>
0と3はレア。1と2が3倍ほど出やすいらしい。
</div>

---
## コイントスしなくても $X$ らしきものを生成できる

- コインを3枚投げたうち表の出る枚数 $X$
- $n = 3, p = 0.5$ の二項分布からサンプルする乱数 $X$

<div class="column-container">
  <div class="column" style="flex-shrink: 2.0;">
<img `r src_alt_fig_chunk("dbinom")`>
  </div>
  <div class="column" style="padding-top: 10px;">
$X \sim \text{Binomial}(n = 3, p = 0.5)$

&nbsp;&nbsp; ↓ サンプル

{2, 0, 1, 2, 1, 3, 0, 2, ...}
  </div>
</div>

これらはとてもよく似ているので<br>
**「コインをn枚投げたうち表の出る枚数は二項分布に従う」**<br>
みたいな言い方をする。逆に言うと<br>
**「二項分布とはn回試行のうちの成功回数を確率変数とする分布」**<br>
のように理解できる。

---
## 統計モデリングの一環とも捉えられる

コイン3枚投げを繰り返して得たデータ {2, 0, 1, 2, 1, 3, 0, 2, ...}

&nbsp;&nbsp; ↓ たった2つのパラメータで記述。情報を圧縮。

$n = 3, p = 0.5$ の二項分布で説明・再現できるぞ

<figure>
<img src="../tokiomarine2021/math-model.drawio.svg" width="960"><br>
<figcaption><small>「<a href="https://amzn.to/3uCxTKo"><cite>データ分析のための数理モデル入門</cite></a>」江崎貴裕 2020 より改変</small></figcaption>
</figure>

こういうふうに現象と対応した確率分布、ほかにもある？

???
ただし「これが3連コイントスの真理だ」ではない。<br>
あくまで「こう単純化して理解できそう・使えそう」なだけ。

ほかの仮定: コインが立つかも。偏ったコインかも。両表かも。

二項分布はn枚コイントスをたった2パラメータで説明する優秀モデル


---
## 有名な確率分布、それに「従う」もの

離散一様分布
: コインの表裏、サイコロの出目1–6

負の二項分布 (幾何分布 if n = 1)
: 成功率pの試行がn回成功するまでの失敗回数

二項分布
: 成功率p、試行回数nのうちの成功回数

ポアソン分布
: 単位時間あたり平均$\lambda$回起こる事象の発生回数

ガンマ分布 (指数分布 if k = 1)
: ポアソン過程でk回起こるまでの待ち時間

正規分布
: 確率変数の和、平均値など。

---
## 離散一様分布

同じ確率で起こるn通りの事象のうちXが起こる確率

e.g., コインの表裏、サイコロの出目1–6

```{r, dunif}
#| echo: false
#| fig.height: 4
#| fig.width: 6
df_coin = tibble::tibble(X = c("head", "tail"), Prob = c(0.5, 0.5), group = "Coin")
df_dice = tibble::tibble(X = as.character(seq_len(6L)), Prob = rep(1 / 6, 6), group = "Dice")
dplyr::bind_rows(df_coin, df_dice) |>
  ggplot() +
  aes(X, Prob) +
  geom_col() +
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1)) +
  facet_grid(cols = vars(group), scale = "free_x", space = "free_x") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

🔰 一様分布になりそうな例を考えてみよう


---
## 幾何分布 $~\text{Geom}(p)$

成功率pの試行が初めて成功するまでの失敗回数

e.g., コイントスで表が出るまでに何回裏が出るか

```{r, geometric}
#| echo: false
#| fig.height: 4
#| fig.width: 12
df = purrr::map(c(0.2, 0.5, 0.9), \(p) {
  tibble::tibble(p, X = seq(0, 25), Prob = dgeom(X, p))
}) |>
  purrr::list_rbind() |>
  dplyr::filter(Prob > 0.001)
x_br = c(seq.int(0, 10), seq.int(15, 100, 5))
ggplot(df) +
  aes(X, Prob) +
  scale_x_continuous(breaks = x_br) +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  geom_col() +
  facet_grid(cols = vars(p), scales = "free_x", space = "free_x", labeller = label_both) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

\\[
\Pr(X = k \mid p) = p (1 - p)^k
\\]

「初めて成功するまでの試行回数」とする定義もある。

🔰 幾何分布になりそうな例を考えてみよう

---
## 負の二項分布 $~\text{NB}(n, p)$

成功率pの試行がn回成功するまでの失敗回数X。
n = 1 のとき幾何分布と一致。

```{r, nbinom}
#| echo: false
#| fig.height: 3.8
#| fig.width: 12
p = 0.5
df = purrr::map(seq.int(1, 3), function(n) {
  tibble::tibble(X = seq.int(0, 10), Prob = dnbinom(X, n, p), n = n)
}) |>  purrr::list_rbind()
ggplot(df) +
  aes(X, Prob) +
  scale_x_continuous(breaks = df[["X"]]) +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  geom_col() +
  facet_grid(cols = vars(n), scales = "free_x", space = "free_x", labeller = label_both) +
  labs(title = paste0("p = ", p)) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

\\[
\Pr(X = k \mid n,~p) = \binom {n + k - 1} k p^n (1 - p)^k
\\]

失敗回数ではなく試行回数を変数とする定義もある。

🔰 負の二項分布になりそうな例を考えてみよう

<!--
平均$\lambda$がガンマ分布でばらついたポアソン分布、とも解釈できる。<br>
($k \to \infty$でポアソン分布と一致)
-->


---
## 二項分布 $~\text{Binomial}(n,~p)$

確率$p$で当たるクジを$n$回引いてX回当たる確率。平均は$np$。

```{r, dbinom-n}
#| echo: false
#| fig.height: 4
#| fig.width: 12
p = 0.25
df = purrr::map(2**seq.int(0, 4), \(n) {
  tibble::tibble(X = seq(0, n), Prob = dbinom(X, n, p), n = n)
}) |> purrr::list_rbind()
ggplot(df) +
  aes(X, Prob) +
  scale_x_continuous(breaks = df[["X"]]) +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  geom_col() +
  facet_grid(cols = vars(n), scales = "free_x", space = "free_x", labeller = label_both) +
  labs(title = paste0("p = ", p)) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

\\[
\Pr(X = k \mid n,~p) = \binom n k p^k (1 - p)^{n - k}
\\]

🔰 二項分布になりそうな例を考えてみよう


---
## ポアソン分布 $~\text{Poisson}(\lambda)$

平均$\lambda$で単位時間(空間)あたりに発生する事象の回数。

e.g., 1時間あたりのメッセージ受信件数、メッシュ区画内の生物個体数

```{r, dpoisson}
#| echo: false
#| fig.height: 3.9
#| fig.width: 12
set.seed(19937)
lambda = c(1, 5, 10)
.arrow = grid::arrow(length = unit(0.1, "inches"), type = "closed")
p_poisson_process = tibble::tibble(y = rev(seq_along(lambda)), lambda) |>
  dplyr::mutate(time = purrr::map(lambda, ~ runif(.x * 3, 0, 3))) |>
  tidyr::unnest(time) |>
  ggplot() +
  aes(time, y) +
  annotate("segment", x = -0.1, xend = 3.1, y = 1:3, yend = 1:3, linewidth = 2, arrow = .arrow, linejoin = "mitre") +
  geom_point(aes(color = lambda, fill = lambda), size = 10, shape = 124, key_glyph = draw_key_rect) +
  scale_color_continuous(guide = NULL) +
  scale_fill_continuous(guide = guide_legend(label.position = "top", title.vjust = 1), breaks = lambda) +
  scale_y_continuous(limits = c(0.5, 3.5), breaks = NULL) +
  labs(y = NULL, x = "time (space)") +
  theme_bw(base_size = 20) +
  theme(
    axis.text.y = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(),
    panel.grid.minor = element_blank(), legend.position = "top"
  )

p2 = tidyr::crossing(X = seq.int(0L, 20L), lambda) |>
  dplyr::mutate(Prob = dpois(X, lambda)) |>
  ggplot() +
  aes(X, Prob) +
  geom_col(aes(fill = lambda), position = "identity", alpha = 0.5) +
  theme_bw(base_size = 20) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank(), legend.position = "none"
  )
cowplot::plot_grid(p_poisson_process, p2, nrow = 1L, rel_widths = c(4, 3))
```

\\[
\Pr(X = k \mid \lambda) = \frac {\lambda^k e^{-\lambda}} {k!}
\\]

二項分布の極限 $(\lambda = np;~n \to \infty;~p \to 0)$。<br>
めったに起きないことを何回も試行するような感じ。


---
## 指数分布 $~\text{Exp}(\lambda)$

ポアソン過程の事象の発生間隔。平均は $1 / \lambda$ 。

e.g., メッセージの受信間隔、道路沿いに落ちてる手袋の間隔

```{r, dexp}
#| echo: false
#| fig.height: 3.9
#| fig.width: 12
p2 = tidyr::crossing(x = seq(0, 3, length.out = 201), lambda) |>
  dplyr::mutate(Prob = dexp(x, rate = lambda)) |>
  ggplot() +
  aes(x, Prob) +
  geom_area(aes(fill = lambda, group = lambda), position = "identity", alpha = 0.5) +
  theme_bw(base_size = 20) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank(), legend.position = "none"
  )
cowplot::plot_grid(p_poisson_process, p2, nrow = 1L, rel_widths = c(4, 3))
```

\\[
\Pr(x \mid \lambda) = \lambda e^{-\lambda x}
\\]

幾何分布の連続値版。

🔰 ポアソン分布・指数分布になりそうな例を考えてみよう


---
## ガンマ分布 $~\text{Gamma}(k,~\lambda)$

ポアソン過程の事象k回発生までの待ち時間

e.g., メッセージを2つ受信するまでの待ち時間

```{r, dgamma}
#| echo: false
#| fig.height: 3.9
#| fig.width: 12
p2 = tidyr::crossing(x = seq(0, 3, length.out = 201), lambda) |>
  dplyr::mutate(Prob = dgamma(x, rate = lambda, shape = 3)) |>
  ggplot() +
  aes(x, Prob) +
  geom_area(aes(fill = lambda, group = lambda), position = "identity", alpha = 0.5) +
  theme_bw(base_size = 20) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank(), legend.position = "none"
  )
cowplot::plot_grid(p_poisson_process, p2, nrow = 1L, rel_widths = c(4, 3))
```

\\[
\Pr(x \mid k,~\lambda) = \frac {\lambda^k x^{k - 1} e^{-\lambda x}} {\Gamma(k)}
\\]

指数分布をkのぶん右に膨らませた感じ。<br>
shapeパラメータ $k = 1$ のとき指数分布と一致。


---
## 正規分布 $~\mathcal{N}(\mu,~\sigma)$

平均 $\mu$、標準偏差 $\sigma$ の美しい分布。よく登場する。<br>
e.g., $\mu = 50, ~\sigma = 10$ (濃い灰色にデータの95%, 99%が含まれる):

```{r, gaussian}
#| echo: false
#| fig.height: 5
#| fig.width: 12
ci = qnorm(c(0.005, 0.025, 0.975, 0.995), 50, 10)
tibble::tibble(x = seq(0, 100, 0.1), Prob = dnorm(x, 50, 10)) |>
  ggplot() +
  aes(x, Prob) +
  geom_area(alpha = 0.4) +
  geom_area(data = function(.x) {
    dplyr::filter(.x, dplyr::between(x, ci[2], ci[3]))
  }, alpha = 0.4) +
  geom_area(data = function(.x) {
    dplyr::filter(.x, dplyr::between(x, ci[1], ci[4]))
  }, alpha = 0.4) +
  theme_bw(base_size = 20) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank(), legend.position = "none"
  )
```

\\[
\Pr(x \mid \mu,~\sigma) = \frac 1 {\sqrt{2 \pi \sigma^2}} \exp \left(\frac {-(x - \mu)^2} {2\sigma^2} \right)
\\]

---
## 正規分布に近づくものがいろいろある

標本平均の反復(**中心極限定理**);
e.g., 一様分布 [0, 100) から40サンプル

```{r, central-limit}
#| echo: false
#| fig.height: 3
#| fig.width: 12
set.seed(19937)
n = 40L
X = replicate(10000L, mean(runif(n, 0, 100)))
purrr::map(c(10, 100, 1000, 10000), \(n) {
  tibble::tibble(X = head(X, n), Repl = n)
}) |>
  purrr::list_rbind() |>
  ggplot() +
  aes(X) +
  geom_histogram(bins = 25) +
  facet_wrap(vars(Repl), nrow = 1L, scale = "free_y", labeller = label_both) +
  theme_bw(base_size = 20) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
    axis.ticks = element_blank()
  )
```

大きい$n$の二項分布

```{r, binom-normal}
#| echo: false
#| fig.height: 3
#| fig.width: 12
purrr::map(c(1, 4, 16, 64, 256), \(n) {
  tibble::tibble(X = seq(0, n), Prob = dbinom(X, n, 0.25), n = n) |>
    dplyr::filter(Prob > 1e-5)
}) |>
  purrr::list_rbind() |>
  ggplot() +
  aes(X, Prob) +
  geom_col(width = 1) +
  facet_wrap(vars(n), nrow = 1L, scale = "free", labeller = label_both) +
  theme_bw(base_size = 20) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
    axis.ticks = element_blank()
  )
```

---
## 正規分布に近づくものがいろいろある

大きい$\lambda$のポアソン分布

```{r, poisson-normal}
#| echo: false
#| fig.height: 2.5
#| fig.width: 12
purrr::map(c(1, 4, 16, 64, 256), \(lambda) {
  tibble::tibble(X = seq(0, 4 * lambda), Prob = dpois(X, lambda), lambda) |>
    dplyr::filter(Prob > 1e-5)
}) |>
  purrr::list_rbind() |>
  ggplot() +
  aes(X, Prob) +
  geom_col(width = 1) +
  facet_wrap(vars(lambda), nrow = 1L, scale = "free", labeller = label_both) +
  theme_bw(base_size = 20) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
    axis.ticks = element_blank()
  )
```

平均値固定なら$k$が大きくなるほど左右対称に尖るガンマ分布

```{r, gamma-normal}
#| echo: false
#| fig.height: 3.8
#| fig.width: 12
.guide = guide_legend(reverse = TRUE, label.position = "left", label.hjust = 1)
tidyr::crossing(x = seq(0, 25, length.out = 300), k = 10**seq.int(0, 3)) |>
  dplyr::mutate(Prob = dgamma(x, rate = k / 10, shape = k)) |>
  ggplot() +
  aes(x, Prob) +
  geom_area(aes(fill = k, group = k), position = "identity", alpha = 0.5) +
  scale_fill_viridis_c(trans = "log10", guide = .guide) +
  coord_cartesian(xlim = c(0, 20)) +
  theme_bw(base_size = 20) +
  theme(
    panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
    axis.ticks = element_blank()
  )
```


---
## 有名な確率分布対応関係ふりかえり

<figure style="float: right;">
<img src="../tokiomarine2021/math-model.drawio.svg" width="420"><br>
</figure>

離散一様分布
: コインの表裏、サイコロの出目1–6

負の二項分布 (幾何分布 if n = 1)
: 成功率pの試行がn回成功するまでの失敗回数

二項分布
: 成功率p、試行回数nのうちの成功回数

ポアソン分布
: 単位時間あたり平均$\lambda$回起こる事象の発生回数

ガンマ分布 (指数分布 if k = 1)
: ポアソン過程でk回起こるまでの待ち時間

正規分布
: 確率変数の和、平均値。使い勝手が良く、よく登場する。


---
## 現実には、確率分布に「従わない」ことが多い

植物100個体から8個ずつ種子を取って植えたら全体で半分ちょい発芽。<br>
親1個体あたりの生存数は<span style="color: #56B4E9;">n=8の二項分布</span>になるはずだけど、<br>
極端な値(全部死亡、全部生存)が多かった。

<img `r src_alt_fig_chunk("overdispersion")`>

「それはなぜ？」と考えて要因を探るのも統計モデリングの仕事。<br>
**「普通はこれに従うはず」を理解してこそ**できる思考。


---
## 疑似乱数生成器 Pseudo Random Number Generator

コンピューター上でランダム**っぽい**数値を出力する装置。<br>
実際には**決定論的**に計算されているので、<br>
シード(出発点)と呼び出し回数が同じなら出る数も同じになる。

```r
set.seed(42)
runif(3L)
# 0.9148060 0.9370754 0.2861395
runif(3L)
# 0.8304476 0.6417455 0.5190959
set.seed(42)
runif(6L)
# 0.9148060 0.9370754 0.2861395 0.8304476 0.6417455 0.5190959
```

シードに適当な固定値を与えておくことで再現性を保てる。<br>
ただし「このシードじゃないと良い結果が出ない」はダメ。

さまざまな「分布に従う」乱数を生成することもできる。

---
## いろんな乱数を生成・可視化して感覚を掴もう

```{r, try-prng}
#| eval: false
n = 100
x = sample.int(6, n, replace = TRUE)  # 一様分布(整数)
x = runif(n, min = 0, max = 1)        # 一様分布
x = rgeom(n, prob = 0.5)              # 幾何分布
x = rbinom(n, size = 3, prob = 0.5)   # 二項分布
x = rpois(n, lambda = 10)             # ポアソン分布
x = rnorm(n, mean = 50, sd = 10)      # 正規分布
print(x)

p1 = ggplot(data.frame(x)) + aes(x)
p1 + geom_histogram() # for continuous values
p1 + geom_bar()       # for discrete values
```

🔰 小さい `n` から徐々に大きくして変化を確認しよう。

🔰 ほかのオプションもいろいろ変えて変化を確認しよう。

🔰 1%の当たりを狙って10連ガチャを回す人が100万人いたら、<br>
   全部はずれ、1つ当たり、2つ当たり... の人はどれくらいいるか？

```{r, hidden-gacha}
#| include: false
x = rbinom(1000000, 10, 0.01)
p = ggplot(data.frame(x)) + aes(x) + geom_bar()
table(x)
```

---
## 本講義のお品書き

<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="400" alt="データ解析のための統計モデリング入門 久保拓弥 2012">
</a>
</figure>

久保先生の"緑本"こと<br>
「データ解析のための統計モデリング入門」<br>
をベースに回帰分析の概要を紹介。

1. イントロ
1. 統計モデルの基本
    - 直線回帰
    - 確率変数・**確率分布** 👈 ここまでやった
    - 尤度・最尤推定
1. 一般化線形モデル、混合モデル
1. ベイズ統計（、階層ベイズモデル）

回帰のキモは**線ではなく分布**


---
## 参考文献

- [データ解析のための統計モデリング入門](https://amzn.to/33suMIZ) 久保拓弥 2012
- [StanとRでベイズ統計モデリング](https://amzn.to/3uwx7Pb) 松浦健太郎 2016
- [RとStanではじめる ベイズ統計モデリングによるデータ分析入門](https://amzn.to/3o1eCzP) 馬場真哉 2019
- [データ分析のための数理モデル入門](https://amzn.to/3uCxTKo) 江崎貴裕 2020
- [分析者のためのデータ解釈学入門](https://amzn.to/3uznzCK) 江崎貴裕 2020
- [統計学を哲学する](https://amzn.to/3ty80Kv) 大塚淳 2020
- 「[進化学実習2024](/slides/tohoku2024r/)」
  東北大学 理学部生物学科 (2024-04)
- 「[Rを用いたデータ解析の基礎と応用](https://comicalcommet.github.io/r-training-2023/)」
  石川由希 2023 名古屋大学
- 「[統計モデリング概論 DSHC 2023](/slides/tokiomarine2023/)」
  東京海上 [DSHC](https://tokiomarine-dshc.com/) (2023-08)

`r .meta$next_link`
