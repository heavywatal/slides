+++
url = "hokudai2021r/6-stats-model.html"
title = "6. 統計モデリング基礎 — Rにやらせて楽しよう 2021 北大"
linktitle = "統計モデリング基礎: 確率分布、尤度、一般化線形モデル"
date = 2021-09-15T13:00:00+09:00
type = "reveal"
draft = false
+++


# [Rにやらせて楽しよう — データの可視化と下ごしらえ](.)

<div class="author">
岩嵜 航 (Watal M. Iwasaki, PhD)
</div>

<div class="affiliation">
東北大学 生命科学研究科 進化ゲノミクス分野 特任助教<br>
(Graduate School of Life Sciences, Tohoku University)
</div>

<ol>
<li><a href="1-introduction.html">入門1: データ解析の全体像。Rを使うメリット。Rの基本。</a>
<li><a href="2-visualization.html">入門2: データ可視化の重要性と方法。</a>
<li><a href="3-structure1.html">データ構造の処理1: 抽出、集約など。</a>
<li><a href="4-structure2.html">データ構造の処理2: 結合、変形など。</a>
<li><a href="5-content.html">データ内容の処理: 数値、文字列、日時など。</a>
<li class="current-deck"><a href="6-stats-model.html">統計モデリング基礎: 確率分布、尤度、一般化線形モデル</a>
</ol>

<div class="footnote">
2021-09-13 北海道大学 生命科学院 特別講義
<a href="https://heavywatal.github.io/slides/hokudai2021r/">https://heavywatal.github.io/slides/hokudai2021r/</a>
</div>

```{r setup-global, include=FALSE, code=readLines("setup.R")}
```

```{r setup-local, include=FALSE}
library(ggplot2)
library(tibble)
library(dplyr)
library(tidyr)
knitr::opts_chunk$set(cache = TRUE)
```


---
## データを使ってやりたいこと

- 現象を**理解**したい
- 将来を**予測**したい
- ものを**分類・判別**したい
- 挙動を**制御**したい
- 新しい何かを**生成**したい

そのために解析は必要？
未加工の生データこそ宝？

---
## データ解析って必要？ 生データ見ればいいべ？

往々にして複雑過ぎ、情報多すぎ、そのままでは手に負えない

```{r diamonds}
print(ggplot2::diamonds)
```

ダイヤモンド53,940個について10項目の値を持つデータセット

---
## 要約統計量を見てみよう

各列の**平均**とか**標準偏差**とか:

```{r summary-diamonds, echo = FALSE}
diamonds %>%
  dplyr::summarize(across(where(is.numeric), list(mean = mean, sd = sd, max = max))) %>%
  dplyr::mutate(across(where(is.numeric), round, digits = 2)) %>%
  tidyr::pivot_longer(everything(), names_to = c(".value", "stat"), names_sep = "_")
```

大きさ `carat` と価格 `price` の**相関係数**は 0.92 (かなり高い)。
```{r cor-diamonds, eval = FALSE, include = FALSE}
cor(diamonds$carat, diamonds$price)
# [1] 0.9215913
```

**生のままよりは把握しやすい**かも。

しかし要注意...

---
## 平均値ばかり見て可視化を怠ると構造を見逃す

<figure style="position: relative;">
<a href="https://www.autodesk.com/research/publications/same-stats-different-graphs">
<img src="/slides/image/rstats/datasaurus.png" width="800">
<figcaption class="url">https://www.autodesk.com/research/publications/same-stats-different-graphs</figcaption>
</a>
<img src="/slides/image/rstats/DataDino-600x455.gif" width="180"
     style="position: absolute; left: 0; top: 0; z-index: 255;">
</figure>


---
## データ可視化は理解の第一歩

情報をうまく絞って整理 → **直感的にわかる**

<img src="figure/simplify-diamonds-1.png" alt="plot of chunk simplify-diamonds">

`carat` が大きいほど `price` も高いらしい。<br>
その度合いは `clarity` によって異なるらしい。

---
## 統計とは

データをうまくまとめ、それに基づいて推論するための手法。

- **記述統計**: データそのものを要約する
    - 要約統計量 (e.g., 平均、標準偏差、etc.)
    - 作図、作表
- **推測統計**: データの背後にある母集団・生成過程を考える
    - 数理モデル
    - 確率分布
    - パラメータ(母数)

「グラフを眺めてなんとなく分かる」以上の分析には**モデル**が必要

---
## モデルとは

対象システムを単純化・理想化して扱いやすくしたもの

Mathematical Model 数理モデル<img src="../tokiomarine2021/image/hill-equation.png" width="150" align="right" style="margin: 0 -5px;">
: 数学的な方程式として記述されるもの。
: e.g., Lotka-Volterra eq., <span style="color: #888;">Hill eq.</span>
: <br>

Computational Model 数値計算モデル<img src="/slides/image/tumopp/Chex_Lconst.gif" width="140" align="right">
: 数値計算の手続きとして記述されるもの。
: e.g., Schelling’s Segregation Model, <span style="color: #888;"><em>tumopp</em></span>
: <br>

Concrete Model 具象モデル<img src="../tokiomarine2021/image/weisberg-sfbay.jpg" width="260" align="right">
: 具体的な事物で作られるもの。
: e.g., San Francisco Bay-Delta Model

<cite>
Weisberg 2012 "Simulation and Similarity" (科学とモデル)
</cite>

???
数理モデルが決定論的、数値計算が確率論的、になる場合が多いけど必ずしもそうではない。
解析的に解くことを諦めて計算機にやらせるという点で実装方法は異なるが、
数理的に記述して解釈するという大枠では同じとみなしたほうがいいかもしれない。

プラモデル: 車や飛行機の重さ・材質は無視して色や形を模倣
<img src="../lifesci2020seminar/image/schelling-segregation.gif" width="160" align="right" style="margin: -20px -15px; height: 160px; object-fit: cover;">

---
## データ科学における数理モデル

データ生成をうまく真似できそうな仮定の数式表現。<br>
&nbsp;

<figure>
<img src="../tokiomarine2021/math-model.drawio.svg"><br>
<figcaption><cite>「データ分析のための数理モデル入門」江崎貴裕 2020 より改変</cite></figcaption>
</figure>


???

確率モデル: 決定論的なモデルじゃなくて確率論的なゆらぎを導入したもの。
ただし、大塚淳さんの定義は異なる。
帰納推論を可能にする枠組みとして自然の斉一性(ヒューム)を仮定した上で、
データを生成している真の現象を確率用語で記述したものが確率モデルだ、という感じ。
そこからさらに強い仮定としてパラメトリックな確率分布を生成元としたのが統計モデル。

---
## データ科学における数理モデル

データ生成をうまく真似できそうな仮定の数式表現。<br>
e.g., 大きいほど高く売れる: $\text{price} = A \times \text{carat} + B + \epsilon$

```{r lm-diamonds, echo = FALSE, fig.height = 5, fig.width = 6}
diamonds %>%
  dplyr::filter(clarity %in% c("I1", "SI2", "IF")) %>%
  ggplot(aes(carat, price)) +
  geom_point(alpha = 0.3, size = 3) +
  stat_smooth(formula = y ~ x, method = lm, se = FALSE) +
  coord_cartesian(ylim = c(0, 20000)) +
  labs(title = "Diamonds") +
  theme_classic(base_size = 22)
```

新しく採れたダイヤモンドの価格予想とかにも使える。

このように「YをXの関数として表す」ようなモデルを**回帰**と呼ぶ。


---
## 今回は回帰を軸とした統計モデリングの解説

単純な直線あてはめから出発し、ちょっとずつ統計モデリング。

<figure>
<a href="https://kuboweb.github.io/-kubo/ce/LinksGlm.html">
<img src="../tokiomarine2021/image/kubo-p2.png" width="100%">
<figcaption class="url">久保さん https://kuboweb.github.io/-kubo/ce/LinksGlm.html</figcaption>
</a>
</figure>


---
## 回帰は教師あり機械学習の一種とも言える

<figure>
<img src="../tokiomarine2021/regression-in-ml.drawio.svg">
</figure>

でも統計モデリングはいわゆる“機械学習”とは違う気もする...?


---
## モデリングにおける2つのアプローチ

<figure>
<img src="../tokiomarine2021/model-approaches.drawio.svg"><br>
<figcaption><cite>「データ分析のための数理モデル入門」江崎貴裕 2020 より改変</cite></figcaption>
</figure>

---
## どっちも知っておいて使い分けたい

項目       | 統計モデリング | 近年の機械学習
---------- | -------------- | ----------------
例 | 一般化線形モデル<br>階層ベイズモデル | ランダムフォレスト<br>ニューラルネットワーク
モデル構造 | 単純化したい | 性能のためなら複雑化
モデル解釈 | **ここが強み** | 難しい。重視しない。途上。
予測・生成 | うまくすれば頑健 | **主目的**。強力。高精度
データ量   | 少なくてもそれなり | 大量に必要
計算量     | 場合による  | 場合による


---
## 本講義のお品書き

<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="300" alt="データ解析のための統計モデリング入門 久保拓弥 2012">
</a>
</figure>

久保先生の"緑本"こと<br>
「データ解析のための統計モデリング入門」<br>
をベースに回帰分析の概要を紹介。

1. イントロ
1. 統計モデルの基本
    - 確率変数・**確率分布** 👈 本日の主役
    - 尤度・最尤推定
1. 一般化線形モデル、<del>混合モデル</del>
1. <del>ベイズ統計、階層ベイズモデル</del>

回帰のキモは**線ではなく分布**


---
## 回帰モデルの2段階

1. Define a **family of models**: だいたいどんな形か、式をたてる
    - 直線: $y = a_1 + a_2 x$
    - 対数: $\log(y) = a_1 + a_2 x$
    - 二次曲線: $y = a_1 + a_2 x^2$

2. Generate a **fitted model**: データに合うようにパラメータを調整
    - $y = 3x + 7$
    - $y = 9x^2$

<a href="https://r4ds.had.co.nz/model-basics.html" class="url">https://r4ds.had.co.nz/model-basics.html</a>


---
## たぶん身長が高いほど体重も重い

なんとなく $y = a x + b$ でいい線が引けそう<br>
&nbsp;

```{r weight-height, echo = FALSE, fig.height = 5, fig.width = 5}
n = 50
df_weight = tibble::tibble(
  height = rnorm(n, 1.70, 0.05),
  bmi = rnorm(n, 22, 1),
  weight = bmi * (height ** 2)
)
p_weight = ggplot(df_weight) + aes(height, weight) +
  geom_point(alpha = 0.5) +
  theme_bw(base_size = 20) + theme(panel.grid = element_blank())
p_weight
```


---
## たぶん身長が高いほど体重も重い

なんとなく $y = a x + b$ でいい線が引けそう<br>
じゃあ切片と傾き、どう決める？

```{r weight-lines, echo = FALSE, fig.height = 5, fig.width = 5}
df_ab = tibble(intercept = runif(n, -50, 50), slope = runif(n, -200, 200)) %>%
  dplyr::mutate(intercept = intercept - 1.7 * slope + 50)
p_weight +
  geom_abline(data = df_ab, aes(intercept = intercept, slope = slope), color = "#3366ff", alpha = 0.5)
```


---
## 最小二乗法

回帰直線からの<strong style="color: #3366ff">残差</strong>平方和(RSS)を最小化する。

```{r weight-residual, echo = FALSE, fig.height = 5, fig.width = 10}
predict_weight = function(parameters, data) {
  parameters[1] + parameters[2] * data$height
}

rss_weight = function(parameters, data) {
  pred = predict_weight(parameters, data)
  sqdev = (data[["weight"]] - pred) ** 2
  sum(sqdev)
}
# rss_weight(c(40, -5), df_weight)

param1 = c(10, 30)
lm_weight = lm(weight ~ height, data = df_weight)
rss1 = rss_weight(param1, df_weight)
rss2 = rss_weight(lm_weight$coefficients, df_weight)
stopifnot(abs(sum(lm_weight$residuals ** 2) - rss2) < 1e-6)

p1 = p_weight %+%
  (df_weight %>% dplyr::mutate(pred = predict_weight(param1, .))) +
  geom_line(aes(y = pred)) +
  geom_linerange(aes(ymin = weight, ymax = pred), colour = "#3366ff") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = sprintf("RSS = %.1f", rss1))
p2 = p_weight %+%
  (df_weight %>% modelr::add_predictions(lm_weight)) +
  geom_line(aes(y = pred)) +
  geom_linerange(aes(ymin = weight, ymax = pred), colour = "#3366ff") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = sprintf("RSS = %.1f", rss2))
cowplot::plot_grid(p1, p2, nrow = 1L)
```


---
## 残差平方和(RSS)が最小となるパラメータを探せ

ランダムに試してみて、上位のものを採用

```{r weight-goodlines, echo = FALSE, fig.height = 5, fig.width = 10}
n = 200
df_ab_random = tibble::tibble(intercept = runif(n, -200, 100), slope = runif(n, 0, 150)) %>%
  dplyr::mutate(rss = purrr::map2_dbl(intercept, slope, ~ rss_weight(c(.x, .y), data = df_weight)))
p_ab = ggplot(df_ab_random) + aes(intercept, slope) +
  geom_point(data = function(x) {filter(x, rank(rss) < 6)}, shape = 1, size = 4) +
  geom_point(aes(color = log10(rss))) +
  theme_bw(base_size = 20) +
  theme(panel.grid = element_blank(), legend.position = c(0.99, 0.99), legend.justification = c(1, 1))

p2 = p_weight +
  geom_abline(data = df_ab_random %>% dplyr::slice_min(rss, n = 5), aes(slope = slope, intercept = intercept), color = "#3366ff", alpha = 0.5)
cowplot::plot_grid(p_ab, p2, nrow = 1L)
```

---
## 残差平方和(RSS)が最小となるパラメータを探せ

**グリッドサーチ**: パラメータ空間の一定範囲内を均等に試す

```{r weight-grid, echo = FALSE, fig.height = 5, fig.width = 10}
df_ab_grid = tidyr::crossing(intercept = seq(-100, -30, 4), slope = seq(50, 100, 4)) %>%
  dplyr::mutate(rss = purrr::map2_dbl(intercept, slope, ~ rss_weight(c(.x, .y), data = df_weight)))

p1 = p_ab %+% df_ab_grid

p2 = p_weight +
  geom_abline(data = df_ab_grid %>% dplyr::slice_min(rss, n = 5), aes(slope = slope, intercept = intercept), color = "#3366ff", alpha = 0.5)
cowplot::plot_grid(p1, p2, nrow = 1L)
```

こうした**最適化**の手法はいろいろあるけど、ここでは扱わない。


---
## これくらいなら一瞬で計算してもらえる

```{r lm}
par_init = c(intercept = 0, slope = 0)
result = optim(par_init, fn = rss_weight, data = df_weight)
result$par
```

```{r weight-lm, echo = FALSE, fig.height = 5, fig.width = 5}
label = sprintf("y = %.1f x + %.1f", result$par["slope"], result$par["intercept"])
p_weight +
  stat_smooth(formula = y ~ x, method = lm, se = FALSE, color = "#3366ff", alpha = 0.5) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = label)
```

---
## 何でもかんでも直線あてはめではよろしくない

```{r lm-bad, echo = FALSE, fig.height = 5, fig.width = 5}
n = 300L
a = 3
b = -3
df_pois = tibble::tibble(x = runif(n, 0.4, 1.7), y = rpois(n, exp(a * x + b)))

x_breaks = c(0.5, 1.0, 1.5)
coeff = lm(y ~ x, data = df_pois)$coefficients
df_lm = tidyr::crossing(x = x_breaks, y = seq(-5, 20, 0.1)) %>%
 dplyr::mutate(density = dnorm(y, coeff[1] + coeff[2] * x, 1.4)) %>%
 dplyr::filter(density > 1e-4)

p_pois = ggplot(df_pois) + aes(x, y) +
  ggridges::geom_vridgeline(data = df_lm, aes(width = density * 0.4, group = x), linetype = 0, alpha = 0) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  scale_x_continuous(breaks = x_breaks) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())
p_pois + stat_smooth(formula = y ~ x, method = lm, se = FALSE)
```

- 観察データは常に**正の値**なのに予測が負に突入してない？
- **縦軸は整数**。しかもの**ばらつき**が横軸に応じて変化？


---
## 何でもかんでも直線あてはめではよろしくない

```{r glm-better, echo = FALSE, fig.height = 5, fig.width = 10}
p_lm = p_pois +
  stat_smooth(formula = y ~ x, method = lm, se = FALSE) +
  ggridges::geom_vridgeline(data = df_lm, aes(width = density * 0.4, group = x), fill = "#3366ffaa", linetype = 0)
# p_lm

df_ridges = tidyr::crossing(x = x_breaks, y = seq_len(30L) - 1L) %>%
 dplyr::mutate(density = dpois(y, exp(a * x + b))) %>%
 dplyr::filter(density > 1e-4)
df_bars = df_ridges %>% wtl::ridges2bars(y, density)

p_poisson = p_pois +
  stat_smooth(formula = y ~ x, method = glm, method.args = list(family = poisson), se = FALSE) +
  ggridges::geom_vridgeline(data = df_bars, aes(width = density * 0.5, group = x), fill = "#3366ffaa", linetype = 0)
# p_poisson

cowplot::plot_grid(p_lm, p_poisson, nrow = 1L)
```

- 観察データは常に**正の値**なのに予測が負に突入してない？
- **縦軸は整数**。しかもの**ばらつき**が横軸に応じて変化？
- **データに合わせた統計モデルを使うとマシ**


---
## ちょっとずつ線形モデルを発展させていく

**線形モデル LM** (単純な直線あてはめ)

<span style="color: #888888;">&nbsp; &nbsp; ↓ いろんな<strong>確率分布</strong>を扱いたい</span> 👈 統計モデルの重要な部品

**一般化線形モデル GLM**

<span style="color: #888888;">&nbsp; &nbsp; ↓ 個体差などの変量効果を扱いたい</span>

**一般化線形混合モデル GLMM**

<span style="color: #888888;">&nbsp; &nbsp; ↓ もっと自由なモデリングを！</span>

**階層ベイズモデル HBM**

<cite>[データ解析のための統計モデリング入門](https://amzn.to/33suMIZ) 久保拓弥 2012 より改変</cite>


---
## 確率分布

発生する事象(値)と頻度の関係。

手元のデータを数えて作るのが**経験分布**<br>
e.g., サイコロを12回投げた結果、学生1000人の身長

```{r distribution, echo = FALSE, fig.height = 4, fig.width = 8}
p1 = tibble::tibble(face = sample.int(6, 12, replace = TRUE)) %>%
  ggplot() + aes(face) +
  geom_bar(aes(y = after_stat(prop))) +
  scale_x_continuous(breaks = seq_len(6L)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.y = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
        axis.ticks = element_blank())

p2 = tibble::tibble(height = rnorm(1000L, c(160, 170), 5.5)) %>%
  ggplot() + aes(height) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 1, boundary = 0) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), axis.ticks = element_blank())
cowplot::plot_grid(p1, p2, nrow = 1L)
```

一方、少数のパラメータと数式で作るのが**理論分布**。<br>
(こちらを単に「確率分布」と呼ぶことが多い印象）

---
## 確率変数$X$はパラメータ$\theta$の確率分布$f$に従う...?

$X \sim f(\theta)$

e.g.,<br>
コインを3枚投げたうち表の出る枚数 $X$ は**二項分布に従う**。<br>
$X \sim \text{Binomial}(n = 3, p = 0.5)$

<div class="column-container">
  <div class="column" style="flex-shrink: 2.0;">

```{r dbinom, echo = FALSE, fig.height = 3, fig.width = 3}
size = 3L; p = 0.5
tibble(X = seq(0, 3), Prob = dbinom(X, size, p), obs = Inf) %>%
  ggplot() + aes(X, Prob) +
  geom_col() +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

  </div>
  <div class="column" style="padding-top: 10px;">
\[\begin{split}
\text{Prob}(X = k) &= \binom n k p^k (1 - p)^{n - k} \\
k &\in \{0, 1, 2, \ldots, n\}
\end{split}\]
  </div>
</div>

一緒に実験してみよう。

---
## 試行を繰り返して記録してみる

コインを3枚投げたうち表の出た枚数 $X$

試行1: **表** 裏 **表** → $X = 2$<br>
試行2: 裏 裏 裏 → $X = 0$<br>
試行3: **表** 裏 裏 → $X = 1$ 続けて $2, 1, 3, 0, 2, \ldots$

```{r rbinom, echo = FALSE, fig.height = 3, fig.width = 11}
size = 3L; p = 0.5
X = c(2L, 0L, 1L, 2L, 1L, 3L, 0L, 2L, rbinom(92L, size, p))
df_rbinom = purrr::map_dfr(c(1, 2, 3, 10, 100), ~{
  tibble::tibble(X = head(X, .x)) %>% dplyr::count(X) %>%
    dplyr::mutate(Freq = n / .x, Repl = .x)
}) %>%
  dplyr::bind_rows(tibble(X = seq(0, 3), Freq = dbinom(X, size, p), Repl = Inf))
df_rbinom %>%
  ggplot() + aes(X, Freq) +
  geom_col() +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  facet_wrap(vars(Repl), nrow = 1L, labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

<div style="text-align: right;">
試行回数を増やすほど<b>二項分布</b>の形に近づく。<br>
0と3はレア。1と2が3倍ほど出やすいらしい。
</div>

---
## コイントスしなくても $X$ らしきものを生成できる

- コインを3枚投げたうち表の出る枚数 $X$
- $n = 3, p = 0.5$ の二項分布からサンプルする乱数 $X$

<div class="column-container">
  <div class="column" style="flex-shrink: 2.0;">
<img src="figure/dbinom-1.png" alt="plot of chunk dbinom">
  </div>
  <div class="column" style="padding-top: 10px;">
$X \sim \text{Binomial}(n = 3, p = 0.5)$

&nbsp;&nbsp; ↓ サンプル

{2, 0, 1, 2, 1, 3, 0, 2, ...}
  </div>
</div>

これらはとてもよく似ているので<br>
**「コインをn枚投げたうち表の出る枚数は二項分布に従う」**<br>
みたいな言い方をする。逆に言うと<br>
**「二項分布とはn回試行のうちの成功回数を確率変数とする分布」**<br>
のように理解できる。

---
## 統計モデリングの一環とも捉えられる

コイン3枚投げを繰り返して得たデータ {2, 0, 1, 2, 1, 3, 0, 2, ...}

&nbsp;&nbsp; ↓ たった2つのパラメータで記述。情報を圧縮。

$n = 3, p = 0.5$ の二項分布で説明・再現できるぞ

<figure>
<img src="../tokiomarine2021/math-model.drawio.svg" width="600"><br>
<figcaption><cite>「データ分析のための数理モデル入門」江崎貴裕 2020 より改変</cite></figcaption>
</figure>

こういうふうに現象と対応した確率分布、ほかにもある？

???
ただし「これが3連コイントスの真理だ」ではない。<br>
あくまで「こう単純化して理解できそう・使えそう」なだけ。

ほかの仮定: コインが立つかも。偏ったコインかも。両表かも。

二項分布はn枚コイントスをたった2パラメータで説明する優秀モデル


---
## 有名な確率分布、それに「従う」もの

離散一様分布
: コインの表裏、サイコロの出目1–6

幾何分布
: 成功率pの試行が初めて成功するまでの失敗回数

二項分布
: 成功率p、試行回数nのうちの成功回数

ポアソン分布
: 単位時間あたり平均$\lambda$回起こる事象の発生回数

ガンマ分布
: ポアソン過程でk回起こるまでの待ち時間
: (k = 1のとき**指数分布**と呼ばれる)

正規分布
: 確率変数の和、平均値

---
## 離散一様分布

同じ確率で起こるn通りの事象のうち実際に起こった事象X

e.g., コインの表裏、サイコロの出目1–6

```{r dunif, echo = FALSE, fig.height = 4, fig.width = 6}
df_coin = tibble::tibble(X = c("head", "tail"), Prob = c(0.5, 0.5), group = "Coin")
df_dice = tibble::tibble(X = as.character(seq_len(6L)), Prob = rep(1 / 6, 6), group = "Dice")
dplyr::bind_rows(df_coin, df_dice) %>%
  ggplot() + aes(X, Prob) +
  geom_col() +
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1)) +
  facet_grid(cols = vars(group), scale = "free_x", space = "free_x") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

🔰 一様分布になりそうな例を考えてみよう


---
## 幾何分布 $~\text{Geom}(p)$

成功率pの試行が初めて成功するまでの失敗回数X

e.g., コイントスで表が出るまでに何回裏が出るか

```{r geometric, echo = FALSE, fig.height = 4, fig.width = 11}
df = purrr::map_dfr(c(0.2, 0.5, 0.9), function(p) {
  tibble::tibble(p, X = seq(0, 25), Prob = dgeom(X, p))
}) %>%
  dplyr::filter(Prob > 0.001)
x_br = c(seq.int(0, 10), seq.int(15, 100, 5))
ggplot(df) + aes(X, Prob) +
  scale_x_continuous(breaks = x_br) +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  geom_col() +
  facet_grid(cols = vars(p), scales = "free_x", space = "free_x", labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

\\[
\text{Prob}(X = k \mid p) = p (1 - p)^k
\\]

「初めて成功するまでの試行回数」とする定義もある。

🔰 幾何分布になりそうな例を考えてみよう

---
## 二項分布 $~\text{Binomial}(n,~p)$

確率$p$で当たるクジを$n$回引いたうち当たった回数X。平均は$np$。

```{r dbinom-n, echo = FALSE, fig.height = 4, fig.width = 11}
p = 0.25
df = purrr::map_dfr(2 ** seq.int(0, 4), function(n) {
  tibble::tibble(X = seq(0, n), Prob = dbinom(X, n, p), n = n)
})
ggplot(df) + aes(X, Prob) +
  scale_x_continuous(breaks = df[["X"]]) +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1)) +
  geom_col() +
  facet_grid(cols = vars(n), scales = "free_x", space = "free_x", labeller = label_both) +
  labs(title = paste0("p = ", p)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

\\[
\text{Prob}(X = k \mid n,~p) = \binom n k p^k (1 - p)^{n - k}
\\]

🔰 二項分布になりそうな例を考えてみよう


---
## ポアソン分布 $~\text{Poisson}(\lambda)$

単位時間(空間)あたりに平均$\lambda$回発生する事象が実際に起きた回数X。

e.g., 1時間あたりのメッセージ受信件数、メッシュ区画内の生物個体数

```{r dpoisson, echo = FALSE, fig.height = 4, fig.width = 11}
lambda = c(1, 5, 10)

p_poisson_process = tibble::tibble(y = rev(seq_along(lambda)), lambda) %>%
  dplyr::mutate(time = purrr::map(lambda, ~runif(.x * 3, 0, 3))) %>%
  tidyr::unnest(time) %>%
  ggplot() + aes(time, y) +
  annotate("segment", x = -0.1, xend = 3.1, y = 1:3, yend = 1:3, size = 2, arrow = grid::arrow(length = unit(0.1, "inches"), type = "closed"), linejoin = "mitre") +
  geom_point(aes(color = lambda, fill = lambda), size = 8, shape = 124, key_glyph = draw_key_rect) +
  scale_color_continuous(guide = NULL) +
  scale_fill_continuous(guide = guide_legend(label.position = "top", title.vjust = 1), breaks = lambda) +
  scale_y_continuous(limits = c(0.5, 3.5), breaks = NULL) +
  labs(y = NULL, x = "time (space)") +
  theme_bw(base_size = 18) +
  theme(axis.text.y = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(),
        panel.grid.minor = element_blank(), legend.position = "top")

p2 = tidyr::crossing(X = seq.int(0L, 20L), lambda) %>%
  dplyr::mutate(Prob = dpois(X, lambda)) %>%
  ggplot() + aes(X, Prob) +
  geom_col(aes(fill = lambda), position = "identity", alpha = 0.5) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        axis.ticks = element_blank(), legend.position = "none")
cowplot::plot_grid(p_poisson_process, p2, nrow = 1L, rel_widths = c(4, 3))
```

\\[
\text{Prob}(X = k \mid \lambda) = \frac {\lambda^k e^{-\lambda}} {k!}
\\]

二項分布の極限 $(\lambda = np;~n \to \infty;~p \to 0)$。<br>
めったに起きないことを何回も試行するような感じ。


---
## 指数分布 $~\text{Exp}(\lambda)$

ポアソン過程の事象の発生間隔x。平均は $1 / \lambda$ 。

e.g., メッセージの受信間隔、道路沿いに落ちてる手袋の間隔

```{r dexp, echo = FALSE, fig.height = 4, fig.width = 11}
p2 = tidyr::crossing(x = seq(0, 3, length.out = 201), lambda) %>%
  dplyr::mutate(Prob = dexp(x, rate = lambda)) %>%
  ggplot() + aes(x, Prob) +
  geom_area(aes(fill = lambda, group = lambda), position = "identity", alpha = 0.5) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        axis.ticks = element_blank(), legend.position = "none")
cowplot::plot_grid(p_poisson_process, p2, nrow = 1L, rel_widths = c(4, 3))
```

\\[
\text{Prob}(x \mid \lambda) = \lambda e^{-\lambda x}
\\]

幾何分布の連続値版。

🔰 ポアソン分布・指数分布になりそうな例を考えてみよう


---
## ガンマ分布 $~\text{Gamma}(k,~\lambda)$

ポアソン過程の事象k回発生までの待ち時間x

e.g., メッセージを2つ受信するまでの待ち時間

```{r dgamma, echo = FALSE, fig.height = 4, fig.width = 11}
p2 = tidyr::crossing(x = seq(0, 3, length.out = 201), lambda) %>%
  dplyr::mutate(Prob = dgamma(x, rate = lambda, shape = 3)) %>%
  ggplot() + aes(x, Prob) +
  geom_area(aes(fill = lambda, group = lambda), position = "identity", alpha = 0.5) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        axis.ticks = element_blank(), legend.position = "none")
cowplot::plot_grid(p_poisson_process, p2, nrow = 1L, rel_widths = c(4, 3))
```

\\[
\text{Prob}(x \mid k,~\lambda) = \frac {\lambda^k x^{k - 1} e^{-\lambda x}} {\Gamma(k)}
\\]

指数分布をkのぶん右に膨らませた感じ。<br>
shapeパラメータ $k = 1$ のとき指数分布と一致。


---
## 正規分布 $~\mathcal{N}(\mu,~\sigma)$

平均 $\mu$、標準偏差 $\sigma$ の美しい分布。よく登場する。<br>
e.g., $\mu = 50, ~\sigma = 10$ (濃い灰色にデータの95%, 99%が含まれる):

```{r gaussian, echo = FALSE, fig.height = 5, fig.width = 11}
ci = qnorm(c(0.005, 0.025, 0.975, 0.995), 50, 10)
tibble::tibble(x = seq(0, 100, 0.1), Prob = dnorm(x, 50, 10)) %>%
  ggplot() + aes(x, Prob) +
  geom_area(alpha = 0.4) +
  geom_area(data = function(.x) {dplyr::filter(.x, dplyr::between(x, ci[2], ci[3]))}, alpha = 0.4) +
  geom_area(data = function(.x) {dplyr::filter(.x, dplyr::between(x, ci[1], ci[4]))}, alpha = 0.4) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        axis.ticks = element_blank(), legend.position = "none")
```

\\[
\text{Prob}(x \mid \mu,~\sigma) = \frac 1 {\sqrt{2 \pi \sigma^2}} \exp \left(\frac {-(x - \mu)^2} {2\sigma^2} \right)
\\]

---
## 正規分布に近づくものがいろいろある

標本平均の反復(**中心極限定理**);
e.g., 一様分布 [0, 100) から40サンプル

```{r central-limit, echo = FALSE, fig.height = 3, fig.width = 11}
n = 40L
X = replicate(10000L, mean(runif(n, 0, 100)))
purrr::map_dfr(c(10, 100, 1000, 10000), ~{tibble::tibble(X = head(X, .x), Repl = .x)}) %>%
  ggplot() + aes(X) +
  geom_histogram(bins = 25) +
  facet_wrap(vars(Repl), nrow = 1L, scale = "free_y", labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
        axis.ticks = element_blank())
```

大きい$n$の二項分布

```{r binom-normal, echo = FALSE, fig.height = 3, fig.width = 11}
purrr::map_dfr(c(1, 4, 16, 64, 256), ~{
  tibble::tibble(X = seq(0, .x), Prob = dbinom(X, .x, 0.25), n = .x) %>%
  dplyr::filter(Prob > 1e-5)
}) %>%
  ggplot() + aes(X, Prob) +
  geom_col(width = 1) +
  facet_wrap(vars(n), nrow = 1L, scale = "free", labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
        axis.ticks = element_blank())
```

---
## 正規分布に近づくものがいろいろある

大きい$\lambda$のポアソン分布

```{r poisson-normal, echo = FALSE, fig.height = 2.5, fig.width = 11}
purrr::map_dfr(c(1, 4, 16, 64, 256), ~{
  tibble::tibble(X = seq(0, 4 * .x), Prob = dpois(X, .x), lambda = .x) %>%
  dplyr::filter(Prob > 1e-5)
}) %>%
  ggplot() + aes(X, Prob) +
  geom_col(width = 1) +
  facet_wrap(vars(lambda), nrow = 1L, scale = "free", labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
        axis.ticks = element_blank())
```

平均値固定なら$k$が大きくなるほど左右対称に尖るガンマ分布

```{r gamma-normal, echo = FALSE, fig.height = 4, fig.width = 11}
.guide = guide_legend(reverse = TRUE, label.position = "left", label.hjust = 1)
tidyr::crossing(x = seq(0, 25, length.out = 300), k = 10 ** seq.int(0, 3)) %>%
  dplyr::mutate(Prob = dgamma(x, rate = k / 10, shape = k)) %>%
  ggplot() + aes(x, Prob) +
  geom_area(aes(fill = k, group = k), position = "identity", alpha = 0.5) +
  scale_fill_viridis_c(trans = "log10", guide = .guide) +
  coord_cartesian(xlim = c(0, 20)) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        axis.ticks = element_blank())
```


---
## 有名な確率分布対応関係ふりかえり

<figure style="float: right;">
<img src="../tokiomarine2021/math-model.drawio.svg" width="300"><br>
</figure>

離散一様分布
: コインの表裏、サイコロの出目1–6

幾何分布
: 成功率pの試行が初めて成功するまでの失敗回数

二項分布
: 成功率p、試行回数nのうちの成功回数

ポアソン分布
: 単位時間あたり平均$\lambda$回起こる事象の発生回数

ガンマ分布
: ポアソン過程でk回起こるまでの待ち時間
: (k = 1のとき**指数分布**と呼ばれる)

正規分布
: 確率変数の和、平均値。使い勝手が良く、よく登場する。


---
## 現実には、確率分布に「従わない」ことが多い

植物100個体から8個ずつ種子を取って植えたら全体で半分ちょい発芽。<br>
親1個体あたりの生存数は<span style="color: #3366ff;">n=8の二項分布</span>になるはずだけど、<br>
極端な値(全部死亡、全部生存)が多かった。

<img src="figure/overdispersion-1.png" alt="plot of chunk overdispersion" width=400>

「それはなぜ？」と考えて要因を探るのも統計モデリングの仕事。<br>
**「普通はこれに従うはず」を理解してこそ**できる思考。


---
## 疑似乱数生成器 Pseudo Random Number Generator

コンピューター上でランダム**っぽい**数値を出力する装置。<br>
実際には**決定論的**に計算されているので、<br>
シード(出発点)と呼び出し回数が同じなら出る数も同じになる。

```r
set.seed(42)
runif(3L)
# 0.9148060 0.9370754 0.2861395
runif(3L)
# 0.8304476 0.6417455 0.5190959
set.seed(42)
runif(6L)
# 0.9148060 0.9370754 0.2861395 0.8304476 0.6417455 0.5190959
```

シードに適当な固定値を与えておくことで再現性を保てる。<br>
ただし「このシードじゃないと良い結果が出ない」はダメ。

さまざまな「分布に従う」乱数を生成することもできる。

---
## いろんな乱数を生成・可視化して感覚を掴もう

🔰 `?rbinom` などヘルプを参照しつつたくさん試そう。

🔰 e.g., 1%の当たりを狙って100連ガチャを回した場合とか

```r
n = 100
x = sample.int(6, n, replace = TRUE)
x = runif(n, min = 0, max = 1)
x = rgeom(n, prob = 0.5)
x = rbinom(n, size = 3, prob = 0.5)
x = rpois(n, lambda = 10)
x = rnorm(n, mean = 50, sd = 10)
print(x)

p1 = ggplot(data.frame(x)) + aes(x)
p1 + geom_histogram() # for continuous values
p1 + geom_bar()       # for discrete values
```

---
## データに分布をあてはめたい

ある植物を50個体調べて、それぞれの種子数Xを数えた。

```{r poisson-seed, echo = FALSE, fig.height = 4, fig.width = 4}
df_rpois = tibble::tibble(X = rpois(50L, 3))
ggplot(df_rpois) + aes(X) +
  geom_bar() +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(),
        axis.ticks = element_blank())
```

カウントデータだからポアソン分布っぽい。<br>
ポアソン分布のパラメータ $\lambda$ はどう決める？


---
## データに分布をあてはめたい

ある植物を50個体調べて、それぞれの種子数Xを数えた。

```{r poisson-seed-lambda, echo = FALSE, fig.height = 4, fig.width = 11}
df_dpois = purrr::map_dfr(c(1, 3, 5), ~{
  tibble(lambda = .x, X = seq(0, 11), Prob = dpois(X, lambda))
})
p_pois = ggplot(df_rpois) + aes(X) +
  geom_bar(aes(y = after_stat(prop)), width = 0.4) +
  geom_col(data = df_dpois, aes(y = Prob), alpha = 0.5, fill = "#3366ff") +
  facet_wrap(vars(lambda), nrow = 1L, labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        panel.grid.major.x = element_blank(), axis.ticks = element_blank())
p_pois
```

カウントデータだからポアソン分布っぽい。<br>
ポアソン分布のパラメータ $\lambda$ はどう決める？<br>
(黒が観察データ。<span style="color: #3366ff;">青がポアソン分布</span>。よく重なるのは？)


---
## <ruby>尤<rt>ゆう</rt>度</ruby> (likelihood)

<ruby>尤<rt>もっと</rt></ruby>もらしさ。
モデルのあてはまりの良さの尺度のひとつ。

**あるモデル$M$の下でそのデータ$D$が観察される確率**。<br>
定義通り素直に書くと<br>
$\text{Prob}(D \mid M)$

データ$D$を固定し、モデル$M$の関数とみなしたものが**尤度関数**:<br>
$L(M \mid D)$

モデルの構造も固定してパラメータ$\theta$だけ動かす場合はこう書く:<br>
$L(\theta \mid D)$ とか $L(\theta)$ とか


---
## 尤度を手計算できる例

コインを5枚投げた結果 $D$: 表 4, 裏 1

表が出る確率 $p = 0.5$ と仮定:
<div>\[\begin{split}
L(0.5 \mid D)
  &= \binom 5 1 \times \text{Prob}(表 \mid 0.5) ^ 4 \times \text{Prob}(裏 \mid 0.5) ^ 1 \\
  &= 5 \times 0.5 ^ 4 \times 0.5 ^ 1 = 0.15625
\end{split}\]</div>

表が出る確率 $p = 0.8$ と仮定:
<div>\[\begin{split}
L(0.8 \mid D)
  &= \binom 5 1 \times \text{Prob}(表 \mid 0.8) ^ 4 \times \text{Prob}(裏 \mid 0.8) ^ 1 \\
  &= 5 \times 0.8 ^ 4 \times 0.2 ^ 1 = 0.4096
\end{split}\]</div>

$L(0.8 \mid D) > L(0.5 \mid D)$

$p = 0.8$ のほうがより尤もらしい。



---
## 種子数ポアソン分布の例でも尤度を計算してみる

ある植物が作った種子を数える。$n = 50$個体ぶん。

<div>\[\begin{split}
L(\lambda \mid D)
  = \prod _i ^n \text{Prob}(X_i \mid \lambda)
  = \prod _i ^n \frac {\lambda ^ {X_i} e ^ {-\lambda}} {X_i !}
\end{split}\]</div>

```{r poisson-seed-likelihood, echo = FALSE, fig.height = 4, fig.width = 11}
df_likelihood = df_rpois %>%
  dplyr::left_join(df_dpois, by = "X") %>%
  dplyr::group_by(lambda) %>%
  dplyr::summarize(L = prod(Prob)) %>%
  dplyr::mutate(logL = log(L), label = sprintf("L(%.0f|D) = %.1e", lambda, L))
p_pois +
  geom_text(data = df_likelihood, aes(label = label), x = Inf, y = Inf, hjust = 1.1, vjust = 1.3, size = 6, color = "#3366ff")
```

この中では $\lambda = 3$ がいいけど、より尤もらしい値を求めたい。

---
## 最尤推定 <u>M</u>aximum <u>L</u>ikelihood <u>E</u>stimation

扱いやすい **対数尤度** (log likelihood) にしてから計算する。<br>
一階微分が0になる $\lambda$ を求めると...**標本平均**と一致。

<div>\[\begin{split}
\log L(\lambda \mid D)
  &= \sum _i ^n \left[ X_i \log (\lambda) - \lambda - \log (X_i !) \right] \\
\frac {\mathrm d \log L(\lambda \mid D)} {\mathrm d \lambda}
  &= \frac 1 \lambda \sum _i ^n X_i - n = 0 \\
\hat \lambda &= \frac 1 n \sum _i ^n X_i
\end{split}\]</div>


```{r poisson-mle, echo = FALSE, fig.height = 3, fig.width = 10}
count_rpois = df_rpois %>% dplyr::count(X)
calc_likelihood_rpois = function(lambda) {
  prod(dpois(count_rpois[["X"]], lambda) ** count_rpois[["n"]])
}
X_mle = mean(df_rpois[["X"]])
L_mle = calc_likelihood_rpois(X_mle)
p_mle = tibble::tibble(lambda = seq(1, 5, 0.1), L = purrr::map_dbl(lambda, calc_likelihood_rpois)) %>%
  dplyr::mutate(logL = log(L)) %>%
  ggplot() + aes(lambda, logL) +
  geom_line() +
  geom_vline(xintercept = X_mle, color = "#3366ff") +
  annotate("point", x = X_mle, y = log(L_mle), size = 3, color = "#3366ff") +
  annotate("text", x = Inf, y = Inf, hjust = 1.1, vjust = 1.2, size = 5, color = "#3366ff", label = sprintf("lambda = %.2f", X_mle)) +
  labs(y = "log L") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        axis.ticks = element_blank())

df_dpois = tibble(lambda = X_mle, X = seq(0, 11), Prob = dpois(X, lambda))
p_pois = ggplot(df_rpois) + aes(X) +
  geom_bar(aes(y = after_stat(prop)), width = 0.4) +
  geom_col(data = df_dpois, aes(y = Prob), alpha = 0.5, fill = "#3366ff") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        panel.grid.major.x = element_blank(), axis.ticks = element_blank())

cowplot::plot_grid(p_mle, p_pois, nrow = 1L, rel_widths = c(2, 1))
```

---
## 最尤推定を使っても“真のλ”は得られない

今回のデータは真の生成ルール“$X \sim \text{Poisson}(\lambda = 3.0)$”で作った。<br>
「50個体サンプル→最尤推定」を1,000回繰り返してみると:

```{r poisson-mle-repl, echo = FALSE, fig.height = 5, fig.width = 11}
nrep = 1000L
df_repl = tibble::tibble(X = rpois(50L * nrep, 3), repl = rep(seq_len(nrep), each = 50L))
df_sum = df_repl %>%
  dplyr::group_by(repl) %>%
  dplyr::summarize(lambda = mean(X))
df_minmax = dplyr::bind_rows(dplyr::slice_max(df_sum, lambda), dplyr::slice_min(df_sum, lambda))

p_repl = df_sum %>%
  ggplot() + aes(lambda) +
  geom_histogram(bins = 25, center = 3) +
  annotate("point", x = df_minmax$lambda, y = 0, color = "#3366ff", size = 8, alpha = 0.5) +
  labs(x = "estimated_lambda") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        axis.ticks = element_blank())

df_dpois = tidyr::crossing(lambda = df_minmax$lambda, X = seq(0, 11)) %>%
  dplyr::mutate(Prob = dpois(X, lambda))
p_minmax = df_repl %>%
  dplyr::filter(repl %in% df_minmax$repl) %>%
  dplyr::left_join(df_minmax, by = "repl") %>%
  ggplot() + aes(X) +
  geom_bar(aes(y = after_stat(prop)), width = 0.4) +
  geom_col(data = df_dpois, aes(y = Prob), alpha = 0.5, fill = "#3366ff") +
  facet_wrap(vars(lambda), ncol = 1L, labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        panel.grid.major.x = element_blank(), axis.ticks = element_blank())

cowplot::plot_grid(p_repl, p_minmax, nrow = 1L, rel_widths = c(2, 1))
```

サンプルの取れ方によってはかなりズレた推定をしてしまう。<br>
(標本データへのあてはまりはかなり良く見えるのに！)


---
## サンプルサイズを増やすほどマシにはなる

“$X \sim \text{Poisson}(\lambda = 3.0)$”からnサンプル→最尤推定を1,000回繰り返す:

```{r poisson-mle-nsam, echo = FALSE, fig.height = 4, fig.width = 11}
nrep = 1000L
purrr::map_dfr(c(5, 50, 500, 5000), ~{
  tibble::tibble(X = rpois(.x * nrep, 3), repl = rep(seq_len(nrep), each = .x)) %>%
    dplyr::group_by(repl) %>%
    dplyr::summarize(estimated_lambda = mean(X)) %>%
    dplyr::mutate(n = .x)
}) %>%
  ggplot() + aes(estimated_lambda) +
  geom_histogram(bins = 25, center = 3) +
  facet_wrap(vars(n), nrow = 1L, labeller = label_both) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        axis.ticks = element_blank())
```

Q. じゃあどれくらいのサンプル数nを確保すればいいのか？<br>
A. 推定したい統計量とか、許容できる誤差とかによる。


---
## すべてのモデルは間違っている

確率分布がいい感じに最尤推定できたとしても、<br>
それはあくまでモデル。仮定。近似。

> All models are wrong, but some are useful. --- George E. P. Box

<figure>
<img src="../tokiomarine2021/math-model.drawio.svg" width="600"><br>
<figcaption><cite>「データ分析のための数理モデル入門」江崎貴裕 2020 より改変</cite></figcaption>
</figure>


---
## 統計モデリングの道具 --- まとめ

- **確率変数** $X$
- **確率分布** $X \sim f(\theta)$
    - **少ないパラメータ** $\theta$ でばらつきの様子を表現
    - **この現象はこの分布を作りがち(〜に従う)** という知見がある
- **尤度**
    - あるモデルでこのデータになる確率 $\text{Prob}(D \mid M)$
    - データ固定でモデル探索 → **尤度関数** $L(M \mid D),~L(\theta \mid D)$
    - 対数を取ったほうが扱いやすい → **対数尤度** $\log L(M \mid D)$
    - これを最大化するようなパラメータ $\hat \theta$ 探し ＝ **最尤法**










---
## ここまで見てきた統計モデル

確率変数$X$はパラメータ$\theta$の確率分布$f$に“従う”:&nbsp;
$X \sim f(\theta) $

e.g., ある植物が作る種の数$X$は平均値$\lambda$のポアソン分布に従う:

<div>\[\begin{split}
X \sim \text{Poisson}(\lambda)
\end{split}\]</div>

```{r only-dist, echo = FALSE, fig.height = 4, fig.width = 4}
df_rpois = tibble::tibble(X = rpois(50L, 3))
df_dpois = tibble(X = seq(0, 11), Prob = dpois(X, mean(df_rpois$X)))
p_pois = ggplot(df_rpois) + aes(X) +
  geom_bar(aes(y = after_stat(prop)), width = 0.3) +
  geom_col(data = df_dpois, aes(y = Prob), alpha = 0.5, fill = "#3366ff") +
  theme_bw(base_size = 18)
p_pois +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

これを一般化線形モデル(GLM)として見ることもできる。

---
## 一般化線形モデル(GLM)として記述してみる

個体$i$の種子数$y_i$は平均値$\lambda_i$のポアソン分布に従う。<br>
平均値$\lambda_i$は**他のデータによらず$\beta_0$で一定**。

<div>\[\begin{split}
y_i &\sim \text{Poisson}(\lambda_i) \\
\lambda_i &= \beta_0
\end{split}\]</div>

```{r glm-without-x, echo = FALSE, fig.height = 4, fig.width = 4}
p_pois +
  labs(x = "y") +
  coord_flip() +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_blank(), axis.ticks = element_blank())
```

種子数をY軸にして、式を2つに分けただけ...?<br>
**説明変数**を含むモデルを見ればご利益が分かるかも。

---
## 説明変数が1つある一般化線形モデル

個体$i$の種子数$y_i$は平均値$\lambda_i$のポアソン分布に従う。<br>
平均値の対数$\log(\lambda_i)$は**その個体の大きさ$x_i$に比例**する。

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0;">

<figure>
<img src="../tokiomarine2021/glm.drawio.svg" width="100%"><br>
</figure>

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r glm-poisson, echo = FALSE, fig.height = 5, fig.width = 5}
n = 300L
a = 3
b = -3
df_pois = tibble::tibble(x = runif(n, 0.4, 1.7), y = rpois(n, exp(a * x + b)))

x_breaks = c(0.5, 1.0, 1.5)
df_ridges = tidyr::crossing(x = x_breaks, y = seq_len(30L) - 1L) %>%
 dplyr::mutate(density = dpois(y, exp(a * x + b))) %>%
 dplyr::filter(density > 1e-4)
df_bars = df_ridges %>% wtl::ridges2bars(y, density)

p_pois = ggplot(df_pois) + aes(x, y) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  scale_x_continuous(breaks = x_breaks) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())

p_pois +
  stat_smooth(formula = y ~ x, method = glm, method.args = list(family = poisson), se = FALSE) +
  ggridges::geom_vridgeline(data = df_bars, aes(width = density * 0.5, group = x), fill = "#3366ffaa", linetype = 0)
```

  </div>
</div>

この場合は**単回帰**。説明変数が複数あると**重回帰**。


---
## 複数の説明変数を同時に扱う重回帰

<p>\[\begin{split}
y_i &\sim \text{Poisson}(\lambda_i) \\
\log(\lambda_i) &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots
\end{split}\]</p>

気温も湿度も高いほどビールが売れる、とか

```{r multiple-regression, echo = FALSE, fig.height = 5, fig.width = 10}
n = 200L
true_coef = c(3, 0.05, 0.006)
df_beer = tibble::tibble(
  temperature = runif(n, 8, 32),
  humidity = runif(n, 20, 80),
  beer_sales = rpois(n, exp(true_coef[1] + true_coef[2] * temperature + true_coef[3] * humidity))
)
glm_multi = glm(beer_sales ~ temperature + humidity, df_beer, family = poisson)

df_pred = tidyr::crossing(temperature = seq(8, 32, 2), humidity = seq(20, 80, 5)) %>%
  modelr::add_predictions(glm_multi) %>%
  dplyr::mutate(y_pred = exp(pred))

p1 = ggplot(df_beer) + aes(temperature, beer_sales, color = humidity) +
  geom_line(data = df_pred, aes(y = y_pred, group = humidity), alpha = 0.7) +
  geom_point(alpha = 0.5) +
  scale_color_viridis_c(option = "cividis", direction = -1) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))
p2 = ggplot(df_beer) + aes(humidity, beer_sales, color = temperature) +
  geom_line(data = df_pred, aes(y = y_pred, group = temperature), alpha = 0.7) +
  geom_point(alpha = 0.5) +
  scale_color_viridis_c(option = "turbo") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))

cowplot::plot_grid(p1, p2, nrow = 1L)
```

今度は**確率分布**と**リンク関数**を変えてみよう。


---
## ロジスティック回帰

- 確率分布: **二項分布**
- リンク関数: $\text{logit}(p) = \log \frac {p} {1 - p}$

何かの成否に対する何かの因子の影響、とか

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

客10人中$y_i$人がビールを注文。<br>
その日$i$の気温$x_i$によって割合が変化。

<p>\[\begin{split}
y_i &\sim \text{Binomial}(n,~p_i) \\
\text{logit}(p_i) &= \beta_0 + \beta_1 x_i \\
p_i &= \frac 1 {1 + e^{-(\beta_0 + \beta_1 x_i)}}
\end{split}\]</p>

ロジスティック関数↑

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r glm-logistic, echo = FALSE, fig.height = 5, fig.width = 5}
nrep = 200L
n = 10L
df_rlogistic = tibble::tibble(
  x = runif(nrep, -10, 35),
  logit_p = -3 + 0.3 * x,
  p = wtl::logistic(logit_p),
  y = rbinom(nrep, n, p),
  response = matrix(c(y, n - y), ncol = 2)
)
glm_logistic = glm(response ~ x, df_rlogistic, family = binomial)
df_pred = df_rlogistic %>%
  modelr::add_predictions(glm_logistic) %>%
  dplyr::mutate(y_pred = n * wtl::logistic(pred))

coef = glm_logistic$coefficients

x_breaks = c(-10, 0, 10, 20, 30)
df_ridges = tidyr::crossing(x = x_breaks, y = seq.int(0, n)) %>%
  dplyr::mutate(p = wtl::logistic(coef[1] + coef[2] * x), density = dbinom(y, n, p)) %>%
  dplyr::filter(density > 1e-4)
df_bars = df_ridges %>% wtl::ridges2bars(y, density)

ggplot(df_pred) + aes(x, y) +
  geom_point(alpha = 0.5, shape = 16) +
  ggridges::geom_vridgeline(data = df_bars, aes(width = density * 6, group = x), fill = "#3366ffaa", linetype = 0) +
  geom_line(aes(y = y_pred), size = 2, color = "#3366ff") +
  scale_x_continuous(breaks = x_breaks) +
  scale_y_continuous(breaks = seq.int(0, 10)) +
  labs(x = "temperature", y = "beer_sales") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())
```

  </div>
</div>

???
ロジット = 対数オッズ
オッズ = 失敗の何倍成功しやすいか
Xが1増えるとオッズがe^a倍に増える。


---
## ロジスティック回帰 (狭義)

- 確率分布: **ベルヌーイ分布** ($n = 1$ の二項分布)
- リンク関数: $\text{logit}(p) = \log \frac {p} {1 - p}$

何かの成否に対する何かの因子の影響、とか

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

風が吹けば桶屋が儲かる。

<p>\[\begin{split}
y_i &\sim \text{Bernoulli}(p_i) \\
  &= \text{Binomial}(1,~p_i) \\
\text{logit}(p_i) &= \beta_0 + \beta_1 x_i \\
p_i &= \frac 1 {1 + e^{-(\beta_0 + \beta_1 x_i)}}
\end{split}\]</p>

ロジスティック関数↑

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r wind, echo = FALSE, fig.height = 4, fig.width = 5}
n = 200
df_wind = tibble::tibble(
  max_wind = runif(n, 0, 40),
  bucket_sales = rbinom(n, 1L, wtl::sigmoid(max_wind - 20, 0.2)) + 0L)
glm_bernoulli = glm(bucket_sales ~ max_wind, df_wind, family = "binomial")

coef = glm_bernoulli$coefficients
x_breaks = c(0, 10, 20, 30, 40)
df_ridges = tidyr::crossing(x = x_breaks, y = c(0, 1)) %>%
  dplyr::mutate(p = wtl::logistic(coef[1] + coef[2] * x), density = dbinom(y, 1, p)) %>%
  dplyr::filter(density > 1e-4)
df_bars = df_ridges %>% wtl::ridges2bars(y, density, width = 0.2)

df_wind %>%
  modelr::add_predictions(glm_bernoulli) %>%
  dplyr::mutate(y_pred = wtl::logistic(pred)) %>%
  ggplot() + aes(max_wind, bucket_sales) +
  geom_point(alpha = 0.3, shape = 124, size = 6) +
  ggridges::geom_vridgeline(data = df_bars, aes(x, y, width = density * 6, group = x), fill = "#3366ffaa", linetype = 0) +
  geom_line(aes(y = y_pred), color = "#3366ff") +
  scale_y_continuous(breaks = c(0, 1)) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())
```

  </div>
</div>


---
## 一般線形モデル (“化”無し) はGLMの一種

- 確率分布: **正規分布**
- リンク関数: **恒等関数**(なにもせずそのまま)

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

<p>\[\begin{split}
y_i &\sim \mathcal{N}(\mu_i,~\sigma^2) \\
\text{identity}(\mu_i) &= \beta_0 + \beta_1 x_i
\end{split}\]</p>

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r glm-weight, echo = FALSE, fig.height = 4, fig.width = 4}
n = 100
df_weight = tibble::tibble(
  height = rnorm(n, 1.70, 0.06),
  bmi = rnorm(n, 22, 0.8),
  weight = bmi * (height ** 2)
) %>%
  dplyr::filter(dplyr::between(height, 1.6, 1.8))

coef = lm(weight ~ height, df_weight)$coefficients

x_breaks = c(1.65, 1.7, 1.75)
df_ridges = tidyr::crossing(height = x_breaks, weight = seq(50, 80, 0.2)) %>%
  dplyr::mutate(density = dnorm(weight, coef[1] + coef[2] * height, 1.8)) %>%
  dplyr::filter(density > 1e-4)

ggplot(df_weight) + aes(height, weight) +
  geom_point(alpha = 0.5, shape = 16) +
  ggridges::geom_vridgeline(data = df_ridges, aes(width = density * 0.08, group = height), fill = "#3366ffaa", linetype = 0) +
  stat_smooth(method = lm, formula = y ~ x, se = FALSE) +
  scale_x_continuous(breaks = x_breaks) +
  theme_bw(base_size = 20) + theme(panel.grid.minor = element_blank())
```

  </div>
</div>

最小二乗法の直線あてはめと結果的に同じになる。

<small style="color: #999999;">単回帰・重回帰と言ったとき一般線形モデルを前提とする人もいる。</small>

---
## 分散分析 (<u>An</u>alysis <u>o</u>f <u>va</u>riance, ANOVA) as GLM

**質的な説明変数**を持つ**正規分布・恒等リンク**のGLM、と解釈可能。<br>
<span title="ダミー変数とも呼ばれる">**指示変数**</span> (0 or 1) に変換してから重回帰する。

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

| 天気 | → | $x_1$ ☀️ 晴れ | $x_2$ ☔️ 雨 |
| ---- | :-: | :---: | :---: |
| ☁️ くもり | | 0 | 0 |
| ☀️ 晴れ | | 1 | 0 |
| ☔️ 雨 | | 0 | 1 |

<p>\[\begin{split}
y_i &= \mathcal{N}(\mu_i,\sigma^2) \\
\mu_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}
\end{split}\]</p>

  </div>
  <div class="column" style="flex-shrink: 1.3;">

```{r glm-anova, echo = FALSE, fig.height = 4.5, fig.width = 4.5}
n = 200L
coef = c(70, 3, 20, -20)
weather_levels = c("sunny", "cloudy", "rainy")
df_beer = tibble::tibble(
    temperature = runif(n, 8, 32),
    weather = factor(sample(weather_levels, n, TRUE), levels = weather_levels)
  ) %>%
  dplyr::mutate(name = weather, value = 1L) %>%
  tidyr::pivot_wider(values_fill = 0L) %>%
  dplyr::select(!cloudy) %>%
  dplyr::mutate(beer_sales = rnorm(n, coef[1] + coef[2] * temperature + coef[3] * sunny + coef[4] * rainy, 10))

lm_anova = lm(beer_sales ~ weather, df_beer)
df_ridges = tidyr::crossing(weather = factor(weather_levels, levels = weather_levels), beer_sales = seq(50, 200, 1)) %>%
  modelr::add_predictions(lm_anova) %>%
  dplyr::mutate(density = dnorm(beer_sales, pred, 10)) %>%
  dplyr::filter(density > 1e-4)

tidy_anova = broom::tidy(lm_anova)

avgs = tidyr::crossing(weather = factor(weather_levels, levels = weather_levels)) %>%
  modelr::add_predictions(lm_anova) %>%
  tibble::deframe()

dfl = tibble::tribble(
  ~x, ~xend, ~y, ~yend,
  -Inf, Inf, avgs["cloudy"], avgs["cloudy"],
  1.5, 2.5, avgs["sunny"], avgs["sunny"],
  2.5, 3.5, avgs["rainy"], avgs["rainy"]
)

dfa = tibble::tribble(
  ~x, ~xend, ~y, ~yend,
  1.75, 1.75, avgs["cloudy"], avgs["sunny"],
  2.75, 2.75, avgs["cloudy"], avgs["rainy"]
)

dfs = tibble::tribble(
  ~x, ~y, ~label,
  0.6, avgs["cloudy"] + (avgs["sunny"] - avgs["cloudy"]) * 0.3, "beta[0]",
  1.55, (avgs["cloudy"] + avgs["sunny"]) / 2, "beta[1]",
  2.55, (avgs["cloudy"] + avgs["rainy"]) / 2, "beta[2]"
)

.arr = grid::arrow(length = grid::unit(0.1, "inches"))
df_beer %>%
  ggplot() + aes(weather, beer_sales, color = weather) +
  ggridges::geom_vridgeline(data = df_ridges, aes(width = density * 6, group = weather), fill = "#3366ffaa", linetype = 0) +
  annotate("segment", x = dfl$x, xend = dfl$xend, y = dfl$y, yend = dfl$yend, color = "#3366ffaa") +
  annotate("segment", x = dfa$x, xend = dfa$xend, y = dfa$y, yend = dfa$yend, arrow = .arr, color = "#3366ffaa") +
  annotate("text", x = dfs$x, y = dfs$y, label = dfs$label, parse = TRUE, size = 6, color = "#3366ffaa") +
  geom_jitter(width = 0.08, height = 0, alpha = 0.66, shape = 16, size = 3) +
  scale_color_viridis_d(direction = -1, guide = guide_legend(title = NULL)) +
  scale_x_discrete(limits = c("cloudy", "sunny", "rainy")) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")
```

  </div>
</div>

くもり☁️ $\beta_0$ を基準に、晴れの効果☀️ $\beta_1$ と雨の効果☔️ $\beta_2$ が求まる。

GLMなら確率分布・リンク関数を変えてもっと柔軟にモデリングできる。


---
## 共分散分析 (<u>An</u>alysis of <u>cova</u>riance, ANCOVA) as GLM

**質的変数と量的変数を両方**含むGLM、と解釈可能。<br>
正規分布・等分散・恒等リンクなどが仮定される。


<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

| 天気 | → | $x_1$ ☀️ 晴れ | $x_2$ ☔️ 雨 |
| ---- | :-: | :---: | :---: |
| ☁️ くもり | | 0 | 0 |
| ☀️ 晴れ | | 1 | 0 |
| ☔️ 雨 | | 0 | 1 |

<p>\[\begin{split}
y_i &= \mathcal{N}(\mu_i,\sigma^2) \\
\mu_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i}
\end{split}\]</p>

  </div>
  <div class="column" style="flex-shrink: 1.3;">


```{r glm-ancova, echo = FALSE, fig.height = 4.5, fig.width = 4.5}
lm_ancova = lm(beer_sales ~ temperature + weather, df_beer)
df_pred = tidyr::crossing(temperature = seq(8, 32, 2), weather = factor(weather_levels, levels = weather_levels)) %>%
  modelr::add_predictions(lm_ancova) %>%
  dplyr::mutate(y_pred = pred)

ggplot(df_beer) + aes(temperature, beer_sales, color = weather) +
  geom_line(data = df_pred, aes(y = y_pred, group = weather), alpha = 0.7, size = 2) +
  geom_point(alpha = 0.6, shape = 16, size = 3) +
  scale_color_viridis_d(direction = -1, guide = guide_legend(title = NULL)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))
```

  </div>
</div>

GLMなら確率分布・リンク関数を変えてもっと柔軟にモデリングできる。


---
## 一般化線形モデル(GLM)ふりかえり

確率分布・リンク関数を変えて柔軟にモデリングできる。<br>
特定の組み合わせには名前がある。

| 名前 | 確率分布 | リンク関数 | 説明変数 |
| ---- | -------- | -------- | -------- |
|ポアソン回帰|ポアソン分布|log| |
|ロジスティック回帰|二項分布|logit| |
|一般線形回帰|正規分布|恒等| |
|分散分析|正規分布|恒等|質的変数|
|共分散分析|正規分布|恒等|質的変数+量的変数|

リンク関数をもう少しだけ掘り下げたい。


---
## リンク関数

統計モデリングにおいて「まっすぐ以外も表現できる」意味

$\text{identity}(\mu_i)$
: $\mu_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots$
: 説明変数の効果が**足し算**的に働く。

$\log(\lambda_i)$
: $\lambda_i = e^{\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots} = e^{\beta_0} \times e^{\beta_1 x_{1i}} \times e^{\beta_2 x_{2i}} \times \ldots$
: 説明変数の効果が**掛け算**的に働く。<br>
  e.g., $\Delta x_1$ 増えると $e^{\beta_1 \Delta x_{1}}$ 倍になる

$\text{logit}(p_i)$
: $p_i = \frac 1 {1 + e^{-(\beta_0 + \beta_1 x_i + \ldots)}} $ (ロジスティック関数)
: 説明変数の効果が**頭打ち**になる。<br>
  e.g., $\lim_{x \to -\infty} p = 0;~\lim_{x \to \infty} p = 1$

ほかに `probit`, `inverse`, `sqrt`, etc.


---
## データはひとつ、モデルはたくさん

どう選ぶ？

1. メカニズム的に納得できるものを選ぶ
    - ポアソン過程の**カウント**ならポアソン分布、**間隔**ならガンマ分布
    - n回中k回のように**割合的なカウント**なら二項分布
1. データを可視化してみて、それっぽい形・性質のものを選ぶ
    - **左右対称のひと山**ならとりあえず正規分布
    - **負の値を取らない**ならガンマ分布
    - 直線的か、指数関数的か、頭打ちか、などなど

客観的な指標もほしい。<br>
モデルの尤もらしさといえば...


---
## <ruby>尤<rt>ゆう</rt>度</ruby> (likelihood)

**あるモデル$M$の下でそのデータ$D$が観察される確率**:<br>
$\text{Prob}(D \mid M)$

データ$D$を固定し、モデル$M$の関数とみなしたものが**尤度関数**:<br>
$L(M \mid D)$

モデルの構造も固定してパラメータ$\theta$だけ動かす場合はこう書く:<br>
$L(\theta \mid D)$ or $L(\theta)$

**対数尤度** $\log L$ の形にしたほうがいろいろ便利。

<hr>

各モデルで最適なパラメータを探して、比較:<br>
$\log L^* (M_1) \text{ vs. } \log L^* (M_2) \text{ vs. } \log L^* (M_3) \ldots$


---
## たしかに尤度はあてはまりの良さを表してそう

この場合は直線回帰よりもポアソン回帰が良さそう:

```{r compare-loglik, echo = FALSE, fig.height = 5, fig.width = 9}
n = 300L
a = 3
b = -3
df_pois = tibble::tibble(x = runif(n, 0.4, 1.7), y = rpois(n, exp(a * x + b)))

models = setNames(, c("gaussian", "poisson")) %>% purrr::map(~{
  glm(y ~ x, family = .x, data = df_pois)
})

x_breaks = c(0.5, 1.0, 1.5)
df_lm = tidyr::crossing(x = x_breaks, y = seq(-5, 20, 0.1)) %>%
  modelr::add_predictions(models[["gaussian"]]) %>%
  dplyr::mutate(density = dnorm(y, pred, 1.4)) %>%
  dplyr::filter(density > 1e-4)

p_pois = ggplot(df_pois) + aes(x, y) +
  ggridges::geom_vridgeline(data = df_lm, aes(width = density * 0.4, group = x), linetype = 0, alpha = 0) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  scale_x_continuous(breaks = x_breaks) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())

label = sprintf("logLik = %.1f", broom::glance(models[["gaussian"]])$logLik)
p_lm = p_pois +
  labs(title = "gaussian, identity link") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = label, color = "#3366ff", size = 6) +
  stat_smooth(formula = y ~ x, method = lm, se = FALSE) +
  ggridges::geom_vridgeline(data = df_lm, aes(width = density * 0.4, group = x), fill = "#3366ffaa", linetype = 0)
# p_lm

df_ridges = tidyr::crossing(x = x_breaks, y = seq_len(30L) - 1L) %>%
  modelr::add_predictions(models[["poisson"]]) %>%
  dplyr::mutate(density = dpois(y, exp(pred))) %>%
  dplyr::filter(density > 1e-4)
df_bars = df_ridges %>% wtl::ridges2bars(y, density)

label = sprintf("logLik = %.1f", broom::glance(models[["poisson"]])$logLik)
p_poisson = p_pois +
  labs(title = "poisson, log link") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = label, color = "#3366ff", size = 6) +
  stat_smooth(formula = y ~ x, method = glm, method.args = list(family = poisson), se = FALSE) +
  ggridges::geom_vridgeline(data = df_bars, aes(width = density * 0.5, group = x), fill = "#3366ffaa", linetype = 0)
# p_poisson

cowplot::plot_grid(p_lm, p_poisson, nrow = 1L)
```

この調子で、より尤度の高いモデルを探していけばいいだろうか？

---
## あてはまりが良ければいいってもんでもない

過剰適合 / 過学習 / overfitting
: パラメータを増やせば**現データへの**適合度・尤度を高くできるが、<br>
  予測・理解の役には立たなくなる。

```{r saturated-model, echo = FALSE, fig.height = 4, fig.width = 11}
n = 16L
true_coef = c(0.1, 0.2)
df_plant = tibble::tibble(
  x = runif(n, 7, 12.5),
  lambda = exp(true_coef[1] + true_coef[2] * x),
  y = rpois(n, lambda)
) %>% tibble::rownames_to_column("id")

models = df_plant %>% modelr::fit_with(glm, family = "poisson", modelr::formulas(~y,
  null = ~ 1,
  x = ~ x,
  saturated = ~ id
))
labels = setNames(sprintf("logLik = %.1f", purrr::map_dbl(models, logLik)), names(models))

p_plant = df_plant %>%
  modelr::add_predictions(models$null) %>%
  ggplot() + aes(x, y) +
  geom_line(aes(y = exp(pred)), color = "#3366ff", size = 2, alpha = 0.7) +
  geom_point(shape = 16, alpha = 0.6) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")

p_null = p_plant +
  labs(title = "null model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["null"]], color = "#3366ff", size = 6)
p_x = p_plant %+% modelr::add_predictions(df_plant, models$x) +
  labs(title = expression(y %~% beta[0] + beta[1] * x)) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x"]], color = "#3366ff", size = 6)
p_saturated = p_plant %+% modelr::add_predictions(df_plant, models$saturated) +
  labs(title = "saturated model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["saturated"]], color = "#3366ff", size = 6)

cowplot::plot_grid(p_null, p_x, p_saturated, nrow = 1L)
```

**帰無モデル**: 説明変数なし。切片のみ。<br>
**飽和モデル**: データ点の数 ≤ パラメータの数。“データ読み上げ”的モデル


---
## 無駄な説明変数を加えても尤度は上がる

ある植物が作る種の数 $y$ は個体のサイズ $x$ に応じて増える。<br>
観察時に着てた服の色 $x_2$ を追加すると尤度が上がる......?

```{r many-models, echo = FALSE, fig.height = 7, fig.width = 7}
set.seed(24601)
n = 120L
true_coef = c(1, 0.12, 0)
df_plant = tibble::tibble(
  x = runif(n, 7, 12.5),
  x2 = sample(c(FALSE, TRUE), n, replace = TRUE),
  lambda = exp(true_coef[1] + true_coef[2] * x + true_coef[3] * x2),
  y = rpois(n, lambda)
) %>% tibble::rownames_to_column("id")

models = df_plant %>% modelr::fit_with(glm, family = "poisson", modelr::formulas(~y,
  null = ~ 1,
  x = ~ x,
  x2 = ~ x2,
  both = ~ x + x2,
  saturated = ~ id
))
labels = setNames(sprintf("logLik = %.1f", purrr::map_dbl(models, logLik)), names(models))

p_plant = df_plant %>%
  modelr::add_predictions(models$null) %>%
  ggplot() + aes(x, y) +
  geom_line(aes(y = exp(pred)), size = 1.5, alpha = 0.6) +
  geom_point(shape = 16, alpha = 0.6) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")

p_null = p_plant +
  labs(title = "null model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["null"]], color = "#3366ff", size = 6)
p_x = p_plant %+% (df_plant %>% modelr::add_predictions(models$x)) +
  labs(title = expression(y %~% beta[0] + beta[1] * x)) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x"]], color = "#3366ff", size = 6)
p_x2 = p_plant %+% (df_plant %>% modelr::add_predictions(models$x2)) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x2"]], color = "#3366ff", size = 6)
p_both = p_plant %+% (df_plant %>% modelr::add_predictions(models$both)) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[1] * x + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["both"]], color = "#3366ff", size = 6)

cowplot::plot_grid(p_null, p_x, p_x2, p_both, nrow = 2L)
```



---
## AIC: 赤池情報量基準

<p>\[\begin{split}
\text{AIC} = -2 (\log L^* - k) = -2 \log L^* + 2k
\end{split}\]</p>

- AICが小さいほど予測精度の良いモデル。
    - 尤度は上げたい。
    - パラメータ数 $k$ が増えるとペナルティ。
- どのデータに対する当てはまりを目指すかという観点
    - 「手元のデータ」に対する対数尤度は $\log L^*$<br>
    - 「真のメカニズムから出てくる未来のデータ」に対する<br>
      平均対数尤度の推定量は $(\log L^* - k)$<br>
      (Kullback--Leibler情報量を使って導出するらしい)


???
https://www.slideshare.net/logics-of-blue/1-6aic


---
## 無駄な説明変数の追加でAIC増加

ある植物が作る種の数 $y$ は個体のサイズ $x$ に応じて増える。<br>
観察時に着てた服の色 $x_2$ を追加したモデルはAICが増加。

```{r many-models-aic, echo = FALSE, fig.height = 7, fig.width = 7}
labels = setNames(sprintf("AIC = %.1f", purrr::map_dbl(models, AIC)), names(models))
p_null = p_plant +
  labs(title = "null model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["null"]], color = "#3366ff", size = 6)
p_x = p_plant %+% (df_plant %>% modelr::add_predictions(models$x)) +
  labs(title = expression(y %~% beta[0] + beta[1] * x)) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x"]], color = "#3366ff", size = 6)
p_x2 = p_plant %+% (df_plant %>% modelr::add_predictions(models$x2)) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x2"]], color = "#3366ff", size = 6)
p_both = p_plant %+% (df_plant %>% modelr::add_predictions(models$both)) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[1] * x + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["both"]], color = "#3366ff", size = 6)
cowplot::plot_grid(p_null, p_x, p_x2, p_both, nrow = 2L)
```

---
## ほかの情報量基準

- $\text{BIC} = -2 \log L^* + k \log n$
    - パラメータ数 $k$ でペナルティを付けるのはAICと同じ。
    - データの観測数 $n$ に依存する点でAICと異なる。<br>
      感覚としては「AICはデータサイズによるペナルティが無い」
    - (周辺尤度の最大化という観点で導出するらしい)
- [WAIC](http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/waic2011.html),
  [WBIC](http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/wbic2012.html)
    - AIC, BICを一般化し、広く使えるようにしたもの。
    - 理想的な条件ではそれぞれAIC, BICとほぼ同じ。<br>
      そうじゃない場合(現実的には常に)こちらが優位。
    - WAICは予測の良さ、WBICは真のモデルへの近さ、を表す。


---
## モデル選択の心構え

「正しい」ものを選べるわけではない。<br>
予測・理解に useful なものを何らかの基準で選ぶだけ。

> All models are wrong, but some are useful. --- George E. P. Box

<figure>
<img src="../tokiomarine2021/math-model.drawio.svg" width="600"><br>
<figcaption><cite>「データ分析のための数理モデル入門」江崎貴裕 2020 より改変</cite></figcaption>
</figure>


---
## 現実的な注意点・悩みどころ

- 多重共線性(multicollinearity):
    - 説明変数同士が強い相関関係にある
- 変数変換:
    - 気安くやるべきじゃないけど、対数変換などしばしば有用
    - 割り算した値は危険
- 交互作用を入れると解釈が難しくなる。


---
## 交互作用

ある説明変数の効果が、別の説明変数によって異なる。<br>
e.g., ビール売上の温度依存性が天気によって異なる。

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 0.1rem;">

| 天気 | $x_1$ |
| ---- | :---: |
| ☀️ 晴れ | 1 |
| ☔️ 雨 | 0 |

<p>\[\begin{split}
y_i &= \mathcal{N}(\mu_i,\sigma^2) \\
\mu_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_{1,2} x_{1i} x_{2i}
\end{split}\]</p>

雨の日は $x_{1i} = 0$ のため $\beta_0,~\beta_2$ の項だけ。<br>
晴れの日はそれに加えて $\beta_1,~\beta_{1,2}$ の項も。

  </div>
  <div class="column" style="flex-shrink: 1.3;">

```{r interaction, echo = FALSE, fig.height = 4.5, fig.width = 4.5}
n = 200L
coef = c(70, 3, 100, -2)
weather_levels = c("sunny", "rainy")
df_beer = tibble::tibble(
    temperature = runif(n, 8, 32),
    weather = factor(sample(weather_levels, n, TRUE), levels = weather_levels)
  ) %>%
  dplyr::mutate(name = weather, value = 1L) %>%
  tidyr::pivot_wider(values_fill = 0L) %>%
  dplyr::mutate(mu = coef[1] * sunny + coef[2] * temperature + coef[3] * rainy + coef[4] * temperature * rainy) %>%
  dplyr::mutate(beer_sales = rnorm(n, mu, 10))

lm_int = lm(beer_sales ~ temperature * weather, df_beer)
df_pred = tidyr::crossing(temperature = seq(8, 32, 2), weather = factor(weather_levels, levels = weather_levels)) %>%
  modelr::add_predictions(lm_int) %>%
  dplyr::mutate(y_pred = pred)

ggplot(df_beer) + aes(temperature, beer_sales, color = weather) +
  geom_line(data = df_pred, aes(y = y_pred, group = weather), alpha = 0.7, size = 2) +
  geom_point(alpha = 0.6, shape = 16, size = 3) +
  scale_color_viridis_d(direction = -1, guide = guide_legend(title = NULL)) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))
```

  </div>
</div>


解釈が一気に難しくなるのでむやみに使わない。

---
## 一般化線形モデル座学まとめ

- 何はともあれ散布図を描く
- 適切な確率分布・リンク関数・説明変数を考える
- パラメータを最尤推定する
- 尤度は「手元のデータへのあてはまり」
- モデルを比較するときは情報量基準を参考にする


---
## penguinsデータセット

<a href="https://allisonhorst.github.io/palmerpenguins/">
<cite>https://allisonhorst.github.io/palmerpenguins/</cite><br>
<img src="/slides/image/rstats/lter_penguins.png" width="45%">
<img src="/slides/image/rstats/culmen_depth.png" width="45%">
</a>

```r
# install.packages("palmerpenguins")
library(palmerpenguins)
print(penguins)
penguins_colors = c(Adelie = "darkorange", Chinstrap = "purple", Gentoo = "cyan4")
```

---
## penguinsデータセット

<a href="https://allisonhorst.github.io/palmerpenguins/">
<cite>https://allisonhorst.github.io/palmerpenguins/</cite><br>
<img src="/slides/image/rstats/lter_penguins.png" width="45%">
<img src="/slides/image/rstats/culmen_depth.png" width="45%">
</a>

```{r penguins, echo = FALSE, fig.height = 4.5, fig.width = 4.5}
if (!require(palmerpenguins, quietly = TRUE)) {
  install.packages("palmerpenguins")
  library(palmerpenguins)
}
penguins_colors = c(Adelie = "darkorange", Chinstrap = "purple", Gentoo = "cyan4")
print(penguins)
```

---
## 単回帰の練習: 1. まず作図

どうやら、重いペンギンほど翼長も長い。

```{r penguins-weight, fig.height = 5, fig.width = 5, warning = FALSE}
p_penweight = ggplot(penguins) +
  aes(body_mass_g, flipper_length_mm) +
  geom_point(shape = 16, alpha = 0.66) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())
p_penweight
```


---
## 単回帰の練習: 2. モデル作成、フィッティング

```{r penguins-fit1}
fit1 = glm(flipper_length_mm ~ body_mass_g, data = penguins)
broom::tidy(fit1)
broom::glance(fit1)
```

---
## 単回帰の練習: 3. フィッティング結果を作図

$y = 136.7 + 0.0153 x$

```{r penguins-weight-glm, fig.height = 5, fig.width = 5, warning = FALSE}
added1 = modelr::add_predictions(penguins, fit1)
p1 = p_penweight +
  geom_line(aes(y = pred), data = added1, size = 1, color = "#3366ff")
p1
```

---
## 重回帰の練習: 1. まず作図

重いペンギンほど翼長も長い。翼長は種によっても違うかも。

```{r penguins-weight-sp, fig.height = 5, fig.width = 7, warning = FALSE}
p_penweight_color = p_penweight + aes(color = species) +
  scale_color_manual(values = penguins_colors)
p_penweight_color
```


---
## 重回帰の練習: 2. モデル作成、フィッティング

Adelieを基準に、ChinstrapとGentooはそれより長め。<br>
体重の効果は単回帰のときより小さい。

```{r penguins-fit2}
fit2 = glm(flipper_length_mm ~ body_mass_g + species, data = penguins)
broom::tidy(fit2)
broom::glance(fit2)
```

---
## 重回帰の練習: 3. フィッティング結果を作図

```{r penguins-weight-sp-glm, fig.height = 5, fig.width = 7, warning = FALSE}
added2 = modelr::add_predictions(penguins, fit2)
p2 = p_penweight_color +
  geom_line(aes(y = pred), data = added2, size = 1)
p2
```

**傾き**も種によって違うかも。**交互作用**を入れてみたい。


---
## 交互作用の練習: モデル作成、フィッティング

Adelieを基準に、Chinstrapの傾きが結構違う。<br>
切片の違いは解釈しにくくなった。

```{r penguins-fit3}
fit3 = glm(flipper_length_mm ~ body_mass_g * species, data = penguins)
broom::tidy(fit3)
broom::glance(fit3)
```

---
## 交互作用の練習: フィッティング結果を作図

```{r penguins-interaction, fig.height = 5, fig.width = 7, warning = FALSE}
added3 = modelr::add_predictions(penguins, fit3)
p3 = p_penweight_color +
  geom_line(aes(y = pred), data = added3, size = 1)
p3
```

---
## ここまでの3つのモデルでどれがいいか？

AICで選ぶなら交互作用入り重回帰のが良さそう。

```r
AIC(fit1, fit2, fit3)$AIC
```
```{r penguins-aic, echo = FALSE, fig.height = 4, fig.width = 11, warning = FALSE}
labels = sprintf("AIC = %.1f", AIC(fit1, fit2, fit3)$AIC)
cowplot::plot_grid(p1 + labs(title = labels[1]),
                   p2 + labs(title = labels[2]) + theme(legend.position = "none"),
                   p3 + labs(title = labels[3]) + theme(legend.position = "none"), nrow = 1L)
```


---
## GLMの練習

🔰クチバシの長さと深さで同じ解析をやってみよう。

```{r penguins-bill, echo = FALSE, fig.height = 4, fig.width = 11, warning = FALSE}
p_bill = penguins %>%
  ggplot() + aes(bill_length_mm, bill_depth_mm) +
  geom_point(shape = 16, alpha = 0.66) +
  scale_color_manual(values = penguins_colors) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())

fit1 = glm(bill_depth_mm ~ bill_length_mm, data = penguins)
fit2 = glm(bill_depth_mm ~ bill_length_mm + species, data = penguins)
fit3 = glm(bill_depth_mm ~ bill_length_mm + species + bill_length_mm:species, data = penguins)
p1 = p_bill +
  geom_line(aes(y = pred), data = modelr::add_predictions(penguins, fit1), size = 1, color = "#3366ff")
p2 = p_bill + aes(color = species) +
  geom_line(aes(y = pred), data = modelr::add_predictions(penguins, fit2), size = 1)
p3 = p_bill + aes(color = species) +
  geom_line(aes(y = pred), data = modelr::add_predictions(penguins, fit3), size = 1)
labels = sprintf("AIC = %.1f", AIC(fit1, fit2, fit3)$AIC)
cowplot::plot_grid(p1 + labs(title = labels[1]),
                   p2 + labs(title = labels[2]) + theme(legend.position = "none"),
                   p3 + labs(title = labels[3]) + theme(legend.position = "none"), nrow = 1L)
```

🔰余裕があったら性別や年なども説明変数に入れてみよう。


---
## 確率分布とリンク関数を明示的に指定したい

何も指定しない場合は正規分布・恒等リンクだった:
```{r glm-default-family}
formula = flipper_length_mm ~ body_mass_g
fit1 = glm(formula, data = penguins)
fit1$family
```

こう書いたのと同じ:
```r
glm(formula, data = penguins, family = gaussian(link = identity))
```

利用可能な確率分布リンク関数は `?family` などを参照。


---
## n個のうちy個生存。二項分布に従......わない！

植物100個体から8個ずつ種子を取って植えたら全体で半分ちょい発芽。<br>
親1個体あたりの生存数は<span style="color: #3366ff;">n=8の二項分布</span>になるはずだけど、<br>
極端な値(全部死亡、全部生存)が多かった。個体差？

```{r overdispersion, echo = FALSE, fig.height = 5, fig.width = 6, warning = FALSE}
ninds = 100L
mu_ind = 0.5
sd_ind = 3
df_od = tibble::tibble(
  z = rnorm(ninds, mu_ind, sd_ind),
  p = wtl::sigmoid(z),
  y = rbinom(ninds, 8L, p))
sum_y = sum(df_od$y)
p_hat = sum_y / 800
tidy_od = df_od %>%
  dplyr::count(y, name = "observed") %>%
  dplyr::mutate(expected = ninds * dbinom(y, 8, p_hat)) %>%
  tidyr::pivot_longer(!y, names_to = "key", values_to = "count") %>%
  dplyr::mutate(width = ifelse(key == "expected", 0.8, 0.4), alpha = ifelse(key == "expected", 0.5, 1))
# label = expression(hat(p) == paste(sprintf("%d/800 = %.2f", sum_y, p_hat)))
label = bquote(hat(p) == .(paste(sprintf("%d/800 = %.2f", sum_y, p_hat))))
tidy_od %>%
  ggplot() + aes(y, count) +
  geom_col(aes(fill = key, width = width, alpha = alpha), position = "identity") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = label, color = "#3366ff", size = 6) +
  scale_alpha_identity() +
  scale_fill_manual(values = c(observed = "#333333", expected = "#3366ff")) +
  coord_cartesian(xlim = c(0, 8)) +
  labs(x = "# survived seeds") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(),
        legend.title = element_blank(), legend.position = "top")
```

もっと柔軟にモデリングしたい

---
## ちょっとずつ線形モデルを発展させていく

**線形モデル LM** (単純な直線あてはめ)

<span style="color: #888888;">&nbsp; &nbsp; ↓ いろんな<strong>確率分布</strong>を扱いたい</span>

**一般化線形モデル GLM**

<span style="color: #888888;">&nbsp; &nbsp; ↓ 個体差などの変量効果を扱いたい</span>

**一般化線形混合モデル GLMM**

<span style="color: #888888;">&nbsp; &nbsp; ↓ もっと自由なモデリングを！</span>

**階層ベイズモデル HBM**

<cite>[データ解析のための統計モデリング入門](https://amzn.to/33suMIZ) 久保拓弥 2012 より改変</cite>



---
## 統計モデリング入門まとめ

- 何はともあれ作図して俯瞰
- GLMは統計モデリングの考え方の根幹
    - 確率分布・リンク関数・説明変数
    - 尤度・最尤法によるパラメータ推定
    - 情報量基準などによるモデル選択
- より柔軟なモデリングについて今回は省略
    - 一般化線形混合モデル (GLMM)
    - 階層ベイズモデル (HBM)
    - フルバージョンの過去資料:
      [統計モデリング概論 DSHC 2021](/slides/tokiomarine2021/)

---
## 参考文献

- [データ解析のための統計モデリング入門](https://amzn.to/33suMIZ) 久保拓弥 2012
- [StanとRでベイズ統計モデリング](https://amzn.to/3uwx7Pb) 松浦健太郎 2016
- [RとStanではじめる ベイズ統計モデリングによるデータ分析入門](https://amzn.to/3o1eCzP) 馬場真哉 2019
- [データ分析のための数理モデル入門](https://amzn.to/3uCxTKo) 江崎貴裕 2020
- [分析者のためのデータ解釈学入門](https://amzn.to/3uznzCK) 江崎貴裕 2020
- [統計学を哲学する](https://amzn.to/3ty80Kv) 大塚淳 2020
- [科学とモデル---シミュレーションの哲学 入門](https://amzn.to/2Q0f6JQ) Michael Weisberg 2017<br>
  (原著: [Simulation and Similarity](https://amzn.to/3bdvhuI) 2013)
