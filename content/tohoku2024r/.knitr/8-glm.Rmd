```{r, setup-common}
#| file: "setup.R"
#| echo: false
#| results: "asis"
```
```{r, setup-local}
#| include: false
#| cache: false
knitr::knit_exit()
```

---
## 統計モデリングのお品書き

<figure style="float: right;">
<a href="https://kuboweb.github.io/-kubo/ce/IwanamiBook.html">
<img src="../tokiomarine2021/image/kubo-book.jpg" width="280" alt="データ解析のための統計モデリング入門 久保拓弥 2012">
</a>
</figure>

久保先生の"緑本"こと<br>
「データ解析のための統計モデリング入門」<br>
をベースに回帰分析の概要を紹介。

1. イントロ [(#7 前回)](7-distribution.html)
1. 統計モデルの基本
    - 確率変数・**確率分布** 👈 主役
    - 尤度・最尤推定
1. 一般化線形モデル、混合モデル [(#8 今回)](8-glm.html)
1. <del>ベイズ統計、階層ベイズモデル</del> (今回はここまでやらない)

回帰のキモは**線ではなく分布**。

<hr>
<cite>

[Data Science Hill Climb 2021 (東京海上) での講義 (~6時間)](https://heavywatal.github.io/slides/tokiomarine2021/)
の演習無し抜粋バージョン (~2時間 x 2回)。

</cite>


---
## 何でもかんでも直線あてはめではよろしくない

<img `r src_alt_fig_chunk("glm-better")`>

- 観察データは常に**正の値**なのに予測が負に突入してない？
- **縦軸は整数**。しかもの**ばらつき**が横軸に応じて変化？
- **データに合わせた統計モデルを使うとマシ**


---
## ここまで見てきた統計モデル

確率変数$X$はパラメータ$\theta$の確率分布$f$に“従う”:&nbsp;
$X \sim f(\theta) $

e.g., ある植物が作る種の数$X$は平均値$\lambda$のポアソン分布に従う:

<div>\[\begin{split}
X \sim \text{Poisson}(\lambda)
\end{split}\]</div>

```{r, df-poisson-playback}
#| echo: false
set.seed(24601)
df_rpois = tibble::tibble(X = rpois(50L, 3))
max_x = 11L
df_dpois = tibble(X = seq(0, max_x), Prob = dpois(X, mean(df_rpois$X)))
```

```{r, only-dist}
#| echo: false
#| fig.height: 4
#| fig.width: 4
p_pois = ggplot(df_rpois) + aes(X) +
  geom_bar(aes(y = after_stat(prop)), width = 0.3) +
  geom_col(data = df_dpois, aes(y = Prob), alpha = 0.5, fill = "#56B4E9") +
  theme_bw(base_size = 18)
p_pois +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

これを一般化線形モデル(GLM)として見ることもできる。


---
## 一般化線形モデル(GLM)として記述してみる

個体$i$の種子数$y_i$は平均値$\lambda_i$のポアソン分布に従う。<br>
平均値$\lambda_i$は**他のデータによらず$\beta_0$で一定**。

<div>\[\begin{split}
y_i &\sim \text{Poisson}(\lambda_i) \\
\lambda_i &= \beta_0
\end{split}\]</div>

```{r, glm-without-x}
#| echo: false
#| fig.height: 4
#| fig.width: 4
p_pois +
  labs(x = "y") +
  coord_flip() +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_blank(), axis.ticks = element_blank())
```

種子数をY軸にして、式を2つに分けただけ...?<br>
**説明変数**を含むモデルを見ればご利益が分かるかも。

---
## 説明変数が1つある一般化線形モデル

個体$i$の種子数$y_i$は平均値$\lambda_i$のポアソン分布に従う。<br>
平均値の対数$\log(\lambda_i)$は**その個体の大きさ$x_i$に比例**する。

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0;">

<figure style="margin-block: 1em 0;">
<img src="../tokiomarine2021/glm.drawio.svg" width="600"><br>
</figure>

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r, df-seeds}
#| include: false
#| cache.vars: ["df_seeds", "a", "b"]
set.seed(24601)
n = 300L
a = 3
b = -3
df_seeds = tibble::tibble(
  body_mass = runif(n, 0.4, 1.7),
  num_seeds = rpois(n, exp(a * body_mass + b))
) |>
  print()
```
```{r, glm-poisson}
#| echo: false
#| cache.vars: []
#| fig.height: 5
#| fig.width: 5
x_breaks = c(0.5, 1.0, 1.5)
df_ridges = tidyr::crossing(x = x_breaks, y = seq_len(30L) - 1L) |>
  dplyr::mutate(density = dpois(y, exp(a * x + b))) |>
  dplyr::filter(density > 1e-4)
df_bars = df_ridges |> wtl::ridges2bars(y, density)

p_pois = ggplot(df_seeds) + aes(body_mass, num_seeds) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  scale_x_continuous(breaks = x_breaks) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())

p_pois +
  stat_smooth(formula = y ~ x, method = glm, method.args = list(family = poisson), se = FALSE) +
  ggridges::geom_vridgeline(aes(x, y, width = density * 0.5, group = x),
    data = df_bars, fill = "#56B4E9AA", linetype = 0)
```

  </div>
</div>

この場合は**単回帰**。説明変数が複数あると**重回帰**。


---
## 複数の説明変数を同時に扱う重回帰

<p>\[\begin{split}
y_i &\sim \text{Poisson}(\lambda_i) \\
\log(\lambda_i) &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots
\end{split}\]</p>

気温も湿度も高いほどビールが売れる架空データ:

```{r, df-beer}
#| include: false
#| cache.vars: "df_beer"
set.seed(24601)
n = 200L
true_coef = c(3, 0.05, 0.006)
df_beer = tibble::tibble(
  temperature = runif(n, 8, 32),
  humidity = runif(n, 20, 80),
  beer_sales = rpois(n, exp(true_coef[1] + true_coef[2] * temperature + true_coef[3] * humidity))
) |>
  print()
```
```{r, multiple-regression}
#| echo: false
#| cache.vars: []
#| fig.height: 5
#| fig.width: 10
glm_multi = glm(beer_sales ~ temperature + humidity, family = poisson, data = df_beer)

df_pred = tidyr::crossing(temperature = seq(8, 32, 2), humidity = seq(20, 80, 5)) |>
  modelr::add_predictions(glm_multi, type = "response")

p1 = ggplot(df_beer) + aes(temperature, beer_sales, color = humidity) +
  geom_line(data = df_pred, aes(y = pred, group = humidity), alpha = 0.7) +
  geom_point(alpha = 0.5) +
  scale_color_viridis_c(option = "cividis", direction = -1) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))
p2 = ggplot(df_beer) + aes(humidity, beer_sales, color = temperature) +
  geom_line(data = df_pred, aes(y = pred, group = temperature), alpha = 0.7) +
  geom_point(alpha = 0.5) +
  scale_color_viridis_c(option = "turbo") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))

cowplot::plot_grid(p1, p2, nrow = 1L)
```

ほかの**確率分布**と**リンク関数**を使う例を見てみよう。


---
## ロジスティック回帰

- 確率分布: **二項分布**
- リンク関数: $\text{logit}(p) = \log \frac {p} {1 - p}$

何かの成否に対する何かの因子の影響、とか

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

客10人中$y_i$人がビールを注文。<br>
その日$i$の気温$x_i$によって割合が変化。

<p>\[\begin{split}
y_i &\sim \text{Binomial}(n,~p_i) \\
\text{logit}(p_i) &= \beta_0 + \beta_1 x_i \\
p_i &= \frac 1 {1 + e^{-(\beta_0 + \beta_1 x_i)}}
\end{split}\]</p>

ロジスティック関数↑

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r, df-logistic}
#| include: false
#| cache.vars: ["df_logistic", "n"]
set.seed(24601)
sigmoid = function(x, gain = 1) {1 / (1 + exp(-gain * x))}
nrep = 200L
n = 10L
df_logistic = tibble::tibble(
  x = runif(nrep, -10, 35),
  logit_p = -3 + 0.3 * x,
  p = sigmoid(logit_p),
  y = rbinom(nrep, n, p),
  response = matrix(c(y, n - y), ncol = 2)
) |>
  print()
```
```{r, glm-logistic}
#| echo: false
#| cache.vars: []
#| fig.height: 5
#| fig.width: 5
glm_logistic = glm(response ~ x, df_logistic, family = binomial)
df_pred = df_logistic |>
  modelr::add_predictions(glm_logistic, type = "response") |>
  dplyr::mutate(pred = n * pred)

coef = glm_logistic$coefficients

x_breaks = c(-10, 0, 10, 20, 30)
df_ridges = tidyr::crossing(x = x_breaks, y = seq.int(0, n)) |>
  dplyr::mutate(p = sigmoid(coef[1] + coef[2] * x), density = dbinom(y, n, p)) |>
  dplyr::filter(density > 1e-4)
df_bars = df_ridges |> wtl::ridges2bars(y, density)

ggplot(df_pred) + aes(x, y) +
  geom_point(alpha = 0.5, shape = 16) +
  ggridges::geom_vridgeline(data = df_bars, aes(width = density * 6, group = x), fill = "#56B4E9AA", linetype = 0) +
  geom_line(aes(y = pred), linewidth = 2, color = "#3366ff") +
  scale_x_continuous(breaks = x_breaks) +
  scale_y_continuous(breaks = seq.int(0, 10)) +
  labs(x = "temperature", y = "beer_sales") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())
```

  </div>
</div>

???
ロジット = 対数オッズ
オッズ = 失敗の何倍成功しやすいか
Xが1増えるとオッズがe^a倍に増える。


---
## ロジスティック回帰 (狭義)

- 確率分布: **ベルヌーイ分布** ($n = 1$ の二項分布)
- リンク関数: $\text{logit}(p) = \log \frac {p} {1 - p}$

何かの成否に対する何かの因子の影響、とか

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

風が吹けば桶屋が儲かる。

<p>\[\begin{split}
y_i &\sim \text{Bernoulli}(p_i) \\
  &= \text{Binomial}(1,~p_i) \\
\text{logit}(p_i) &= \beta_0 + \beta_1 x_i \\
p_i &= \frac 1 {1 + e^{-(\beta_0 + \beta_1 x_i)}}
\end{split}\]</p>

ロジスティック関数↑

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r, df-wind}
#| include: false
#| cache.vars: "df_wind"
set.seed(24601)
n = 200
df_wind = tibble::tibble(
  max_wind = runif(n, 0, 40),
  bucket_sales = rbinom(n, 1L, sigmoid(max_wind - 20, 0.2)) + 0L) |>
  print()
```
```{r, wind}
#| echo: false
#| cache.vars: []
#| fig.height: 4
#| fig.width: 5
glm_bernoulli = glm(bucket_sales ~ max_wind, df_wind, family = "binomial")

coef = glm_bernoulli$coefficients
x_breaks = c(0, 10, 20, 30, 40)
df_ridges = tidyr::crossing(x = x_breaks, y = c(0, 1)) |>
  dplyr::mutate(p = sigmoid(coef[1] + coef[2] * x), density = dbinom(y, 1, p)) |>
  dplyr::filter(density > 1e-4)
df_bars = df_ridges |> wtl::ridges2bars(y, density, width = 0.2)

df_wind |>
  modelr::add_predictions(glm_bernoulli, type = "response") |>
  ggplot() + aes(max_wind, bucket_sales) +
  geom_point(alpha = 0.3, shape = 124, size = 6) +
  ggridges::geom_vridgeline(aes(x, y, width = density * 6, group = x),
    data = df_bars, fill = "#56B4E9AA", linetype = 0) +
  geom_line(aes(y = pred), color = "#3366ff") +
  scale_y_continuous(breaks = c(0, 1)) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())
```

  </div>
</div>


---
## 一般線形モデル (“化”無し) はGLMの一種

- 確率分布: **正規分布**
- リンク関数: **恒等関数**(なにもせずそのまま)

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

<p>\[\begin{split}
y_i &\sim \mathcal{N}(\mu_i,~\sigma^2) \\
\text{identity}(\mu_i) &= \beta_0 + \beta_1 x_i
\end{split}\]</p>

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r, df-weight}
#| include: false
#| cache.vars: "df_weight"
set.seed(19937)
n = 50
df_weight = tibble::tibble(
  height = rnorm(n, 1.70, 0.05),
  bmi = rnorm(n, 22, 1),
  weight = bmi * (height**2)
) |>
  print()
```
```{r, glm-weight}
#| echo: false
#| cache.vars: []
#| fig.height: 4
#| fig.width: 4
fit = lm(weight ~ height, df_weight)
coef = coef(fit)

x_breaks = c(1.65, 1.7, 1.75)
df_ridges = tidyr::crossing(height = x_breaks, weight = seq(50, 80, 0.2)) |>
  dplyr::mutate(density = dnorm(weight, coef[1] + coef[2] * height, 1.8)) |>
  dplyr::filter(density > 1e-4)

ggplot(df_weight) + aes(height, weight) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  ggridges::geom_vridgeline(aes(width = density * 0.08, group = height),
    data = df_ridges, fill = "#56B4E9AA", linetype = 0) +
  stat_smooth(method = lm, formula = y ~ x, se = FALSE) +
  scale_x_continuous(breaks = x_breaks) +
  theme_bw(base_size = 20) + theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())
```

  </div>
</div>

最小二乗法の直線あてはめと結果的に同じになる。

<small style="color: #999999;">単回帰・重回帰と言ったとき一般線形モデルを前提とする人もいる。</small>

---
## 分散分析 (<u>An</u>alysis <u>o</u>f <u>va</u>riance, ANOVA) as GLM

**質的な説明変数**を持つ**正規分布・恒等リンク**のGLM、と解釈可能。<br>
<span title="ダミー変数とも呼ばれる">**指示変数**</span> (0 or 1) に変換してから重回帰する。

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

| 天気 | → | $x_1$ ☀️ 晴れ | $x_2$ ☔️ 雨 |
| ---- | :-: | :---: | :---: |
| ☁️ くもり | | 0 | 0 |
| ☀️ 晴れ | | 1 | 0 |
| ☔️ 雨 | | 0 | 1 |

<p>\[\begin{split}
y_i &\sim \mathcal{N}(\mu_i,\sigma^2) \\
\mu_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}
\end{split}\]</p>

  </div>
  <div class="column" style="flex-shrink: 1.3;">

```{r, df-ancova}
#| include: false
#| cache.vars: ["df_ancova", "weather_levels"]
set.seed(19937)
n = 200L
b = c(70, 3, 20, -20)  # true coef
weather_levels = c("sunny", "cloudy", "rainy")
df_ancova = tibble::tibble(
    temperature = runif(n, 8, 32),
    weather = factor(sample(weather_levels, n, TRUE), levels = weather_levels)
  ) |>
  dplyr::mutate(name = weather, value = 1L) |>
  tidyr::pivot_wider(values_fill = 0L) |>
  dplyr::select(!cloudy) |>
  dplyr::mutate(mu = b[1] + b[2] * temperature + b[3] * sunny + b[4] * rainy) |>
  dplyr::mutate(beer_sales = rnorm(n, mu, 10)
) |>
  print()
```

```{r, glm-anova}
#| echo: false
#| cache.vars: []
#| fig.height: 4.5
#| fig.width: 4.5
lm_anova = lm(beer_sales ~ weather, df_ancova)
df_ridges = tidyr::crossing(weather = factor(weather_levels, levels = weather_levels), beer_sales = seq(50, 200, 1)) |>
  modelr::add_predictions(lm_anova) |>
  dplyr::mutate(density = dnorm(beer_sales, pred, 10)) |>
  dplyr::filter(density > 1e-4)

tidy_anova = broom::tidy(lm_anova)

avgs = tidyr::crossing(weather = factor(weather_levels, levels = weather_levels)) |>
  modelr::add_predictions(lm_anova) |>
  tibble::deframe()

dfl = tibble::tribble(
  ~x, ~xend, ~y, ~yend,
  -Inf, Inf, avgs["cloudy"], avgs["cloudy"],
  1.5, 2.5, avgs["sunny"], avgs["sunny"],
  2.5, 3.5, avgs["rainy"], avgs["rainy"]
)

dfa = tibble::tribble(
  ~x, ~xend, ~y, ~yend,
  1.75, 1.75, avgs["cloudy"], avgs["sunny"],
  2.75, 2.75, avgs["cloudy"], avgs["rainy"]
)

dfs = tibble::tribble(
  ~x, ~y, ~label,
  0.6, avgs["cloudy"] + (avgs["sunny"] - avgs["cloudy"]) * 0.3, "beta[0]",
  1.55, (avgs["cloudy"] + avgs["sunny"]) / 2, "beta[1]",
  2.55, (avgs["cloudy"] + avgs["rainy"]) / 2, "beta[2]"
)

set.seed(1)
.arr = grid::arrow(length = grid::unit(0.1, "inches"))
df_ancova |>
  ggplot() + aes(weather, beer_sales, color = weather) +
  ggridges::geom_vridgeline(aes(width = density * 6, group = weather),
    data = df_ridges, fill = "#56B4E9AA", linetype = 0) +
  annotate("segment", x = dfl$x, xend = dfl$xend, y = dfl$y, yend = dfl$yend, color = "#56B4E9AA") +
  annotate("segment", x = dfa$x, xend = dfa$xend, y = dfa$y, yend = dfa$yend, arrow = .arr, color = "#56B4E9AA") +
  annotate("text", x = dfs$x, y = dfs$y, label = dfs$label, parse = TRUE, size = 6, color = "#56B4E9AA") +
  geom_jitter(width = 0.08, height = 0, alpha = 0.66, shape = 16, size = 3) +
  scale_color_viridis_d(direction = -1, guide = guide_legend(title = NULL)) +
  scale_x_discrete(limits = c("cloudy", "sunny", "rainy")) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank(),
        legend.position = "none")
```

  </div>
</div>

くもり☁️ $\beta_0$ を基準に、晴れの効果☀️ $\beta_1$ と雨の効果☔️ $\beta_2$ が求まる。

GLMなら確率分布・リンク関数を変えてもっと柔軟にモデリングできる。


---
## 共分散分析 (<u>An</u>alysis of <u>cova</u>riance, ANCOVA) as GLM

**質的変数と量的変数を両方**含むGLM、と解釈可能。<br>
正規分布・等分散・恒等リンクなどが仮定される。


<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

| 天気 | → | $x_1$ ☀️ 晴れ | $x_2$ ☔️ 雨 |
| ---- | :-: | :---: | :---: |
| ☁️ くもり | | 0 | 0 |
| ☀️ 晴れ | | 1 | 0 |
| ☔️ 雨 | | 0 | 1 |

<p>\[\begin{split}
y_i &\sim \mathcal{N}(\mu_i,\sigma^2) \\
\mu_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i}
\end{split}\]</p>

  </div>
  <div class="column" style="flex-shrink: 1.3;">


```{r, glm-ancova}
#| echo: false
#| cache.vars: []
#| fig.height: 4.5
#| fig.width: 4.5
lm_ancova = lm(beer_sales ~ temperature + weather, df_ancova)
df_pred = tidyr::crossing(temperature = seq(8, 32, 2), weather = factor(weather_levels, levels = weather_levels)) |>
  modelr::add_predictions(lm_ancova, type = "response")

ggplot(df_ancova) + aes(temperature, beer_sales, color = weather) +
  geom_line(data = df_pred, aes(y = pred, group = weather), alpha = 0.7, linewidth = 2) +
  geom_point(alpha = 0.6, shape = 16, size = 3) +
  scale_color_viridis_d(direction = -1, guide = guide_legend(title = NULL)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))
```

  </div>
</div>

GLMなら確率分布・リンク関数を変えてもっと柔軟にモデリングできる。


---
## 交互作用

ある説明変数の効果が、別の説明変数によって異なる。<br>
e.g., ビール売上の温度依存性が天気によって異なる。

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 0.1rem;">

| 天気 | $x_1$ |
| ---- | :---: |
| ☀️ 晴れ | 1 |
| ☔️ 雨 | 0 |

<p>\[\begin{split}
y_i &\sim \mathcal{N}(\mu_i,\sigma^2) \\
\mu_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_{1,2} x_{1i} x_{2i}
\end{split}\]</p>

雨の日は $x_{1i} = 0$ のため $\beta_0,~\beta_2$ の項だけ。<br>
晴れの日はそれに加えて $\beta_1,~\beta_{1,2}$ の項も。

  </div>
  <div class="column" style="flex-shrink: 1.3;">

```{r, df-interact}
#| include: false
#| cache.vars: ["df_interact", "weather_levels"]
set.seed(19937)
n = 200L
b = c(70, 3, 100, -2)  # true coef
weather_levels = c("sunny", "rainy")
df_interact = tibble::tibble(
    temperature = runif(n, 8, 32),
    weather = factor(sample(weather_levels, n, TRUE), levels = weather_levels)
  ) |>
  dplyr::mutate(name = weather, value = 1L) |>
  tidyr::pivot_wider(values_fill = 0L) |>
  dplyr::mutate(mu = b[1] * sunny + b[2] * temperature + b[3] * rainy + b[4] * temperature * rainy) |>
  dplyr::mutate(beer_sales = rnorm(n, mu, 10)) |>
  print()
```
```{r, interaction}
#| echo: false
#| cache.vars: []
#| fig.height: 4.5
#| fig.width: 4.5
lm_int = lm(beer_sales ~ temperature * weather, df_interact)
df_pred = tidyr::crossing(temperature = seq(8, 32, 2), weather = factor(weather_levels, levels = weather_levels)) |>
  modelr::add_predictions(lm_int, type = "response")

ggplot(df_interact) + aes(temperature, beer_sales, color = weather) +
  geom_line(data = df_pred, aes(y = pred, group = weather), alpha = 0.7, linewidth = 2) +
  geom_point(alpha = 0.6, shape = 16, size = 3) +
  scale_color_viridis_d(direction = -1, guide = guide_legend(title = NULL)) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))
```

  </div>
</div>


解釈が一気に難しくなるのでむやみに使わない。



---
## 一般化線形モデル(GLM)ふりかえり

確率分布・リンク関数を変えて柔軟にモデリングできる。<br>
特定の組み合わせには名前がある。

| 名前 | 確率分布 | リンク関数 | 説明変数 |
| ---- | -------- | -------- | -------- |
|ポアソン回帰|ポアソン分布|log| |
|ロジスティック回帰|二項分布|logit| |
|一般線形回帰|正規分布|恒等| |
|分散分析|正規分布|恒等|質的変数|
|共分散分析|正規分布|恒等|質的変数+量的変数|

リンク関数をもう少しだけ掘り下げたい。


---
## リンク関数

統計モデリングにおいて「まっすぐ以外も表現できる」意味

$\text{identity}(\mu_i)$
: $\mu_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots$
: 説明変数の効果が**足し算**的に働く。

$\log(\lambda_i)$
: $\lambda_i = e^{\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots} = e^{\beta_0} \times e^{\beta_1 x_{1i}} \times e^{\beta_2 x_{2i}} \times \ldots$
: 説明変数の効果が**掛け算**的に働く。<br>
  e.g., $\Delta x_1$ 増えると $e^{\beta_1 \Delta x_{1}}$ 倍になる

$\text{logit}(p_i)$
: $p_i = \frac 1 {1 + e^{-(\beta_0 + \beta_1 x_i + \ldots)}} $ (ロジスティック関数)
: 説明変数の効果が**頭打ち**になる。<br>
  e.g., $\lim_{x \to -\infty} p = 0;~\lim_{x \to \infty} p = 1$

ほかに `probit`, `inverse`, `sqrt`, etc.


---
## RにおけるGLMのやりかた

直線回帰のときの `lm` とほぼ同じ。

```{r, glm}
formula = weight ~ height
fit = glm(formula, data = df_weight)
coef(fit)
```

デフォルトは正規分布・恒等リンクで `lm` と同じ結果。<br>
`family=` オプションで確率分布とリンク関数を明示的に指定:
```r
glm(formula, family = gaussian(link = identity), data = mydata)
glm(formula, family = poisson(link = log), data = mydata)
glm(formula, family = binomial(link = logit), data = mydata)
```

See [`?family`](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/family.html) for more details.


---
## 🔰 とにかくGLMを使ってみる練習

とりあえず当てはめと作図だけ。<br>
結果の解釈やモデルの評価はこの後。

```{r, generate-df-weight}
#| ref.label: "df-weight"
#| echo: -1
#| cache.vars: []
```

---
## 🔰 とにかくGLMを使ってみる練習 解答例

```{r, glm-df-weight}
#| fig.width: 4
#| fig.height: 4
fit_wh = glm(weight ~ height, family = gaussian(link = identity), data = df_weight)
coef(fit_wh)
df_fit_wh = modelr::add_predictions(df_weight, fit_wh, type = "response")
ggplot(df_fit_wh) +
  aes(height, weight) +
  geom_point() +
  geom_line(aes(y = pred), linewidth = 1, color = "#3366ff")
```

---
## 🔰 ポアソン回帰

```{r, generate-df-seeds}
#| ref.label: "df-seeds"
#| echo: -1
#| cache.vars: []
```

---
## 🔰 重回帰

`pred` で回帰線を引くには `add_predictions()` の使い方に工夫が必要。<br>
とりあえず `geom_point()` で"回帰点々"を表示してみるとこまでで可とする。

```{r, generate-df-beer}
#| ref.label: "df-beer"
#| echo: -1
#| cache.vars: []
```

---
## 🔰 ロジスティック回帰

次ページにヒント。

```{r, generate-df-logistic}
#| ref.label: "df-logistic"
#| echo: -1
#| cache.vars: []
```

---
## ロジスティック回帰のヒント

左辺の応答変数に指定できるのはだいたい次の2種類:
- 成功を1、失敗を0で表す整数vector (狭義のロジスティック回帰)
- 1列目が成功回数、2列目が失敗回数の整数matrix

今回の場合、成功回数 `y` だけをformulaに入れると怒られる
```{r, logistic-error}
#| error: true
glm(y ~ x, df_logistic, family = binomial)
```
ので失敗回数もモデルに含むよう `response ~ x` とする。

(今回のように試行回数が10回固定じゃなくても使える、ということ)


---
## 🔰 共分散分析: GLM with 質的変数 + 量的変数

まずはweatherだけで分散分析、次にtemperatureを入れて共分散分析。

```{r, generate-df-ancova}
#| ref.label: "df-ancova"
#| echo: -1
#| cache.vars: []
```

---
## 🔰 交互作用


```{r, generate-df-interact}
#| ref.label: "df-interact"
#| echo: -1
#| cache.vars: []
```

---
## データはひとつ、モデルはたくさん

どう選ぶ？

1. メカニズム的に納得できるものを選ぶ
    - ポアソン過程の**カウント**ならポアソン分布、**間隔**ならガンマ分布
    - n回中k回のように**割合的なカウント**なら二項分布
1. データを可視化してみて、それっぽい形・性質のものを選ぶ
    - **左右対称のひと山**ならとりあえず正規分布
    - **負の値を取らない**ならガンマ分布
    - 直線的か、指数関数的か、頭打ちか、などなど

客観的な指標もほしい。<br>
モデルの尤もらしさといえば...


---
## <ruby>尤<rt>ゆう</rt>度</ruby> (likelihood)

**あるモデル$M$の下でそのデータ$D$が観察される確率**:<br>
$\text{Prob}(D \mid M)$

データ$D$を固定し、モデル$M$の関数とみなしたものが**尤度関数**:<br>
$L(M \mid D)$

モデルの構造も固定してパラメータ$\theta$だけ動かす場合はこう書く:<br>
$L(\theta \mid D)$ or $L(\theta)$

**対数尤度** $\log L$ の形にしたほうがいろいろ便利。

<hr>

各モデルで最適なパラメータを探して、比較:<br>
$\log L^* (M_1) \text{ vs. } \log L^* (M_2) \text{ vs. } \log L^* (M_3) \ldots$

```{r, llf}
broom::glance(fit)
```

---
## たしかに尤度はあてはまりの良さを表してそう

この場合は直線回帰よりもポアソン回帰が良さそう:

```{r, compare-loglik}
#| echo: false
#| fig.height: 5
#| fig.width: 9
models = setNames(nm = c("gaussian", "poisson")) |> purrr::map(~{
  glm(num_seeds ~ body_mass, family = .x, data = df_seeds)
})

x_breaks = c(0.5, 1.0, 1.5)
df_lm = tidyr::crossing(body_mass = x_breaks, num_seeds = seq(-5, 20, 0.1)) |>
  modelr::add_predictions(models[["gaussian"]], type = "response") |>
  dplyr::mutate(density = dnorm(num_seeds, pred, 1.4)) |>
  dplyr::filter(density > 1e-4)

p_pois = ggplot(df_seeds) + aes(body_mass, num_seeds) +
  ggridges::geom_vridgeline(data = df_lm, aes(width = density * 0.4, group = body_mass), linetype = 0, alpha = 0) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  scale_x_continuous(breaks = x_breaks) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())

label = sprintf("logLik = %.1f", broom::glance(models[["gaussian"]])$logLik)
p_lm = p_pois +
  labs(title = "gaussian, identity link") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = label, color = "#3366ff", size = 8) +
  stat_smooth(formula = y ~ x, method = lm, se = FALSE) +
  ggridges::geom_vridgeline(aes(width = density * 0.4, group = body_mass),
    data = df_lm, fill = "#56B4E9AA", linetype = 0)
# p_lm

df_ridges = tidyr::crossing(body_mass = x_breaks, num_seeds = seq_len(30L) - 1L) |>
  modelr::add_predictions(models[["poisson"]], type = "response") |>
  dplyr::mutate(density = dpois(num_seeds, pred)) |>
  dplyr::filter(density > 1e-4)
df_bars = df_ridges |> wtl::ridges2bars(num_seeds, density)

label = sprintf("logLik = %.1f", broom::glance(models[["poisson"]])$logLik)
p_poisson = p_pois +
  labs(title = "poisson, log link") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = label, color = "#3366ff", size = 8) +
  stat_smooth(formula = y ~ x, method = glm, method.args = list(family = poisson), se = FALSE) +
  ggridges::geom_vridgeline(aes(width = density * 0.5, group = body_mass),
    data = df_bars, fill = "#56B4E9AA", linetype = 0)
# p_poisson

cowplot::plot_grid(p_lm, p_poisson, nrow = 1L)
```

この調子で、より尤度の高いモデルを探していけばいいだろうか？

---
## あてはまりが良ければいいってもんでもない

過剰適合 / 過学習 / overfitting
: パラメータを増やせば**現データへの**適合度・尤度を高くできるが、<br>
  予測・理解の役には立たなくなる。

```{r, saturated-model}
#| echo: false
#| fig.height: 4
#| fig.width: 11
#| cache.vars: []
set.seed(19937)
n = 16L
true_coef = c(0.1, 0.2)
df_plant = tibble::tibble(
  x = runif(n, 7, 12.5),
  lambda = exp(true_coef[1] + true_coef[2] * x),
  y = rpois(n, lambda)
) |> tibble::rownames_to_column("id")

models = df_plant |> modelr::fit_with(glm, family = "poisson", modelr::formulas(~y,
  null = ~ 1,
  x = ~ x,
  saturated = ~ id
))
labels = setNames(sprintf("logLik = %.1f", purrr::map_dbl(models, logLik)), names(models))

p_plant = df_plant |>
  modelr::add_predictions(models$null, type = "response") |>
  ggplot() + aes(x, y) +
  geom_line(aes(y = pred), color = "#3366ff", linewidth = 2, alpha = 0.7) +
  geom_point(shape = 16, alpha = 0.6) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")

p_null = p_plant +
  labs(title = "null model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2,
    label = labels[["null"]], color = "#3366ff", size = 6)
p_x = p_plant %+% modelr::add_predictions(df_plant, models$x, type = "response") +
  labs(title = expression(y %~% beta[0] + beta[1] * x)) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2,
    label = labels[["x"]], color = "#3366ff", size = 6)
p_saturated = p_plant %+% modelr::add_predictions(df_plant, models$saturated, type = "response") +
  labs(title = "saturated model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2,
    label = labels[["saturated"]], color = "#3366ff", size = 6)

cowplot::plot_grid(p_null, p_x, p_saturated, nrow = 1L)
```

**帰無モデル**: 説明変数なし。切片のみ。<br>
**飽和モデル**: データ点の数 ≤ パラメータの数。“データ読み上げ”的モデル


---
## 無駄な説明変数を加えても尤度は上がる

ある植物が作る種の数 $y$ は個体のサイズ $x$ に応じて増える。<br>
観察時に着てた服の色 $x_2$ を追加すると尤度が上がる......?

```{r, many-models}
#| echo: false
#| fig.height: 7
#| fig.width: 7
#| cache.vars: ["df_plant", "models", "p_plant"]
set.seed(24601)
n = 120L
true_coef = c(1, 0.12, 0)
df_plant = tibble::tibble(
  x = runif(n, 7, 12.5),
  x2 = sample(c(FALSE, TRUE), n, replace = TRUE),
  lambda = exp(true_coef[1] + true_coef[2] * x + true_coef[3] * x2),
  y = rpois(n, lambda)
) |> tibble::rownames_to_column("id")

models = df_plant |> modelr::fit_with(glm, family = "poisson", modelr::formulas(~y,
  null = ~ 1,
  x = ~ x,
  x2 = ~ x2,
  both = ~ x + x2,
  saturated = ~ id
))
labels = setNames(sprintf("logLik = %.1f", purrr::map_dbl(models, logLik)), names(models))

p_plant = df_plant |>
  modelr::add_predictions(models$null, type = "response") |>
  ggplot() + aes(x, y) +
  geom_line(aes(y = pred), linewidth = 1.5, alpha = 0.6) +
  geom_point(shape = 16, alpha = 0.6) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")

p_null = p_plant +
  labs(title = "null model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["null"]], color = "#3366ff", size = 6)
p_x = p_plant %+% (df_plant |> modelr::add_predictions(models$x, type = "response")) +
  labs(title = expression(y %~% beta[0] + beta[1] * x)) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x"]], color = "#3366ff", size = 6)
p_x2 = p_plant %+% (df_plant |> modelr::add_predictions(models$x2, type = "response")) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x2"]], color = "#3366ff", size = 6)
p_both = p_plant %+% (df_plant |> modelr::add_predictions(models$both, type = "response")) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[1] * x + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["both"]], color = "#3366ff", size = 6)

cowplot::plot_grid(p_null, p_x, p_x2, p_both, nrow = 2L)
```



---
## AIC: 赤池情報量基準

<p>\[\begin{split}
\text{AIC} = -2 (\log L^* - k) = -2 \log L^* + 2k
\end{split}\]</p>

- **AICが小さいほど予測精度の良いモデル**。
    - 尤度は上げたい。
    - パラメータ数 $k$ が増えるとペナルティ。
- どのデータに対する当てはまりを目指すかという観点
    - 「手元のデータ」に対する対数尤度は $\log L^*$<br>
    - 「真のメカニズムから出てくる未来のデータ」に対する<br>
      平均対数尤度の推定量は $(\log L^* - k)$<br>
      (Kullback--Leibler情報量を使って導出するらしい)


```{r, print-llf}
#| ref.label: "llf"
```

???
https://www.slideshare.net/logics-of-blue/1-6aic


---
## 無駄な説明変数の追加でAIC増加

ある植物が作る種の数 $y$ は個体のサイズ $x$ に応じて増える。<br>
観察時に着てた服の色 $x_2$ を追加したモデルはAICが増加。

```{r, many-models-aic}
#| echo: false
#| fig.height: 7
#| fig.width: 7
labels = setNames(sprintf("AIC = %.1f", purrr::map_dbl(models, AIC)), names(models))
p_null = p_plant +
  labs(title = "null model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["null"]], color = "#3366ff", size = 6)
p_x = p_plant %+% (df_plant |> modelr::add_predictions(models$x, type = "response")) +
  labs(title = expression(y %~% beta[0] + beta[1] * x)) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x"]], color = "#3366ff", size = 6)
p_x2 = p_plant %+% (df_plant |> modelr::add_predictions(models$x2, type = "response")) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x2"]], color = "#3366ff", size = 6)
p_both = p_plant %+% (df_plant |> modelr::add_predictions(models$both, type = "response")) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[1] * x + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["both"]], color = "#3366ff", size = 6)
cowplot::plot_grid(p_null, p_x, p_x2, p_both, nrow = 2L)
```

---
## ほかの情報量基準

- $\text{BIC} = -2 \log L^* + k \log n$
    - パラメータ数 $k$ でペナルティを付けるのはAICと同じ。
    - データの観測数 $n$ に依存する点でAICと異なる。<br>
      感覚としては「AICはデータサイズによるペナルティが無い」
    - (周辺尤度の最大化という観点で導出するらしい)
- [WAIC](http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/waic2011.html),
  [WBIC](http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/wbic2012.html)
    - AIC, BICを一般化し、広く使えるようにしたもの。
    - 理想的な条件ではそれぞれAIC, BICとほぼ同じ。<br>
      そうじゃない場合(現実的には常に)こちらが優位。
    - WAICは予測の良さ、WBICは真のモデルへの近さ、を表す。


---
## モデル選択の心構え

「正しい」ものを選べるわけではない。<br>
予測・理解に useful なものを何らかの基準で選ぶだけ。

> All models are wrong, but some are useful. --- George E. P. Box

<figure>
<img src="../tokiomarine2021/math-model.drawio.svg" width="900"><br>
<figcaption><cite>「データ分析のための数理モデル入門」江崎貴裕 2020 より改変</cite></figcaption>
</figure>


---
## 現実的な注意点・悩みどころ

- 多重共線性(multicollinearity):
    - 説明変数同士が強い相関関係にある
- 変数変換:
    - 気安くやるべきじゃないけど、対数変換などしばしば有用
    - 割り算した値は危険
- 交互作用を入れると解釈が難しくなる。


---
## 一般化線形モデル座学まとめ

- 何はともあれ散布図を描く
- 適切な確率分布・リンク関数・説明変数を考える
- パラメータを最尤推定する
- 尤度は「手元のデータへのあてはまり」
- モデルを比較するときは情報量基準を参考にする



---
## penguinsデータセット

<a href="https://allisonhorst.github.io/palmerpenguins/">
<cite>https://allisonhorst.github.io/palmerpenguins/</cite><br>
<img src="/slides/image/rstats/lter_penguins.png" width="45%">
<img src="/slides/image/rstats/culmen_depth.png" width="45%">
</a>

```r
install.packages("palmerpenguins")
library(palmerpenguins)
penguins_colors = c(Adelie = "darkorange", Chinstrap = "purple", Gentoo = "cyan4")
print(penguins)
```
```{r, penguins}
#| include: false
withr::local_package("palmerpenguins")
penguins_colors = c(Adelie = "darkorange", Chinstrap = "purple", Gentoo = "cyan4")
```

---
## penguinsデータセット

<a href="https://allisonhorst.github.io/palmerpenguins/">
<cite>https://allisonhorst.github.io/palmerpenguins/</cite><br>
<img src="/slides/image/rstats/lter_penguins.png" width="45%">
<img src="/slides/image/rstats/culmen_depth.png" width="45%">
</a>

```{r, penguins-print}
#| echo: false
print(penguins)
```

---
## 欠損値のある行を取り除いておく

性別はとりあえず使わないので、体長関連だけでも。

```{r, penguins-dropna}
penguins |> dplyr::filter(dplyr::if_any(everything(), is.na))
penguins_dropna = penguins |> tidyr::drop_na(body_mass_g)
dim(penguins_dropna)
```

---
## 🔰 penguinsでGLMの練習

次の課題を解いてみよう。<br>
(次ページ以降に解答。まずは自力で。)

1. `body_mass_g` を横軸、 `flipper_length_mm` を縦軸に、まず作図。
1. 単回帰して、切片と傾きを求める。そして作図。
1. `species` で色分けして作図。
1. `species` も説明変数に加えて重回帰し、切片と傾きを求める。そして作図。
1. 余裕があれば、クチバシの長さと深さを縦横軸にして同様の解析。


---
## 単回帰の練習: 1. まず作図

どうやら、重いペンギンほど翼長も長い。

```{r, penguins-weight}
#| fig.height: 4.5
#| fig.width: 4.5
p_penweight = ggplot(penguins_dropna) +
  aes(body_mass_g, flipper_length_mm) +
  geom_point(shape = 16, alpha = 0.66) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())
p_penweight
```


---
## 単回帰の練習: 2. モデル作成、フィッティング

とりあえずデフォルトの正規分布・恒等リンク。
$y = 136.7 + 0.0153 x$

```{r, penguins-fit1}
fit1 = glm(flipper_length_mm ~ body_mass_g, data = penguins_dropna)
broom::tidy(fit1)
broom::glance(fit1)
```

---
## 単回帰の練習: 3. フィッティング結果を作図

結果とデータから予測値を作って回帰線を引く。

```{r, penguins-weight-glm}
#| fig.height: 5
#| fig.width: 5
added1 = modelr::add_predictions(penguins_dropna, fit1, type = "response")
p1 = p_penweight +
  geom_line(aes(y = pred), data = added1, linewidth = 1, color = "#3366ff")
p1
```

---
## 重回帰の練習: 1. まず作図

種によって色分けしてみると、傾向の違いが見える。

```{r, penguins-weight-sp}
#| fig.height: 5
#| fig.width: 7
p_penweight_color = p_penweight + aes(color = species) +
  scale_color_manual(values = penguins_colors)
p_penweight_color
```


---
## 重回帰の練習: 2. モデル作成、フィッティング

Adelieを基準に、ChinstrapとGentooはそれより長め。<br>
体重の効果は単回帰のとき(0.0153)より小さい。

```{r, penguins-fit2}
fit2 = glm(flipper_length_mm ~ body_mass_g + species, data = penguins_dropna)
broom::tidy(fit2)
broom::glance(fit2)
```

---
## 重回帰の練習: 3. フィッティング結果を作図

```{r, penguins-weight-sp-glm}
#| fig.height: 5
#| fig.width: 7
added2 = modelr::add_predictions(penguins_dropna, fit2, type = "response")
p2 = p_penweight_color +
  geom_line(aes(y = pred), data = added2, linewidth = 1)
p2
```

**傾き**も種によって違うかも。**交互作用**を入れてみたい。


---
## 交互作用の練習: モデル作成、フィッティング

Adelieを基準に、Chinstrapの傾きが結構違う。<br>
切片の違いは解釈しにくくなった。

```{r, penguins-fit3}
fit3 = glm(flipper_length_mm ~ body_mass_g * species, data = penguins_dropna)
broom::tidy(fit3)
broom::glance(fit3)
```

---
## 交互作用の練習: フィッティング結果を作図

```{r, penguins-interaction}
#| fig.height: 5
#| fig.width: 7
added3 = modelr::add_predictions(penguins_dropna, fit3, type = "response")
p3 = p_penweight_color +
  geom_line(aes(y = pred), data = added3, linewidth = 1)
p3
```

---
## ここまでの3つのモデルでどれがいいか？

AICで選ぶなら交互作用入り重回帰が良さそう。

```{r, penguins-aic}
#| fig.height: 4
#| fig.width: 11
labels = sprintf("AIC = %.1f", AIC(fit1, fit2, fit3)$AIC)
cowplot::plot_grid(p1 + labs(title = labels[1]),
                   p2 + labs(title = labels[2]) + theme(legend.position = "none"),
                   p3 + labs(title = labels[3]) + theme(legend.position = "none"), nrow = 1L)
```


---
## 余裕があったら追加の練習

🔰クチバシの長さと深さで同じ解析をやってみよう。

```{r, penguins-bill}
#| echo: false
#| fig.height: 4
#| fig.width: 11
#| cache.vars: [p_bill]
p_bill = penguins_dropna |>
  ggplot() + aes(bill_length_mm, bill_depth_mm) +
  geom_point(shape = 16, alpha = 0.66) +
  scale_color_manual(values = penguins_colors) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())

fit1 = glm(bill_depth_mm ~ bill_length_mm, data = penguins_dropna)
fit2 = glm(bill_depth_mm ~ bill_length_mm + species, data = penguins_dropna)
fit3 = glm(bill_depth_mm ~ bill_length_mm + species + bill_length_mm:species, data = penguins_dropna)
d1 = modelr::add_predictions(penguins_dropna, fit1, type = "response")
d2 = modelr::add_predictions(penguins_dropna, fit2, type = "response")
d3 = modelr::add_predictions(penguins_dropna, fit3, type = "response")
p1 = p_bill + geom_line(aes(y = pred), d1, linewidth = 1, color = "#3366ff")
p2 = p_bill + aes(color = species) + geom_line(aes(y = pred), d2, linewidth = 1)
p3 = p_bill + aes(color = species) + geom_line(aes(y = pred), d3, linewidth = 1)
labels = sprintf("AIC = %.1f", AIC(fit1, fit2, fit3)$AIC)
cowplot::plot_grid(p1 + labs(title = labels[1]),
                   p2 + labs(title = labels[2]) + theme(legend.position = "none"),
                   p3 + labs(title = labels[3]) + theme(legend.position = "none"), nrow = 1L)
```

```{r, penguins-multiple}
#| echo: false
#| include: false
fit4 = glm(bill_depth_mm ~ bill_length_mm + sex, data = penguins)
broom::tidy(fit4)
broom::glance(fit4)
added4 = modelr::add_predictions(penguins, fit4)
p_bill + geom_line(aes(y = pred, color = sex), data = added4, linewidth = 1) +
  scale_color_discrete()
```


---
## 🔰 4日目の課題2: 架空データでGLM

宿屋の主人が宿泊客の冒険者に話を聞いてデータを集めた。<br>
冒険者の**レベル**、**職業**、通ってきた**ルート**、魔物との**遭遇回数**。<br>
これらの変数の間に、なんらかの関係は見られるだろうか？

[adventure.tsv](adventure.tsv)

```{r, adventure-data}
#| echo: false
set.seed(19937)
samplesize = 1000L
b0 = 0.7
b1 = 0.01
b2 = -0.5
b3 = 0.02
b12 = 0.03

dfraw = tibble::tibble(
  level = sample.int(50L, samplesize, replace = TRUE),
  is_mage = rbinom(samplesize, 1, 0.4),
  via_cave = rbinom(samplesize, 1, ifelse(is_mage, 0.5, 0.3)),
  lambda = exp(b0 + b1 * level + b2 * is_mage + b3 * via_cave + b12 * level * is_mage),
  encounter = rpois(samplesize, lambda)
)

dfout = dfraw |> dplyr::mutate(
    job = ifelse(is_mage, "mage", "fighter"),
    route = ifelse(via_cave, "cave", "field"),
    .before = encounter
  ) |>
  dplyr::select(!lambda & !is_mage & !via_cave)

readr::write_tsv(dfout, "../adventure.tsv") |> print()
```

```{r, adventure-analysis}
#| include: false
fit = glm(encounter ~ level + job + route + level:job, data = dfout, family = poisson)
summary(fit)
df_pred = modelr::add_predictions(dfout, fit, type = "response")

p_enc = ggplot(df_pred) +
  aes(level, encounter, color = job) +
  geom_point() +
  geom_line(aes(y = pred)) +
  facet_wrap(vars(route))

p_route = ggplot(df_pred) +
  aes(job, fill = job) +
  geom_bar() +
  facet_wrap(vars(route))
```

---
## 🔰 最終課題: オープンデータを拾って解析しよう。

- [e-Stat](https://www.e-stat.go.jp/): 政府統計の総合窓口
- [data.go.jp データカタログサイト](https://www.data.go.jp/data/dataset?res_format=CSV): 中央省庁
- [BODIKオープンデータカタログサイト](https://odcs.bodik.jp/): 地方自治体
- [気象庁](https://www.data.jma.go.jp/gmd/risk/obsdl/index.php)
- [DATA.GOV](https://www.data.gov/): U.S. Government’s open data
- ほか、なんでも興味のあるデータ

発表・レポートの条件
: 最低1枚の図と、そこに至る前処理＋可視化のコード。
: グラフから読み取れることを一言。
: GLMで変数間の関係を考察できるとなおよし。

週明けに班内で話し合ってベスト2を選び、全体に向けて発表。


---
## 一般化線形モデルまとめ

- 何はともあれ作図して俯瞰
- GLMは統計モデリングの考え方の根幹
    - 確率分布・リンク関数・説明変数
    - 尤度・最尤法によるパラメータ推定
    - 情報量基準などによるモデル選択












---
## 今回のお品書き

✅ データ入力の心得
- 野良データを読み込む難しさ
- 自分が一次データを作るときに気をつけること

⬜ データ解釈の心得 (理論や手法の詳細には触れない)
- 誤差 (バイアス、ばらつき)
- 統計的仮説検定
- 因果関係と相関関係



---
## Garbage in, garbage out

どんなに素晴らしい統計処理をしようとも、データがゴミなら結論もゴミ。

<figure>
<img src="../tohoku2022r/image/garbage-in-garbage-out.drawio.svg" width="1200">
</figure>

- 目的に応じてちゃんとデータを採ることが重要！
- 何がデータを悪くしてしまうのか、見ていこう


---
## 観測には必ず何らかの歪みがある

現象・母集団の性質を知りたいけど、そのものは測れない。<br>
: 質的な問題 → 測れる側面だけ測るしかない
: 量的な問題 → 一部の標本を抽出して測るしかない

<figure>
<img src="../tohoku2022r/image/math-model-biased.drawio.svg" width="1080"><br>
<figcaption><cite>「データ分析のための数理モデル入門」江崎貴裕 2020 より改変</cite></figcaption>
</figure>


---
## 2種類の誤差

系統的誤差 systematic error / バイアス bias
: 一定の傾向をもって生じる誤差。原因を特定して対処。

偶然誤差 random error
: 偶然生じてしまうばらつき。計測を繰り返して対処。

e.g., 服を着たまま体重測定を10回繰り返す → どれも違う値

<figure>
<img src="../tohoku2022r/image/error-random-systematic.drawio.svg" width="640"><br>
<figcaption><cite>「分析者のためのデータ解釈学入門」江崎貴裕 2020 より改変</cite></figcaption>
</figure>


---
## 選択バイアス

<figure style="position: absolute; top: 40px; right: 40px;"><a href="https://en.wikipedia.org/wiki/Survivorship_bias">
<img src="/slides/image/free/Survivorship-bias.svg" width="400"><br>
<figcaption class="url">https://en.wikipedia.org/wiki/Survivorship_bias</figcaption>
</a></figure>

データの採れかたが結果に傾向をもたらしてしまう。

生存(者)バイアス
: 生き残ったものだけがデータとして観測される。
: 帰還した多数の戦闘機✈️✈️の損傷の分布(右図)。<br>
  被弾 → **<span style="color: #bb0000;">生還 → 観測される</span>**<br>
  被弾 → **墜落 → 観測されない**<br>
  損傷が観察されなかった場所をむしろ補強すべき。

サンプリングバイアス
: サンプル対象を決めた時点で偏っている。
: 📞電話でアンケート調査 →
  電話機を持てる裕福な人、その時間に通話できる人、
  知らない番号からでも出る人、調査している新聞社に共感している人。

🔰 生物学研究における選択バイアスにはどういうものがありそう？

---
## 測定基準に関するバイアスの例

基準が揃っていない
: 国民1人あたりの弁護士数を国際比較すると日本が少ない。<br>
  諸外国では税理士や司法書士を弁護士に含めて集計している。

基準が時間とともに変化
: 自閉症と診断された児童の数は**見かけ上**年々増えている。<br>
  診断基準の変更、病気の知名度向上が主な原因。<br>
  (親の高齢化の寄与を示唆する研究もあるらしい)

---
## データの扱いに起因するバイアス

チェリー・ピッキング 🍒
: 自分の仮説と矛盾する情報を無視し、都合のいい情報だけ集めること。
: [研究活動における3つの不正](https://www.mext.go.jp/b_menu/shingi/gijyutu/gijyutu12/houkoku/attach/1334660.htm)のうち**改竄**にあたる。

確証バイアス
: 無意識にチェリー・ピッキングしてしまう心理的傾向。
: 悪気なくやってしまわないよう、自覚を持つことが大事。

表に数字、裏にアルファベットが書いてあるカード。<br>
「偶数の裏は必ずA、という法則があるか？」<br>
これを確かめるには次の4枚のうちどれを裏返せばいいか？

```{r, four-cards}
#| echo: false
#| fig.width: 6
#| fig.height: 2
n = 20
g = (1 + sqrt(5)) / 2
card = tibble::tibble(x = c(0, 1, 1, 0), y = c(0, 0, g, g))
dplyr::bind_rows(
  card |> dplyr::mutate(id = 0),
  card |> dplyr::mutate(id = 1, x = x + 1.1),
  card |> dplyr::mutate(id = 2, x = x + 2.2),
  card |> dplyr::mutate(id = 3, x = x + 3.3)
) |>
  ggplot() + aes(x, y) +
  geom_polygon(aes(group = id), fill = NA, color = "#333333") +
  scale_fill_identity() +
  annotate("text", x = c(0.5, 1.6, 2.7, 3.8), y = g / 2, label = c("1", "2", "A", "B"), size = 20) +
  coord_fixed() +
  theme_void()
```

???
論理学的な正解は2, B。



---
## たまたま生じたパターンにも意味を見出しがち

前後即因果の誤謬
: たまたま前後で生じた現象に因果関係があると思いがち

ギャンブラーの誤謬
: N回も連続で裏が出たら、次は表が出やすいと思いがち

```{r, random-pattern}
#| echo: false
#| fig.width: 4
#| fig.height: 4
set.seed(19937)
n = 20
tibble::tibble(x = runif(n), y = runif(n)) |>
  ggplot() + aes(x, y) +
  geom_point(shape = 16, size = 3, alpha = 0.66) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1), expand = FALSE) +
  theme_minimal()
```

---
## データ解釈に関わる認知バイアス

利用可能性ヒューリスティック
: 思い浮かびやすいものほど過大評価しがち
: e.g., 1文字目がkの単語、3文字目がkの単語、多いのはどっち？

代表性ヒューリスティック
: ステレオタイプを過大評価しがち
: e.g., リンダは31歳の独身女性。率直で聡明。大学では哲学専攻。
  人種差別や社会正義などに深く関心を持ち、学生時代には反核デモにも参加。<br>
  現在のリンダは次のどちらである可能性が高いか？
  1. リンダは銀行員として働いている。
  1. リンダは銀行員として働きながら、フェミニスト運動に参加している。

認知バイアス・誤謬はほかにもいろいろある。<br>
データ解析とは関係なく、知っておくのは悪くない。

???

https://lambtani.hatenablog.jp/entry/2017/05/18/032108



---
## 偶然誤差によるばらつき方 = 確率分布

次回 [7. 統計モデリング1](7-distribution.html) でやります。

背後にあるメカニズムによっていろんな形になる。

<iframe width="600" height="450" src="./7-distribution.html#/33"></iframe>


---
## 分布を特徴づける代表値 central tendency

<div class="column-container">
  <div class="column" style="flex-shrink: 1.6;">

平均値 mean
: 和を観察数で割る

中央値 median
: 順に並べて真ん中

最頻値 mode
: 最も頻度が高い値

  </div>
  <div class="column">
  <a href="https://www.mhlw.go.jp/toukei/list/20-21.html">
  <img src="../tohoku2022r/image/hist-income-japan-2019.png" width="100%" style="">
  <figcaption><cite>所得金額階級別世帯数の頻度分布 厚生労働省 国民生活基礎調査 2019</cite></figcaption>
  </a>
  </div>
</div>

目的や状況に応じて使い分けよう。

外れ値に対する応答
: もし総資産額20兆円の大富豪が鳥取県に引っ越してきたら<br>
  → 県民の**平均**資産は4000万円上昇。**中央値**・**最頻値**はほぼそのまま。

---
## ばらつきを捉える記述統計量

分散 variance
: 平均値からの差の自乗の平均。 $\frac 1 n \sum _i ^n (X_i - \bar X)^2$
: これの平方根が**標準偏差 (standard deviation)**。

Percentile, Quantile (四分位)
: 小さい順にならべて上位何%にあるか。
: 中央値 = 50th percentile = 第二四分位(Q2)

```{r, quantile}
#| fig.width: 10
#| fig.height: 4
#| echo: false
set.seed(24601)
df = tibble::tibble(x = rnorm(200)) |>
  dplyr::filter(abs(x) < 2) |>
  dplyr::mutate(x = x + 3)
probs = c(min = 0, Q1 = 0.25, Q2 = 0.5, Q3 = 0.75, max = 1)
dfq = df |>
  dplyr::reframe(x = quantile(x, probs)) |>
  dplyr::mutate(name = names(probs), percent = sprintf("%d%%", 100 * probs))

p_hist = ggplot(df) + aes(x) +
  geom_histogram(bins = 30) +
  theme_void()
built = ggplot_build(p_hist)
bdata = built$data[[1]]
xlim = bdata |> dplyr::select(xmin, xmax) |> range()
p_box = ggplot(df) + aes(x) +
  geom_boxplot() +
  geom_rug(sides = "t", length = unit(0.06, "npc"), alpha = 0.66) +
  geom_point(data = dfq, aes(y = -0.6), shape = 17, size = 4) +
  geom_text(data = dfq, aes(y = -0.75, label = name), size = 5) +
  geom_text(data = dfq, aes(y = -0.95, label = percent), size = 5) +
  coord_cartesian(xlim = xlim, ylim = c(-1.2, 0.5)) +
  theme_void()
cowplot::plot_grid(p_hist, p_box, ncol = 1L)
```



---
## 記述統計量に頼りすぎず分布を可視化する

同じデータでも見せ方で印象・情報量が変わる。

```{r, visualize-distribution}
#| echo: false
#| fig.width: 9
#| fig.height: 6
df = mpg |> dplyr::mutate(y = hwy)
df_mean = df |> dplyr::summarize(y = mean(y))
p0 = ggplot(df) + aes(y = y) +
  theme_classic() +
  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank())

ylim = c(0, max(df$y))
coord_y = coord_cartesian(ylim = ylim)
coord_one = coord_cartesian(ylim = ylim, xlim = c(-1, 1))

pcol = p0 + aes(x = 0) +
  geom_col(data = df_mean, fill = "#999999") +
  stat_summary(fun.data = wtl::mean_sd, geom = "linerange")

set.seed(1)
cowplot::plot_grid(nrow = 2L,
  pcol + coord_one,
  p0 + geom_boxplot() + coord_one,
  p0 + geom_density(fill = "#999999", color = NA) + coord_y,
  p0 + geom_histogram(fill = "#999999", bins = 30L) + coord_y,
  p0 + geom_jitter(aes(x = 0), height = 0, shape = 16, alpha = 0.3, stroke = 0) + coord_one,
  p0 + geom_violin(aes(x = 0), fill = "#999999", color = NA) + coord_one,
  p0 + geom_dotplot(aes(x = 0), binaxis = "y", binwidth = 1, stackratio = 0.8,
    stroke = 0, alpha = 0.66, stackdir = "center") + coord_y,
  p0 + geom_dotplot(aes(x = 0), binaxis = "y", binwidth = 1, stackratio = 0.8,
    stroke = 0, alpha = 0.66) + coord_y
)
```


---
## 2つの量の関係性: 大小の比較

ばらつきの度合いも加味して判断する。

<div class="column-container" style="padding-left: 10px;">
<div class="column" style="flex-shrink: 1.1;">
観測値1つずつ。<br>
たまたまかも。
</div>
<div class="column" style="flex-shrink: 1;">
ばらつき大きい。<br>
Bが高いのもたまたま?
</div>
<div class="column" style="flex-shrink: 1;">
ばらつき小さい。<br>
AとBには差がありそう。
</div>
</div>

```{r, comparison}
#| fig.width: 11
#| fig.height: 4
#| echo: false
set.seed(19937)
n = 20
df1 = tibble::tibble(x = c("A", "B"), y = c(42, 51))
df2 = dplyr::bind_rows(
  tibble::tibble(x = "A", y = runif(n, 42 - 20, 42 + 20)),
  tibble::tibble(x = "B", y = runif(n, 51 - 20, 51 + 20)))
df3 = dplyr::bind_rows(
  tibble::tibble(x = "A", y = rnorm(n, 42, 1)),
  tibble::tibble(x = "B", y = rnorm(n, 51, 1)))
.lim = c(0, max(df2$y, df3$y))
.th = list(coord_cartesian(ylim = .lim),
  theme_classic(base_size = 20),
  theme(legend.position = "none", axis.title = element_blank()))

p1 = ggplot(df1) + aes(x, y, color = x) +
  geom_point(shape = 16, size = 5) +
  .th
p2 = ggplot(df2) + aes(x, y, color = x) +
  geom_jitter(height = 0, width = 0.2, shape = 16, size = 4, alpha = 0.66) +
  .th
cowplot::plot_grid(p1, p2, p2 %+% df3, nrow = 1)
```

「こんなことがたまたま起こる確率はすごく低いです！」<br>
をちゃんと示す手続きが**統計的仮説検定**→


---
## 目的やデータに応じて正しい検定を選ぶ

例えば「2つの量の大小を比較したい」だけでもいくつかある。<br>
今回は選定には深入りせず、検定の考え方だけ触れておく。

<figure><a href="https://comicalcommet.github.io/r-training-2023/">
<img src="/slides/image/rstats/comicalcommet-test-chart.jpg" width="90%">
<figcaption><cite>
「Rを用いたデータ解析の基礎と応用」石川由希さん@名古屋大
</cite></figcaption>
</a></figure>


---
## 統計的仮説検定の考え方

🎲サイコロを**10回**振ったら**9回**も6の目が出たぞ。イカサマじゃないか？

帰無仮説
: 6の目が出る確率 = 1/6。普通のサイコロです。

対立仮説
: 6の目が出る確率 ≠ 1/6。イカサマです。

<div>\[\begin{split}
p = \binom {10} {9} \times {\frac 1 6} ^ {9} \times {\frac 5 6} ^ 1 + {\frac 1 6} ^ {10}
  = 8.43 \times 10 ^ {-7}
\end{split}\]</div>

1. 帰無仮説のもとで今回のデータ以上に極端な値が得られる確率*p*を計算
1. この ***p*-value** が**有意水準(危険率)**$\alpha$より低い場合、帰無仮説を棄却<br>
   (大概 $\alpha$ = 0.05 とか 0.01 とか。予め決めておく。)
1. 対立仮説を採択。6の目の出る確率は1/6と**有意に異なる**。

```{r, test-dice-hidden}
#| include: false
n = 10
k = 9
p = 1 / 6
pbinom(k - 1, n, p, lower.tail = FALSE)
x = seq(k, n)
stopifnot(all.equal(dbinom(x, n, p), choose(n, x) * p ** x * (1 - p) ** (n - x)))
stopifnot(all.equal(sum(dbinom(x, n, p)), pbinom(k - 1, n, p, lower.tail = FALSE)))
```

---
## 統計的仮説検定の考え方

🎲サイコロを**12回**振ったら**4回**も6の目が出たぞ。<br>
これも1/6よりは高いけど、イカサマのダイスかな？

<span style="color: #990000;">帰無仮説のもとで12回中4回以上6が出る確率: $p = 0.125 > \alpha$</span><br>

帰無仮説を棄却できず。6の目の出る確率が1/6と**有意に異なるとは言えない**。

```{r, test-dice}
#| echo: false
#| fig.height: 4
#| fig.width: 6
X = 4L
n = 12L
tibble::tibble(x = seq.int(0L, n), Probability = dbinom(x, n, 1 / 6)) |>
  dplyr::mutate(color = ifelse(x >= X, "#990000", "#666666")) |>
  ggplot() + aes(x, Probability, fill = color) +
  geom_col() +
  scale_fill_identity() +
  theme_classic(base_size = 20)
# pbinom(X - 1L, n, 1 / 6, lower.tail = FALSE)
# sum(dbinom(seq(X, n), n, 1 / 6))
```

平たく言えば「普通のサイコロでもそんなに珍しい出目じゃない」


---
## 多重検定で偽陽性のリスクが上昇

危険率$\alpha=0.05$の検定は、最大5%の確率で誤って帰無仮説を棄却(**偽陽性**)。<br>
同様の検定を10回やると、そのうち少なくとも1つで偽陽性になる確率は<br>
$1 - (1 - 0.05)^{10} \approx 0.40$ まで上昇。<br>

```{r, multiple-tests}
#| fig.width: 11
#| fig.height: 4.2
#| echo: false
# 1 - (1 - 0.05) ** 10
set.seed(1279)
nsam = 10L
nrep = 20L
y = split(rnorm(nsam * nrep * 2), ceiling(seq_len(nsam * nrep * 2) / nsam))
df = tidyr::crossing(repl = seq_len(nrep), x = c("A", "B")) |>
  dplyr::mutate(y = y) |>
  tidyr::unnest(y)
dfp = df |>
  tidyr::nest(data = !repl) |>
  dplyr::mutate(p = purrr::map_dbl(data, ~ t.test(y ~ x, data = .x)$p.value)) |>
  dplyr::select(!data) |>
  dplyr::mutate(x = 1.5, y = Inf, label = ifelse(p < 0.05, sprintf("p = %.3f", p), ""))
df |>
  ggplot() + aes(x, y) +
  geom_boxplot(aes(fill = x)) +
  geom_text(data = dfp, aes(label = label), hjust = 0.5, vjust = 1.2) +
  facet_wrap(vars(repl), nrow = 2L) +
  theme_bw() +
  theme(legend.position = "none")
```

多重比較補正
: 検定を繰り返したぶんだけ基準を厳しくして偽陽性率を抑えること。
: Bonferroni法、Holm法、Benjamini and Hochberg法など。


---
## 広くて深い統計解析の世界

> データの荒波を泳ぎきってもどこにも「究極の真実」などありはしないのだ。
> 統計学はそのときその場かぎりでの「最良の結論」を導くための方便に過ぎないのだ。
> ------ [三中信宏「統計思考の世界」](https://amzn.to/3urpls1)

<figure style="margin-top: -1em;">
<a href="http://leeswijzer.org/R/R-top.html">
<img src="/slides/image/rstats/Mandala2004-small.jpg" width="80%">
<figcaption><cite>「大統計曼荼羅」三中信宏</cite></figcaption>
</a>
</figure>




<!--  -->


---
## 2つの量の関係性: 相関関係と因果関係

<style>
.spurious {color: #fde725;}
.correlation {color: #35B779;}
.causality-wrong {color: #31688e;}
.causality {color: #440154;}
</style>

<div class="column-container">
<div class="column" style="flex-shrink: 1;">

<span class="correlation">相関関係</span>
: ある値が大きいほど、別の値も大きいor小さい。
: e.g., 数学の成績と物理の成績。

<span class="causality">因果関係</span>
: ある事象が別の事象に影響を与える。
: e.g., 1時間勉強するごとに成績が3点伸びる。

</div>
<div class="column" style="flex-shrink: 1.8;">

```{r, causal-relationship}
#| echo: false
#| fig.width: 3.2
#| fig.height: 3.2
set.seed(19937)
n = 40L
tibble::tibble(
  study_time = runif(n, 0, 40),
  score = pmin(rnorm(n, 3 * study_time, 10), 100)) |>
  ggplot() +
  aes(study_time, score) +
  geom_point(shape = 16, alpha = 0.66, size = 3) +
  stat_smooth(method = lm, formula = y ~ x + 0, se = FALSE, color = "#440154", size = 2, alpha = 0.7) +
  theme_classic(base_size = 20) +
  theme(axis.text = element_blank(), axis.ticks = element_blank())
```

</div>
</div>


<hr>

- <span class="causality">因果関係</span>があれば<span class="correlation">相関関係</span>として表れる
- **<span class="correlation">相関関係</span>があるからといって<span class="causality">因果関係</span>もあるとは限らない**→


---
## 因果・相関を見誤るパターン1: 交絡因子

**誤:** 🍦アイスの売り上げが増えるほど🍺ビールの売り上げが増える。

**正:** 🍦アイスも🍺ビールも**気温が高いほど**売れる。

<div class="column-container">
  <div class="column" style="flex-shrink: 1.2;">

```{r, confounding-factor}
#| echo: false
#| fig.width: 4
#| fig.height: 4
set.seed(19937)
n = 40L
tibble::tibble(
  temperature = runif(n, 0, 40),
  beer_sales = rpois(n, temperature * 1.3),
  icecream_sales = rpois(n, temperature)) |>
  ggplot() +
  aes(icecream_sales, beer_sales) +
  geom_point(shape = 16, alpha = 0.66, size = 3) +
  stat_smooth(method = lm, formula = y ~ x, se = FALSE, color = "#358779", size = 2, alpha = 0.7) +
  theme_classic(base_size = 20) +
  theme(axis.text = element_blank(), axis.ticks = element_blank())
```

</div>
<div class="column" style="flex-shrink: 1;">

<figure>
<img src="../tohoku2022r/image/hermeneutics-4-1-3.drawio.svg" width="750">
</figure>

</div>
</div>


---
## 因果・相関を見誤るパターン2: 逆の因果関係

**誤:** <span class="causality-wrong">警察官が多いほど犯罪が増える。</span>

**正:** <span class="causality">犯罪が多いから警察官が多く配備される。</span>

<br>

## 因果・相関を見誤るパターン3: 選択バイアス

(x + y) が一定の幅に収まるようなペアだけを集めてしまうとか

```{r, spurious-correlation-selection-bias}
#| echo: false
#| fig.width: 4
#| fig.height: 4
set.seed(19937)
n = 256
tibble::tibble(x = runif(n), y = runif(n)) |>
  dplyr::mutate(sampled = abs(x + y - 1) < 0.2) |>
  dplyr::mutate(color = ifelse(sampled, "#fde725", "#aaaaaa")) |>
  ggplot() +
  aes(x, y) +
  geom_point(aes(color = color), shape = 16, size = 3, alpha = 1) +
  scale_color_identity() +
  coord_fixed() +
  theme_classic(base_size = 20) +
  theme(axis.text = element_blank(), axis.ticks = element_blank())
```


---
## 因果・相関を見誤るパターン4: 外れ値・グループ構造

**少数の外れ値**や**グループ構造**によって相関係数rが跳ね上がりうる。

```{r, correlation-lies}
#| echo: false
#| fig.width: 11
#| fig.height: 3.6
set.seed(19937)
n = 32
df = tibble::tibble(x = rnorm(n), y = rnorm(n))
df_outlier = df |> dplyr::bind_rows(data.frame(x = 10, y = 10))
df_subgroups = df |> dplyr::bind_rows(df + 8)
.lim = range(df_outlier, df_subgroups)

.make_label_cor = function(df) {
  r = cor(df[["x"]], df[["y"]])
  sprintf("r = %.2f", r)
}
# .make_label_cor(df_usbb)

.plot_cor = function(df) {
  ggplot(df) + aes(x, y) +
    geom_point(shape = 16, size = 3, alpha = 0.66) +
    stat_smooth(method = lm, formula = y ~ x, se = FALSE, size = 3, color = "#fde725") +
    annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, size = 8, label = .make_label_cor(df)) +
    coord_fixed(xlim = .lim, ylim = .lim) +
    theme_classic(base_size = 20) +
    theme(axis.text = element_blank(), axis.ticks = element_blank())
}

cowplot::plot_grid(
  .plot_cor(df),
  .plot_cor(df_outlier),
  .plot_cor(df_subgroups),
  nrow = 1L
)
```


---
## 因果・相関を見誤るパターン5: 偶然

ニコラス・ケイジの映画出演が増えるほど溺死する人が増える？

<figure>
<img src="../tohoku2022r/image/nicholas-cage-drowned.svg" width="1200">
<figcaption><cite>
<a href="https://www.tylervigen.com/spurious-correlations">https://www.tylervigen.com/spurious-correlations</a>
</cite></figcaption>
</figure>

このウェブサイトにはジョークとしてこうした例が多数集められている↑


---
## 変数間の関係性まとめ

<figure>
<img src="../tohoku2022r/image/hermeneutics-4-1-5.drawio.svg" width="1200">
<figcaption><cite>
<a href="https://amzn.to/3uznzCK">「分析者のためのデータ解釈学入門」江崎貴裕 2020</a>より改変
</cite></figcaption>
</figure>

<hr>

- **<span class="correlation">相関関係</span>があるからといって<span class="causality">因果関係</span>もあるとは限らない**。
- <span class="causality-wrong">逆の因果関係</span>や<span class="spurious">見せかけの相関</span>にも要注意。

🔰 これら4つの関係性に該当する事例をそれぞれ1つ以上探してみよう。

???
16--17世紀、怪我をしたら武器に軟膏を塗ると早く治るという迷信。
質の低い軟膏を傷口に塗らないことがプラスに働いたというのが真相。



---
## 🔰 3日目の課題2

書籍、ニュースサイト、ブログ記事などを検索し、<br>
**良くないデータ解析の例**を探して**改善案**を挙げよう。

1. データの集め方(サンプルの選び方)が偏っていて良くない例
1. データの解釈(相関や因果)を誤っている例
1. 図の描き方で嘘をついている例 (誇張、印象操作)
1. 生データを公開してくれるのはいいけど形式が汚くて使いにくい例

それぞれの例について、下記の項目を報告してください:

- 出典。URLや書籍名。
- データの概要、書き手の主張。
- ここが良くないので、こう改善すべき。












---
## 今日の残り時間

- 班やTAに相談し、消化しきれなかった部分をなるべく解消する。
- まずは個人で課題1と2に取り組み、ある程度できたら班内で見せ合う。
- 最終課題に向けて話し合ったり。


---
## 参考文献

- [データ解析のための統計モデリング入門](https://amzn.to/33suMIZ) 久保拓弥 2012
- [データ分析のための数理モデル入門](https://amzn.to/3uCxTKo) 江崎貴裕 2020
- [分析者のためのデータ解釈学入門](https://amzn.to/3uznzCK) 江崎貴裕 2020
- [統計学を哲学する](https://amzn.to/3ty80Kv) 大塚淳 2020
- [科学とモデル---シミュレーションの哲学 入門](https://amzn.to/2Q0f6JQ) Michael Weisberg 2017<br>
  (原著: [Simulation and Similarity](https://amzn.to/3bdvhuI) 2013)

`r .meta$next_link`
