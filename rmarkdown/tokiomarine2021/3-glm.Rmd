+++
url = "tokiomarine2021/3-glm.html"
title = "3. 一般化線形モデル、混合モデル — 統計モデリング概論 DSHC 2021"
linktitle = "一般化線形モデル、混合モデル"
date = 2021-06-30T13:00:00+09:00
type = "reveal"
draft = false
+++

<link rel="stylesheet" href="style.css">

# [統計モデリング概論 DSHC 2021](.)

<div class="author">
岩嵜 航 (Watal M. Iwasaki, PhD)
</div>

<div class="affiliation">
東北大学 生命科学研究科 進化ゲノミクス分野 特任助教<br>
(Graduate School of Life Sciences, Tohoku University)
</div>

<ol>
<li><a href="1-introduction.html">導入</a>
<li><a href="2-stats-model.html">統計モデルの基本: 確率分布、尤度</a>
<li class="current-deck"><a href="3-glm.html">一般化線形モデル、混合モデル</a>
<li><a href="4-bayesian.html">ベイズ推定、階層ベイズモデル</a>
</ol>

<div class="footnote">
2021-06-30 東京海上 Data Science Hill Climb
<a href="https://heavywatal.github.io/slides/tokiomarine2021/">https://heavywatal.github.io/slides/tokiomarine2021/</a>
</div>

```{r setup-global, include=FALSE, code=readLines("setup.R")}
```

```{r setup-local, include=FALSE}
library(ggplot2)
library(tibble)
library(dplyr)
library(tidyr)
```

---
## 前章で見た統計モデル

確率変数$X$はパラメータ$\theta$の確率分布$f$に“従う”:&nbsp;
$X \sim f(\theta) $

e.g., ある植物が作る種の数$X$は平均値$\lambda$のポアソン分布に従う:

<div>\[\begin{split}
X \sim \text{Poisson}(\lambda)
\end{split}\]</div>

```{r only-dist, echo = FALSE, fig.height = 4, fig.width = 4, cache = FALSE}
df_rpois = tibble::tibble(X = rpois(50L, 3))
df_dpois = tibble(X = seq(0, 11), Prob = dpois(X, mean(df_rpois$X)))
p_pois = ggplot(df_rpois) + aes(X) +
  geom_bar(aes(y = after_stat(prop)), width = 0.3) +
  geom_col(data = df_dpois, aes(y = Prob), alpha = 0.5, fill = "#3366ff") +
  theme_bw(base_size = 18)
p_pois +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        panel.grid.major.x = element_blank(), axis.ticks = element_blank())
```

これを一般化線形モデル(GLM)として見ることもできる。

---
## 一般化線形モデル(GLM)として記述してみる

個体$i$の種子数$y_i$は平均値$\lambda_i$のポアソン分布に従う。<br>
平均値$\lambda_i$は**他のデータによらず$\beta_0$で一定**。

<div>\[\begin{split}
y_i &\sim \text{Poisson}(\lambda_i) \\
\lambda_i &= \beta_0
\end{split}\]</div>

```{r glm-without-x, echo = FALSE, fig.height = 4, fig.width = 4, cache = FALSE}
p_pois +
  labs(x = "y") +
  coord_flip() +
  theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_blank(), axis.ticks = element_blank())
```

種子数をY軸にして、式を2つに分けただけ...?<br>
**説明変数**を含むモデルを見ればご利益が分かるかも。

---
## 説明変数が1つある一般化線形モデル

個体$i$の種子数$y_i$は平均値$\lambda_i$のポアソン分布に従う。<br>
平均値の対数$\log(\lambda_i)$は**その個体の大きさ$x_i$に比例**する。

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0;">

<figure>
<img src="glm.drawio.svg" width="100%"><br>
</figure>

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r glm-poisson, echo = FALSE, fig.height = 5, fig.width = 5, cache = FALSE}
n = 300L
a = 3
b = -3
df_pois = tibble::tibble(x = runif(n, 0.4, 1.7), y = rpois(n, exp(a * x + b)))

x_breaks = c(0.5, 1.0, 1.5)
df_ridges = tidyr::crossing(x = x_breaks, y = seq_len(30L) - 1L) %>%
 dplyr::mutate(density = dpois(y, exp(a * x + b))) %>%
 dplyr::filter(density > 1e-4)
df_bars = df_ridges %>% wtl::ridges2bars(y, density)

p_pois = ggplot(df_pois) + aes(x, y) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  scale_x_continuous(breaks = x_breaks) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())

p_pois +
  stat_smooth(formula = y ~ x, method = glm, method.args = list(family = poisson), se = FALSE) +
  ggridges::geom_vridgeline(data = df_bars, aes(width = density * 0.5, group = x), fill = "#3366ffaa", linetype = 0)
```

  </div>
</div>

この場合は**単回帰**。説明変数が複数あると**重回帰**。


---
## 複数の説明変数を同時に扱う重回帰

<p>\[\begin{split}
y_i &\sim \text{Poisson}(\lambda_i) \\
\log(\lambda_i) &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots
\end{split}\]</p>

気温も湿度も高いほどビールが売れる、とか

```{r multiple-regression, echo = FALSE, fig.height = 5, fig.width = 10, cache = FALSE}
n = 200L
true_coef = c(3, 0.05, 0.006)
df_beer = tibble::tibble(
  temperature = runif(n, 8, 32),
  humidity = runif(n, 20, 80),
  beer_sales = rpois(n, exp(true_coef[1] + true_coef[2] * temperature + true_coef[3] * humidity))
)
glm_multi = glm(beer_sales ~ temperature + humidity, df_beer, family = poisson)

df_pred = tidyr::crossing(temperature = seq(8, 32, 2), humidity = seq(20, 80, 5)) %>%
  modelr::add_predictions(glm_multi) %>%
  dplyr::mutate(y_pred = exp(pred))

p1 = ggplot(df_beer) + aes(temperature, beer_sales, color = humidity) +
  geom_line(data = df_pred, aes(y = y_pred, group = humidity), alpha = 0.7) +
  geom_point(alpha = 0.5) +
  scale_color_viridis_c(option = "cividis", direction = -1) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))
p2 = ggplot(df_beer) + aes(humidity, beer_sales, color = temperature) +
  geom_line(data = df_pred, aes(y = y_pred, group = temperature), alpha = 0.7) +
  geom_point(alpha = 0.5) +
  scale_color_viridis_c(option = "turbo") +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))

cowplot::plot_grid(p1, p2, nrow = 1L)
```

今度は**確率分布**と**リンク関数**を変えてみよう。


---
## ロジスティック回帰

- 確率分布: **二項分布**
- リンク関数: $\text{logit}(p) = \log \frac {p} {1 - p}$

何かの成否に対する何かの因子の影響、とか

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

客10人中$y_i$人がビールを注文。<br>
その日$i$の気温$x_i$によって割合が変化。

<p>\[\begin{split}
y_i &\sim \text{Binomial}(n,~p_i) \\
\text{logit}(p_i) &= \beta_0 + \beta_1 x_i \\
p_i &= \frac 1 {1 + e^{-(\beta_0 + \beta_1 x_i)}}
\end{split}\]</p>

ロジスティック関数↑

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r glm-logistic, echo = FALSE, fig.height = 5, fig.width = 5, cache = FALSE}
nrep = 200L
n = 10L
df_rlogistic = tibble::tibble(
  x = runif(nrep, -10, 35),
  logit_p = -3 + 0.3 * x,
  p = wtl::logistic(logit_p),
  y = rbinom(nrep, n, p),
  response = matrix(c(y, n - y), ncol = 2)
)
glm_logistic = glm(response ~ x, df_rlogistic, family = binomial)
df_pred = df_rlogistic %>%
  modelr::add_predictions(glm_logistic) %>%
  dplyr::mutate(y_pred = n * wtl::logistic(pred))

coef = glm_logistic$coefficients

x_breaks = c(-10, 0, 10, 20, 30)
df_ridges = tidyr::crossing(x = x_breaks, y = seq.int(0, n)) %>%
  dplyr::mutate(p = wtl::logistic(coef[1] + coef[2] * x), density = dbinom(y, n, p)) %>%
  dplyr::filter(density > 1e-4)
df_bars = df_ridges %>% wtl::ridges2bars(y, density)

ggplot(df_pred) + aes(x, y) +
  geom_point(alpha = 0.5, shape = 16) +
  ggridges::geom_vridgeline(data = df_bars, aes(width = density * 6, group = x), fill = "#3366ffaa", linetype = 0) +
  geom_line(aes(y = y_pred), size = 2, color = "#3366ff") +
  scale_x_continuous(breaks = x_breaks) +
  scale_y_continuous(breaks = seq.int(0, 10)) +
  labs(x = "temperature", y = "beer_sales") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())
```

  </div>
</div>

???
ロジット = 対数オッズ
オッズ = 失敗の何倍成功しやすいか
Xが1増えるとオッズがe^a倍に増える。


---
## ロジスティック回帰 (狭義)

- 確率分布: **ベルヌーイ分布** ($n = 1$ の二項分布)
- リンク関数: $\text{logit}(p) = \log \frac {p} {1 - p}$

何かの成否に対する何かの因子の影響、とか

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

風が吹けば桶屋が儲かる。

<p>\[\begin{split}
y_i &\sim \text{Bernoulli}(p_i) \\
  &= \text{Binomial}(1,~p_i) \\
\text{logit}(p_i) &= \beta_0 + \beta_1 x_i \\
p_i &= \frac 1 {1 + e^{-(\beta_0 + \beta_1 x_i)}}
\end{split}\]</p>

ロジスティック関数↑

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r wind, echo = FALSE, fig.height = 4, fig.width = 5, cache = FALSE}
n = 200
df_wind = tibble::tibble(
  max_wind = runif(n, 0, 40),
  bucket_sales = rbinom(n, 1L, wtl::sigmoid(max_wind - 20, 0.2)) + 0L)
glm_bernoulli = glm(bucket_sales ~ max_wind, df_wind, family = "binomial")

coef = glm_bernoulli$coefficients
x_breaks = c(0, 10, 20, 30, 40)
df_ridges = tidyr::crossing(x = x_breaks, y = c(0, 1)) %>%
  dplyr::mutate(p = wtl::logistic(coef[1] + coef[2] * x), density = dbinom(y, 1, p)) %>%
  dplyr::filter(density > 1e-4)
df_bars = df_ridges %>% wtl::ridges2bars(y, density, width = 0.2)

df_wind %>%
  modelr::add_predictions(glm_bernoulli) %>%
  dplyr::mutate(y_pred = wtl::logistic(pred)) %>%
  ggplot() + aes(max_wind, bucket_sales) +
  geom_point(alpha = 0.3, shape = 124, size = 6) +
  ggridges::geom_vridgeline(data = df_bars, aes(x, y, width = density * 6, group = x), fill = "#3366ffaa", linetype = 0) +
  geom_line(aes(y = y_pred), color = "#3366ff") +
  scale_y_continuous(breaks = c(0, 1)) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())
```

  </div>
</div>


---
## 一般線形モデル (“化”無し) はGLMの一種

- 確率分布: **正規分布**
- リンク関数: **恒等関数**(なにもせずそのまま)

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

<p>\[\begin{split}
y_i &\sim \mathcal{N}(\mu_i,~\sigma^2) \\
\text{identity}(\mu_i) &= \beta_0 + \beta_1 x_i
\end{split}\]</p>

  </div>
  <div class="column" style="flex-shrink: 1.0;">

```{r glm-weight, echo = FALSE, fig.height = 4, fig.width = 4, cache = FALSE}
n = 100
df_weight = tibble::tibble(
  height = rnorm(n, 1.70, 0.06),
  bmi = rnorm(n, 22, 0.8),
  weight = bmi * (height ** 2)
) %>%
  dplyr::filter(dplyr::between(height, 1.6, 1.8))

coef = lm(weight ~ height, df_weight)$coefficients

x_breaks = c(1.65, 1.7, 1.75)
df_ridges = tidyr::crossing(height = x_breaks, weight = seq(50, 80, 0.2)) %>%
  dplyr::mutate(density = dnorm(weight, coef[1] + coef[2] * height, 1.8)) %>%
  dplyr::filter(density > 1e-4)

ggplot(df_weight) + aes(height, weight) +
  geom_point(alpha = 0.5, shape = 16) +
  ggridges::geom_vridgeline(data = df_ridges, aes(width = density * 0.08, group = height), fill = "#3366ffaa", linetype = 0) +
  stat_smooth(method = lm, formula = y ~ x, se = FALSE) +
  scale_x_continuous(breaks = x_breaks) +
  theme_bw(base_size = 20) + theme(panel.grid.minor = element_blank())
```

  </div>
</div>

最小二乗法の直線あてはめと結果的に同じになる。

<small style="color: #999999;">単回帰・重回帰と言ったとき一般線形モデルを前提とする人もいる。</small>

---
## 分散分析 (<u>An</u>alysis <u>o</u>f <u>va</u>riance, ANOVA) as GLM

**質的な説明変数**を持つ**正規分布・恒等リンク**のGLM、と解釈可能。<br>
<span title="ダミー変数とも呼ばれる">**指示変数**</span> (0 or 1) に変換してから重回帰する。

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

| 天気 | → | $x_1$ ☀️ 晴れ | $x_2$ ☔️ 雨 |
| ---- | :-: | :---: | :---: |
| ☁️ くもり | | 0 | 0 |
| ☀️ 晴れ | | 1 | 0 |
| ☔️ 雨 | | 0 | 1 |

<p>\[\begin{split}
y_i &= \mathcal{N}(\mu_i,\sigma^2) \\
\mu_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}
\end{split}\]</p>

  </div>
  <div class="column" style="flex-shrink: 1.3;">

```{r glm-anova, echo = FALSE, fig.height = 4.5, fig.width = 4.5, cache = FALSE}
n = 200L
coef = c(70, 3, 20, -20)
weather_levels = c("sunny", "cloudy", "rainy")
df_beer = tibble::tibble(
    temperature = runif(n, 8, 32),
    weather = factor(sample(weather_levels, n, TRUE), levels = weather_levels)
  ) %>%
  dplyr::mutate(name = weather, value = 1L) %>%
  tidyr::pivot_wider(values_fill = 0L) %>%
  dplyr::select(!cloudy) %>%
  dplyr::mutate(beer_sales = rnorm(n, coef[1] + coef[2] * temperature + coef[3] * sunny + coef[4] * rainy, 10))

lm_anova = lm(beer_sales ~ weather, df_beer)
df_ridges = tidyr::crossing(weather = factor(weather_levels, levels = weather_levels), beer_sales = seq(50, 200, 1)) %>%
  modelr::add_predictions(lm_anova) %>%
  dplyr::mutate(density = dnorm(beer_sales, pred, 10)) %>%
  dplyr::filter(density > 1e-4)

tidy_anova = broom::tidy(lm_anova)

avgs = tidyr::crossing(weather = factor(weather_levels, levels = weather_levels)) %>%
  modelr::add_predictions(lm_anova) %>%
  tibble::deframe()

dfl = tibble::tribble(
  ~x, ~xend, ~y, ~yend,
  -Inf, Inf, avgs["cloudy"], avgs["cloudy"],
  1.5, 2.5, avgs["sunny"], avgs["sunny"],
  2.5, 3.5, avgs["rainy"], avgs["rainy"]
)

dfa = tibble::tribble(
  ~x, ~xend, ~y, ~yend,
  1.75, 1.75, avgs["cloudy"], avgs["sunny"],
  2.75, 2.75, avgs["cloudy"], avgs["rainy"]
)

dfs = tibble::tribble(
  ~x, ~y, ~label,
  0.6, avgs["cloudy"] + (avgs["sunny"] - avgs["cloudy"]) * 0.3, "beta[0]",
  1.55, (avgs["cloudy"] + avgs["sunny"]) / 2, "beta[1]",
  2.55, (avgs["cloudy"] + avgs["rainy"]) / 2, "beta[2]"
)

.arr = grid::arrow(length = grid::unit(0.1, "inches"))
df_beer %>%
  ggplot() + aes(weather, beer_sales, color = weather) +
  ggridges::geom_vridgeline(data = df_ridges, aes(width = density * 6, group = weather), fill = "#3366ffaa", linetype = 0) +
  annotate("segment", x = dfl$x, xend = dfl$xend, y = dfl$y, yend = dfl$yend, color = "#3366ffaa") +
  annotate("segment", x = dfa$x, xend = dfa$xend, y = dfa$y, yend = dfa$yend, arrow = .arr, color = "#3366ffaa") +
  annotate("text", x = dfs$x, y = dfs$y, label = dfs$label, parse = TRUE, size = 6, color = "#3366ffaa") +
  geom_jitter(width = 0.08, height = 0, alpha = 0.66, shape = 16, size = 3) +
  scale_color_viridis_d(direction = -1, guide = guide_legend(title = NULL)) +
  scale_x_discrete(limits = c("cloudy", "sunny", "rainy")) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")
```

  </div>
</div>

くもり☁️ $\beta_0$ を基準に、晴れの効果☀️ $\beta_1$ と雨の効果☔️ $\beta_2$ が求まる。

GLMなら確率分布・リンク関数を変えてもっと柔軟にモデリングできる。


---
## 共分散分析 (<u>An</u>alysis of <u>cova</u>riance, ANCOVA) as GLM

**質的変数と量的変数を両方**含むGLM、と解釈可能。<br>
正規分布・等分散・恒等リンクなどが仮定される。


<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 1rem;">

| 天気 | → | $x_1$ ☀️ 晴れ | $x_2$ ☔️ 雨 |
| ---- | :-: | :---: | :---: |
| ☁️ くもり | | 0 | 0 |
| ☀️ 晴れ | | 1 | 0 |
| ☔️ 雨 | | 0 | 1 |

<p>\[\begin{split}
y_i &= \mathcal{N}(\mu_i,\sigma^2) \\
\mu_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i}
\end{split}\]</p>

  </div>
  <div class="column" style="flex-shrink: 1.3;">


```{r glm-ancova, echo = FALSE, fig.height = 4.5, fig.width = 4.5, cache = FALSE}
lm_ancova = lm(beer_sales ~ temperature + weather, df_beer)
df_pred = tidyr::crossing(temperature = seq(8, 32, 2), weather = factor(weather_levels, levels = weather_levels)) %>%
  modelr::add_predictions(lm_ancova) %>%
  dplyr::mutate(y_pred = pred)

ggplot(df_beer) + aes(temperature, beer_sales, color = weather) +
  geom_line(data = df_pred, aes(y = y_pred, group = weather), alpha = 0.7, size = 2) +
  geom_point(alpha = 0.6, shape = 16, size = 3) +
  scale_color_viridis_d(direction = -1, guide = guide_legend(title = NULL)) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))
```

  </div>
</div>

GLMなら確率分布・リンク関数を変えてもっと柔軟にモデリングできる。


---
## 一般化線形モデル(GLM)ふりかえり

確率分布・リンク関数を変えて柔軟にモデリングできる。<br>
特定の組み合わせには名前がある。

| 名前 | 確率分布 | リンク関数 | 説明変数 |
| ---- | -------- | -------- | -------- |
|ポアソン回帰|ポアソン分布|log| |
|ロジスティック回帰|二項分布|logit| |
|一般線形回帰|正規分布|恒等| |
|分散分析|正規分布|恒等|質的変数|
|共分散分析|正規分布|恒等|質的変数+量的変数|

確率分布については[前章を参照](2-stats-model.html)。<br>
リンク関数をもう少しだけ掘り下げたい。


---
## リンク関数

統計モデリングにおいて「まっすぐ以外も表現できる」意味

$\text{identity}(\mu_i)$
: $\mu_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots$
: 説明変数の効果が**足し算**的に働く。

$\log(\lambda_i)$
: $\lambda_i = e^{\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots} = e^{\beta_0} \times e^{\beta_1 x_{1i}} \times e^{\beta_2 x_{2i}} \times \ldots$
: 説明変数の効果が**掛け算**的に働く。<br>
  e.g., $\Delta x_1$ 増えると $e^{\beta_1 \Delta x_{1}}$ 倍になる

$\text{logit}(p_i)$
: $p_i = \frac 1 {1 + e^{-(\beta_0 + \beta_1 x_i + \ldots)}} $ (ロジスティック関数)
: 説明変数の効果が**頭打ち**になる。<br>
  e.g., $\lim_{x \to -\infty} p = 0;~\lim_{x \to \infty} p = 1$

ほかに `probit`, `inverse`, `sqrt`, etc.


---
## データはひとつ、モデルはたくさん

どう選ぶ？

1. メカニズム的に納得できるものを選ぶ
    - ポアソン過程の**カウント**ならポアソン分布、**間隔**ならガンマ分布
    - n回中k回のように**割合的なカウント**なら二項分布
1. データを可視化してみて、それっぽい形・性質のものを選ぶ
    - **左右対称のひと山**ならとりあえず正規分布
    - **負の値を取らない**ならガンマ分布
    - 直線的か、指数関数的か、頭打ちか、などなど

客観的な指標もほしい。<br>
モデルの尤もらしさといえば...


---
## <ruby>尤<rt>ゆう</rt>度</ruby> (likelihood)

**あるモデル$M$の下でそのデータ$D$が観察される確率**:<br>
$\text{Prob}(D \mid M)$

データ$D$を固定し、モデル$M$の関数とみなしたものが**尤度関数**:<br>
$L(M \mid D)$

モデルの構造も固定してパラメータ$\theta$だけ動かす場合はこう書く:<br>
$L(\theta \mid D)$ or $L(\theta)$

**対数尤度** $\log L$ の形にしたほうがいろいろ便利。

<hr>

各モデルで最適なパラメータを探して、比較:<br>
$\log L^* (M_1) \text{ vs. } \log L^* (M_2) \text{ vs. } \log L^* (M_3) \ldots$


---
## たしかに尤度はあてはまりの良さを表してそう

この場合は直線回帰よりもポアソン回帰が良さそう:

```{r compare-loglik, echo = FALSE, fig.height = 5, fig.width = 9, cache = FALSE}
n = 300L
a = 3
b = -3
df_pois = tibble::tibble(x = runif(n, 0.4, 1.7), y = rpois(n, exp(a * x + b)))

models = setNames(, c("gaussian", "poisson")) %>% purrr::map(~{
  glm(y ~ x, family = .x, data = df_pois)
})

x_breaks = c(0.5, 1.0, 1.5)
df_lm = tidyr::crossing(x = x_breaks, y = seq(-5, 20, 0.1)) %>%
  modelr::add_predictions(models[["gaussian"]]) %>%
  dplyr::mutate(density = dnorm(y, pred, 1.4)) %>%
  dplyr::filter(density > 1e-4)

p_pois = ggplot(df_pois) + aes(x, y) +
  ggridges::geom_vridgeline(data = df_lm, aes(width = density * 0.4, group = x), linetype = 0, alpha = 0) +
  geom_point(alpha = 0.5, shape = 16, size = 2) +
  scale_x_continuous(breaks = x_breaks) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())

label = sprintf("logLik = %.1f", broom::glance(models[["gaussian"]])$logLik)
p_lm = p_pois +
  labs(title = "gaussian, identity link") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = label, color = "#3366ff", size = 6) +
  stat_smooth(formula = y ~ x, method = lm, se = FALSE) +
  ggridges::geom_vridgeline(data = df_lm, aes(width = density * 0.4, group = x), fill = "#3366ffaa", linetype = 0)
# p_lm

df_ridges = tidyr::crossing(x = x_breaks, y = seq_len(30L) - 1L) %>%
  modelr::add_predictions(models[["poisson"]]) %>%
  dplyr::mutate(density = dpois(y, exp(pred))) %>%
  dplyr::filter(density > 1e-4)
df_bars = df_ridges %>% wtl::ridges2bars(y, density)

label = sprintf("logLik = %.1f", broom::glance(models[["poisson"]])$logLik)
p_poisson = p_pois +
  labs(title = "poisson, log link") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = label, color = "#3366ff", size = 6) +
  stat_smooth(formula = y ~ x, method = glm, method.args = list(family = poisson), se = FALSE) +
  ggridges::geom_vridgeline(data = df_bars, aes(width = density * 0.5, group = x), fill = "#3366ffaa", linetype = 0)
# p_poisson

cowplot::plot_grid(p_lm, p_poisson, nrow = 1L)
```

この調子で、より尤度の高いモデルを探していけばいいだろうか？

---
## あてはまりが良ければいいってもんでもない

過剰適合 / 過学習 / overfitting
: パラメータを増やせば**現データへの**適合度・尤度を高くできるが、<br>
  予測・理解の役には立たなくなる。

```{r saturated-model, echo = FALSE, fig.height = 4, fig.width = 11, cache = FALSE}
n = 16L
true_coef = c(0.1, 0.2)
df_plant = tibble::tibble(
  x = runif(n, 7, 12.5),
  lambda = exp(true_coef[1] + true_coef[2] * x),
  y = rpois(n, lambda)
) %>% tibble::rownames_to_column("id")

models = df_plant %>% modelr::fit_with(glm, family = "poisson", modelr::formulas(~y,
  null = ~ 1,
  x = ~ x,
  saturated = ~ id
))
labels = setNames(sprintf("logLik = %.1f", purrr::map_dbl(models, logLik)), names(models))

p_plant = df_plant %>%
  modelr::add_predictions(models$null) %>%
  ggplot() + aes(x, y) +
  geom_line(aes(y = exp(pred)), color = "#3366ff", size = 2, alpha = 0.7) +
  geom_point(shape = 16, alpha = 0.6) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")

p_null = p_plant +
  labs(title = "null model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["null"]], color = "#3366ff", size = 6)
p_x = p_plant %+% modelr::add_predictions(df_plant, models$x) +
  labs(title = expression(y %~% beta[0] + beta[1] * x)) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x"]], color = "#3366ff", size = 6)
p_saturated = p_plant %+% modelr::add_predictions(df_plant, models$saturated) +
  labs(title = "saturated model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["saturated"]], color = "#3366ff", size = 6)

cowplot::plot_grid(p_null, p_x, p_saturated, nrow = 1L)
```

**帰無モデル**: 説明変数なし。切片のみ。<br>
**飽和モデル**: データ点の数 ≤ パラメータの数。“データ読み上げ”的モデル


---
## 無駄な説明変数を加えても尤度は上がる

ある植物が作る種の数 $y$ は個体のサイズ $x$ に応じて増える。<br>
観察時に着てた服の色 $x_2$ を追加すると尤度が上がる......?

```{r many-models, echo = FALSE, fig.height = 7, fig.width = 7, cache = FALSE}
set.seed(24601)
n = 120L
true_coef = c(1, 0.12, 0)
df_plant = tibble::tibble(
  x = runif(n, 7, 12.5),
  x2 = sample(c(FALSE, TRUE), n, replace = TRUE),
  lambda = exp(true_coef[1] + true_coef[2] * x + true_coef[3] * x2),
  y = rpois(n, lambda)
) %>% tibble::rownames_to_column("id")

models = df_plant %>% modelr::fit_with(glm, family = "poisson", modelr::formulas(~y,
  null = ~ 1,
  x = ~ x,
  x2 = ~ x2,
  both = ~ x + x2,
  saturated = ~ id
))
labels = setNames(sprintf("logLik = %.1f", purrr::map_dbl(models, logLik)), names(models))

p_plant = df_plant %>%
  modelr::add_predictions(models$null) %>%
  ggplot() + aes(x, y) +
  geom_line(aes(y = exp(pred)), size = 1.5, alpha = 0.6) +
  geom_point(shape = 16, alpha = 0.6) +
  theme_bw(base_size = 18) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")

p_null = p_plant +
  labs(title = "null model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["null"]], color = "#3366ff", size = 6)
p_x = p_plant %+% (df_plant %>% modelr::add_predictions(models$x)) +
  labs(title = expression(y %~% beta[0] + beta[1] * x)) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x"]], color = "#3366ff", size = 6)
p_x2 = p_plant %+% (df_plant %>% modelr::add_predictions(models$x2)) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x2"]], color = "#3366ff", size = 6)
p_both = p_plant %+% (df_plant %>% modelr::add_predictions(models$both)) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[1] * x + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["both"]], color = "#3366ff", size = 6)

cowplot::plot_grid(p_null, p_x, p_x2, p_both, nrow = 2L)
```



---
## AIC: 赤池情報量基準

<p>\[\begin{split}
\text{AIC} = -2 (\log L^* - k) = -2 \log L^* + 2k
\end{split}\]</p>

- AICが小さいほど予測精度の良いモデル。
    - 尤度は上げたい。
    - パラメータ数 $k$ が増えるとペナルティ。
- どのデータに対する当てはまりを目指すかという観点
    - 「手元のデータ」に対する対数尤度は $\log L^*$<br>
    - 「真のメカニズムから出てくる未来のデータ」に対する<br>
      平均対数尤度の推定量は $(\log L^* - k)$<br>
      (Kullback--Leibler情報量を使って導出するらしい)


???
https://www.slideshare.net/logics-of-blue/1-6aic


---
## 無駄な説明変数の追加でAIC増加

ある植物が作る種の数 $y$ は個体のサイズ $x$ に応じて増える。<br>
観察時に着てた服の色 $x_2$ を追加したモデルはAICが増加。

```{r many-models-aic, echo = FALSE, fig.height = 7, fig.width = 7, cache = FALSE}
labels = setNames(sprintf("AIC = %.1f", purrr::map_dbl(models, AIC)), names(models))
p_null = p_plant +
  labs(title = "null model") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["null"]], color = "#3366ff", size = 6)
p_x = p_plant %+% (df_plant %>% modelr::add_predictions(models$x)) +
  labs(title = expression(y %~% beta[0] + beta[1] * x)) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x"]], color = "#3366ff", size = 6)
p_x2 = p_plant %+% (df_plant %>% modelr::add_predictions(models$x2)) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["x2"]], color = "#3366ff", size = 6)
p_both = p_plant %+% (df_plant %>% modelr::add_predictions(models$both)) %+%
  aes(color = x2, group = x2) +
  labs(title = expression(y %~% beta[0] + beta[1] * x + beta[2] * x[2])) +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = labels[["both"]], color = "#3366ff", size = 6)
cowplot::plot_grid(p_null, p_x, p_x2, p_both, nrow = 2L)
```

---
## ほかの情報量基準

- $\text{BIC} = -2 \log L^* + k \log n$
    - パラメータ数 $k$ でペナルティを付けるのはAICと同じ。
    - データの観測数 $n$ に依存する点でAICと異なる。<br>
      感覚としては「AICはデータサイズによるペナルティが無い」
    - (周辺尤度の最大化という観点で導出するらしい)
- [WAIC](http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/waic2011.html),
  [WBIC](http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/wbic2012.html)
    - AIC, BICを一般化し、広く使えるようにしたもの。
    - 理想的な条件ではそれぞれAIC, BICとほぼ同じ。<br>
      そうじゃない場合(現実的には常に)こちらが優位。
    - WAICは予測の良さ、WBICは真のモデルへの近さ、を表す。


---
## モデル選択の心構え

「正しい」ものを選べるわけではない。<br>
予測・理解に useful なものを何らかの基準で選ぶだけ。

> All models are wrong, but some are useful. --- George E. P. Box

<figure>
<img src="math-model.drawio.svg" width="600"><br>
<figcaption><cite>「データ分析のための数理モデル入門」江崎貴裕 2020 より改変</cite></figcaption>
</figure>


---
## 現実的な注意点・悩みどころ

- 多重共線性(multicollinearity):
    - 説明変数同士が強い相関関係にある
- 変数変換:
    - 気安くやるべきじゃないけど、対数変換などしばしば有用
    - 割り算した値は危険
- 交互作用を入れると解釈が難しくなる。


---
## 交互作用

ある説明変数の効果が、別の説明変数によって異なる。<br>
e.g., ビール売上の温度依存性が天気によって異なる。

<div class="column-container">
  <div class="column" style="flex-shrink: 1.0; padding-top: 0.1rem;">

| 天気 | $x_1$ |
| ---- | :---: |
| ☀️ 晴れ | 1 |
| ☔️ 雨 | 0 |

<p>\[\begin{split}
y_i &= \mathcal{N}(\mu_i,\sigma^2) \\
\mu_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_{1,2} x_{1i} x_{2i}
\end{split}\]</p>

雨の日は $x_{1i} = 0$ のため $\beta_0,~\beta_2$ の項だけ。<br>
晴れの日はそれに加えて $\beta_1,~\beta_{1,2}$ の項も。

  </div>
  <div class="column" style="flex-shrink: 1.3;">

```{r interaction, echo = FALSE, fig.height = 4.5, fig.width = 4.5, cache = FALSE}
n = 200L
coef = c(70, 3, 100, -2)
weather_levels = c("sunny", "rainy")
df_beer = tibble::tibble(
    temperature = runif(n, 8, 32),
    weather = factor(sample(weather_levels, n, TRUE), levels = weather_levels)
  ) %>%
  dplyr::mutate(name = weather, value = 1L) %>%
  tidyr::pivot_wider(values_fill = 0L) %>%
  dplyr::mutate(mu = coef[1] * sunny + coef[2] * temperature + coef[3] * rainy + coef[4] * temperature * rainy) %>%
  dplyr::mutate(beer_sales = rnorm(n, mu, 10))

lm_int = lm(beer_sales ~ temperature * weather, df_beer)
df_pred = tidyr::crossing(temperature = seq(8, 32, 2), weather = factor(weather_levels, levels = weather_levels)) %>%
  modelr::add_predictions(lm_int) %>%
  dplyr::mutate(y_pred = pred)

ggplot(df_beer) + aes(temperature, beer_sales, color = weather) +
  geom_line(data = df_pred, aes(y = y_pred, group = weather), alpha = 0.7, size = 2) +
  geom_point(alpha = 0.6, shape = 16, size = 3) +
  scale_color_viridis_d(direction = -1, guide = guide_legend(title = NULL)) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(),
        legend.position = c(0.01, 0.99), legend.justification = c(0, 1))
```

  </div>
</div>


解釈が一気に難しくなるのでむやみに使わない。

---
## GLM座学まとめ

- 何はともあれ散布図を描く
- 適切な確率分布・リンク関数・説明変数を考える
- パラメータを最尤推定する
- 尤度は「手元のデータへのあてはまり」
- モデルを比較するときは情報量基準を参考にする


---
## PythonでGLMをやるパッケージ

[statsmodels](https://www.statsmodels.org)
: 統計モデリング寄り。今回はこちらを紹介。
: AICやp値などの指標も適宜計算してくれるので楽

[scikit-learn](https://scikit-learn.org)
: 機械学習寄り
: 回帰以外のさまざまな手法も統一的な書き方で使える

以降のコードで共通のライブラリ読み込み

```python
import statsmodels.api as sm
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt
import seaborn as sns
palette = {"Adelie": "#ff6600", "Gentoo": "#c35bcc", "Chinstrap": "#007174"}
```


---
## penguinsデータセット

<a href="https://allisonhorst.github.io/palmerpenguins/">
<cite>https://allisonhorst.github.io/palmerpenguins/</cite><br>
<img src="/slides/image/rstats/lter_penguins.png" width="45%">
<img src="/slides/image/rstats/culmen_depth.png" width="45%">
</a>

```python
# Python
import statsmodels.api as sm
penguins = sm.datasets.get_rdataset('penguins', 'palmerpenguins').data
print(penguins)
```

```r
# R
library(palmerpenguins)
print(penguins)
```

---
## penguinsデータセット

<a href="https://allisonhorst.github.io/palmerpenguins/">
<cite>https://allisonhorst.github.io/palmerpenguins/</cite><br>
<img src="/slides/image/rstats/lter_penguins.png" width="45%">
<img src="/slides/image/rstats/culmen_depth.png" width="45%">
</a>

```{r penguins, echo = FALSE, fig.height = 4.5, fig.width = 4.5, cache = FALSE}
if (!require(palmerpenguins, quietly = TRUE)) {
  install.packages("palmerpenguins")
  library(palmerpenguins)
}
penguins_colors = c(Adelie = "darkorange", Chinstrap = "purple", Gentoo = "cyan4")
print(penguins)
```

---
## 単回帰の練習: 1. まず作図

どうやら、重いペンギンほど翼長も長い。

```python
sns.relplot(x='body_mass_g', y='flipper_length_mm', data=penguins)
```

```{r penguins-weight, echo = FALSE, fig.height = 5, fig.width = 5, cache = FALSE, warning = FALSE}
p_penweight = penguins %>%
  ggplot() + aes(body_mass_g, flipper_length_mm) +
  geom_point(shape = 16, alpha = 0.66) +
  scale_color_manual(values = penguins_colors) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())
p_penweight

fit1 = glm(flipper_length_mm ~ body_mass_g, data = penguins)
fit2 = glm(flipper_length_mm ~ body_mass_g + species, data = penguins)
fit3 = glm(flipper_length_mm ~ body_mass_g * species, data = penguins)
p1 = p_penweight +
  geom_line(aes(y = pred), data = modelr::add_predictions(penguins, fit1), size = 1, color = "#3366ff")
p2 = p_penweight + aes(color = species) +
  geom_line(aes(y = pred), data = modelr::add_predictions(penguins, fit2), size = 1)
p3 = p_penweight + aes(color = species) +
  geom_line(aes(y = pred), data = modelr::add_predictions(penguins, fit3), size = 1)
```


---
## 単回帰の練習: 2. モデル作成、フィッティング

結構たくさん出力される。上がモデルの評価、下が推定結果。

```python
formula = 'flipper_length_mm ~ body_mass_g'
model1 = smf.glm(formula, data=penguins)
results1 = model1.fit()
print(results1.summary())
```
```
Dep. Variable:      flipper_length_mm   No. Observations:                  342
Model:                            GLM   Df Residuals:                      340
Model Family:                Gaussian   Df Model:                            1
Link Function:               identity   Scale:                          47.795
Method:                          IRLS   Log-Likelihood:                -1145.5
Date:                Tue, 29 Jun 2021   Deviance:                       16250.
Time:                        10:32:40   Pearson chi2:                 1.63e+04
No. Iterations:                     3
Covariance Type:            nonrobust
===============================================================================
                  coef    std err          z      P>|z|      [0.025      0.975]
-------------------------------------------------------------------------------
Intercept     136.7296      1.997     68.473      0.000     132.816     140.643
body_mass_g     0.0153      0.000     32.722      0.000       0.014       0.016
```

---
## 単回帰の練習: 3. フィッティング結果を作図

$y = 136.7 + 0.0153 x$

```python
y_pred = results1.predict(penguins)
grid = sns.relplot(x='body_mass_g', y='flipper_length_mm', data=penguins)
grid.map(sns.lineplot, x=penguins['body_mass_g'], y=y_pred)
```

```{r penguins-weight-glm, echo = FALSE, fig.height = 5, fig.width = 5, cache = FALSE, warning = FALSE}
p1
```

---
## 重回帰の練習: 1. まず作図

重いペンギンほど翼長も長い。翼長は種によっても違うかも。

```python
sns.relplot(x='body_mass_g', y='flipper_length_mm', hue='species', data=penguins, palette=palette)
```

```{r penguins-weight-sp, echo = FALSE, fig.height = 5, fig.width = 7, cache = FALSE, warning = FALSE}
p_penweight + aes(color = species)
```


---
## 重回帰の練習: 2. モデル作成、フィッティング

Adelieを基準に、ChinstrapとGentooはそれより長め。<br>
体重の効果は単回帰のときより小さい。

```python
formula = 'flipper_length_mm ~ body_mass_g + species'
model2 = smf.glm(formula, data=penguins)
results2 = model2.fit()
print(results2.summary())
```
```
                           coef    std err          z      P>|z|      [0.025      0.975]
----------------------------------------------------------------------------------------
Intercept              158.8603      2.387     66.564      0.000     154.183     163.538
species[T.Chinstrap]     5.5974      0.788      7.101      0.000       4.053       7.142
species[T.Gentoo]       15.6775      1.091     14.374      0.000      13.540      17.815
body_mass_g              0.0084      0.001     13.255      0.000       0.007       0.010
```

---
## 重回帰の練習: 3. フィッティング結果を作図

```python
pen_pred = penguins.assign(pred=results2.predict(penguins))
grid = sns.relplot(x='body_mass_g', y='flipper_length_mm', hue='species', data=pen_pred, palette=palette)
grid.map(sns.lineplot, x='body_mass_g', y='pred', hue='species', data=pen_pred, palette=palette)
```

```{r penguins-weight-sp-glm, echo = FALSE, fig.height = 5, fig.width = 7, cache = FALSE, warning = FALSE}
p2
```

**傾き**も種によって違うかも。**交互作用**を入れてみたい。


---
## 交互作用の練習: モデル作成、フィッティング

Adelieを基準に、Chinstrapの傾きが結構違う。<br>
切片の違いは解釈しにくくなった。

```python
formula = 'flipper_length_mm ~ body_mass_g + species + body_mass_g:species'
model3 = smf.glm(formula, data=penguins)
results3 = model3.fit()
print(results3.summary())
```
```
                                       coef    std err          z      P>|z|      [0.025      0.975]
----------------------------------------------------------------------------------------------------
Intercept                          165.2448      3.551     46.536      0.000     158.285     172.204
species[T.Chinstrap]               -13.8639      7.301     -1.899      0.058     -28.174       0.446
species[T.Gentoo]                    6.0594      6.051      1.001      0.317      -5.800      17.919
body_mass_g                          0.0067      0.001      7.011      0.000       0.005       0.009
body_mass_g:species[T.Chinstrap]     0.0052      0.002      2.683      0.007       0.001       0.009
body_mass_g:species[T.Gentoo]        0.0024      0.001      1.746      0.081      -0.000       0.005
```

---
## 交互作用の練習: フィッティング結果を作図

```python
pen_pred = penguins.assign(pred=results3.predict(penguins))
grid = sns.relplot(x='body_mass_g', y='flipper_length_mm', hue='species', data=pen_pred, palette=palette)
grid.map(sns.lineplot, x='body_mass_g', y='pred', hue='species', data=pen_pred, palette=palette)
```
```{r penguins-interaction, echo = FALSE, fig.height = 5, fig.width = 7, cache = FALSE, warning = FALSE}
p3
```

---
## ここまでの3つのモデルでどれがいいか？

AICで選ぶなら交互作用入り重回帰のが良さそう。

```python
results1.aic
results2.aic
results3.aic
```

```{r penguins-aic, echo = FALSE, fig.height = 4, fig.width = 11, cache = FALSE, warning = FALSE}
labels = sprintf("AIC = %.1f", AIC(fit1, fit2, fit3)$AIC)
cowplot::plot_grid(p1 + labs(title = labels[1]),
                   p2 + labs(title = labels[2]) + theme(legend.position = "none"),
                   p3 + labs(title = labels[3]) + theme(legend.position = "none"), nrow = 1L)
```


---
## GLMの練習

🔰クチバシの長さと深さで同じ解析をやってみよう。

```{r penguins-bill, echo = FALSE, fig.height = 4, fig.width = 11, cache = FALSE, warning = FALSE}
p_bill = penguins %>%
  ggplot() + aes(bill_length_mm, bill_depth_mm) +
  geom_point(shape = 16, alpha = 0.66) +
  scale_color_manual(values = penguins_colors) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())

fit1 = glm(bill_depth_mm ~ bill_length_mm, data = penguins)
fit2 = glm(bill_depth_mm ~ bill_length_mm + species, data = penguins)
fit3 = glm(bill_depth_mm ~ bill_length_mm + species + bill_length_mm:species, data = penguins)
p1 = p_bill +
  geom_line(aes(y = pred), data = modelr::add_predictions(penguins, fit1), size = 1, color = "#3366ff")
p2 = p_bill + aes(color = species) +
  geom_line(aes(y = pred), data = modelr::add_predictions(penguins, fit2), size = 1)
p3 = p_bill + aes(color = species) +
  geom_line(aes(y = pred), data = modelr::add_predictions(penguins, fit3), size = 1)
labels = sprintf("AIC = %.1f", AIC(fit1, fit2, fit3)$AIC)
cowplot::plot_grid(p1 + labs(title = labels[1]),
                   p2 + labs(title = labels[2]) + theme(legend.position = "none"),
                   p3 + labs(title = labels[3]) + theme(legend.position = "none"), nrow = 1L)
```

🔰余裕があったら性別や年なども説明変数に入れてみよう。


---
## 確率分布とリンク関数を明示的に指定したい

何も指定しない場合は正規分布・恒等リンクだった:
```python
formula = 'flipper_length_mm ~ body_mass_g'
model = smf.glm(formula, data=penguins)
#### results.summary()
# Model Family:                Gaussian
# Link Function:               identity
```

こう書いたのと同じ:
```python
formula = 'flipper_length_mm ~ body_mass_g'
link = sm.families.links.identity
family = sm.families.Gaussian(link=link)
model = smf.glm(formula, data=penguins, family=family)
```

利用可能な[確率分布](https://www.statsmodels.org/stable/glm.html#families)と[リンク関数](https://www.statsmodels.org/stable/glm.html#link-functions)は公式ドキュメントを参照。


---
## 手元のデータ、公共データなどでGLMしてみよう

正規分布・恒等リンクじゃないものだとなお良し。

Pythonパッケージに付属のものを探すのもあり。<br>
e.g., [`sm.datasets.get_rdataset(item, package)`](https://vincentarelbundock.github.io/Rdatasets/articles/data.html)

```python
import seaborn as sns
sns.get_dataset_names()
titanic = sns.load_dataset('titanic')

import statsmodels.api as sm
iris = sm.datasets.get_rdataset('iris').data
diamonds = sm.datasets.get_rdataset('diamonds', 'ggplot2').data
```

Slackで報告していただけると嬉しいです。


---
## n個のうちy個生存。二項分布に従......わない！

植物100個体から8個ずつ種子を取って植えたら全体で半分ちょい発芽。<br>
親1個体あたりの生存数は<span style="color: #3366ff;">n=8の二項分布</span>になるはずだけど、<br>
極端な値(全部死亡、全部生存)が多かった。個体差？

```{r overdispersion, echo = FALSE, fig.height = 6, fig.width = 7, cache = FALSE, warning = FALSE}
ninds = 100L
mu_ind = 0.5
sd_ind = 3
df_od = tibble::tibble(
  z = rnorm(ninds, mu_ind, sd_ind),
  p = wtl::sigmoid(z),
  y = rbinom(ninds, 8L, p))
sum_y = sum(df_od$y)
p_hat = sum_y / 800
tidy_od = df_od %>%
  dplyr::count(y, name = "observed") %>%
  dplyr::mutate(expected = ninds * dbinom(y, 8, p_hat)) %>%
  tidyr::pivot_longer(!y, names_to = "key", values_to = "count") %>%
  dplyr::mutate(width = ifelse(key == "expected", 0.8, 0.4), alpha = ifelse(key == "expected", 0.5, 1))
# label = expression(hat(p) == paste(sprintf("%d/800 = %.2f", sum_y, p_hat)))
label = bquote(hat(p) == .(paste(sprintf("%d/800 = %.2f", sum_y, p_hat))))
tidy_od %>%
  ggplot() + aes(y, count) +
  geom_col(aes(fill = key, width = width, alpha = alpha), position = "identity") +
  annotate("text", x = -Inf, y = Inf, hjust = -0.1, vjust = 2, label = label, color = "#3366ff", size = 6) +
  scale_alpha_identity() +
  scale_fill_manual(values = c(observed = "#333333", expected = "#3366ff")) +
  coord_cartesian(xlim = c(0, 8)) +
  labs(x = "# survived seeds") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(),
        legend.title = element_blank(), legend.position = "top")
```


---
## 個体差をモデルに組み込みたい

各個体の生存率$p_i$をそのままパラメータにすると**過剰適合**。<br>
「パラメータ数 ≥ サンプルサイズ」の“データ読み上げ”モデル。<br>
i.e., この個体は4個生き残って生存率0.5だね。次の個体は2個体だから......

```{r saturated-glmm, echo = FALSE, fig.height = 3, fig.width = 11, cache = FALSE, warning = FALSE}
df_od %>%
  tibble::rowid_to_column("id") %>%
  ggplot() + aes(id, y / 8) +
  geom_col(width = 0.6, fill = "#3366ff", alpha = 0.66) +
  # geom_col(width = 0.4) +
  # scale_x_continuous(breaks = seq_len(100)) +
  # coord_cartesian(xlim = c(0.4, 12.9), expand = FALSE) +
  labs(y = expression(italic(p[i]))) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(),
        legend.title = element_blank(), legend.position = "top")
```

個体の生存能力をもっと少ないパラメータで表現できないか？


---
## 個体差をモデルに組み込みたい

各個体の生存率$p_i$が能力値$z_i$のシグモイド関数で決まると仮定。<br>
その能力値は全個体共通の正規分布に従うと仮定:
$z_i \sim \mathcal{N}(\hat z, \sigma)$

```{r sigmoid, echo = FALSE, fig.height = 4, fig.width = 11, cache = FALSE, warning = FALSE}
# tibble::tibble(z = seq(-6, 6, 0.1), p = wtl::sigmoid(z)) %>%
z_hat = wtl::logit(p_hat)
p_sigmoid =
df_od %>%
  ggplot() + aes(z, p) +
  geom_line(size = 2, alpha = 0.6, color = "#3366ff") +
  annotate("line", x = c(-Inf, z_hat, z_hat), y = c(p_hat, p_hat, -Inf), color = "#3366ff") +
  annotate("point", x = z_hat, y = p_hat, shape = 16, size = 3, color = "#3366ff") +
  scale_x_continuous(limits = range(df_od$z), expand = c(0, 0)) +
  labs(x = expression(italic(z[i])), y = expression(italic(p[i]))) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())
p_normal =
tibble::tibble(z = seq(min(df_od$z), max(df_od$z), length.out = 100), Density = dnorm(z, z_hat, sd_ind)) %>%
  ggplot() + aes(z, Density) +
  geom_area(fill = "#3366ff", alpha = 0.5) +
  geom_vline(xintercept = z_hat, color = "#3366ff") +
  scale_x_continuous(limits = range(df_od$z), expand = c(0, 0)) +
  labs(x = expression(italic(z))) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank())

cowplot::plot_grid(p_sigmoid, p_normal, nrow = 1L)
```

パラメータ2つで済む: 平均 $\hat z$, ばらつき $\sigma$ 。

前者は標本平均 $\hat p$ から求まるとして、後者どうする？

---
## 個体能力のばらつき $\sigma$ が大きいと両端が増える

普通の二項分布は個体差無し $\sigma = 0$ を仮定してるのと同じ。

```{r alter-sigma, echo = FALSE, fig.height = 3.5, fig.width = 11, cache = FALSE, warning = FALSE}
sigma = c(0.5, 1.5, 3)
breaks = qnorm(c(0.2, 0.4, 0.6, 0.8), z_hat, 3)
df_z = tidyr::crossing(z = seq(min(df_od$z), max(df_od$z), 0.1), sigma) %>%
  dplyr::mutate(Density = dnorm(z, z_hat, sigma))

p_z = df_z %>%
  ggplot() + aes(z, Density) +
  scale_x_continuous(limits = range(df_od$z), expand = c(0, 0)) +
  facet_wrap(vars(sigma), nrow = 1L, labeller = label_both) +
  labs(x = expression(italic(z))) +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")

p_z + geom_col(width = 0.1, fill = "#3366ff", alpha = 0.6)

df_unmixed = tibble::tibble(z = seq(min(df_od$z), max(df_od$z), length.out = 20)) %>%
  dplyr::mutate(p = wtl::sigmoid(z)) %>%
  purrr::pmap_dfr(function(z, p) {
    tibble::tibble(z, y = seq.int(0L, 8L), Density = dbinom(y, 8L, p))
  }) %>%
  dplyr::arrange(y)

df_mixed = tibble::tibble(sigma) %>%
  dplyr::mutate(data = purrr::map(sigma, ~{
    df_unmixed %>%
      dplyr::mutate(w = dnorm(z, z_hat, .x), Density = w * Density) %>%
      dplyr::mutate(Density = Density / sum(Density))
  })) %>%
  tidyr::unnest(data)

df_binom = tidyr::crossing(y = seq.int(0, 8)) %>% dplyr::mutate(Density = dbinom(y, 8, p_hat))

p_mixed = df_mixed %>%
  ggplot() + aes(y, Density) +
  geom_col(data = df_binom, alpha = 0.5) +
  facet_wrap(vars(sigma), nrow = 1L, labeller = label_both) +
  labs(x = "# survived seeds") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")

p_mixed + geom_col(width = 0.4, fill = "#3366ff", alpha = 0.6)
```

---
## zの値で色分けしてみると想像しやすい

正規分布と二項分布の混ぜ合わせ......?

```{r alter-sigma-z, echo = FALSE, fig.height = 3.5, fig.width = 11, cache = FALSE, warning = FALSE}
p_z + geom_col(aes(fill = z), width = 0.1) +
  scale_fill_viridis_b(option = "turbo", breaks = breaks)

p_mixed + geom_col(aes(fill = z), width = 0.4) +
  scale_fill_viridis_b(option = "turbo", breaks = breaks)
```

---
## 混合分布。ただの二項分布よりも良いあてはまり。

パラメータp(を決めるz)ごとに二項分布を作って、重み付けして足したもの。

```{r before-mixing, echo = FALSE, fig.height = 3.5, fig.width = 11, cache = FALSE, warning = FALSE}
df_mixed %>%
  dplyr::filter(sigma == max(sigma)) %>%
  dplyr::mutate(Z = cut(z, c(-Inf, breaks, Inf))) %>%
  dplyr::group_by(Z) %>%
  dplyr::mutate(Density = Density / sum(Density)) %>%
  dplyr::ungroup() %>%
  ggplot() + aes(y, Density) +
  geom_col(aes(fill = z)) +
  scale_fill_viridis_b(option = "turbo", breaks = breaks) +
  facet_wrap(vars(Z), nrow = 1, labeller = label_both) +
  labs(x = "# survived seeds") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")
```

<div align="center">

```{r after-mixing, echo = FALSE, fig.height = 3.5, fig.width = 7, cache = FALSE, warning = FALSE}
p1 = df_mixed %>%
  dplyr::filter(sigma == max(sigma)) %>%
  ggplot() + aes(y, Density) +
  geom_col(aes(fill = z)) +
  scale_fill_viridis_b(option = "turbo", breaks = breaks) +
  labs(x = "# survived seeds", title = "expected") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(), legend.position = "none")

p2 = tidy_od %>%
  dplyr::filter(key == "observed") %>%
  ggplot() + aes(y, count) +
  geom_col() +
  labs(x = "# survived seeds", title = "observed") +
  theme_bw(base_size = 20) +
  theme(panel.grid.minor = element_blank(),
        legend.title = element_blank(), legend.position = "top")

cowplot::plot_grid(p1, p2, nrow = 1L)
```

</div>

---
## 一般化線形混合モデル GLMM

**固定効果(fixed effects)** のみ扱っていたGLMを拡張して、<br>
**変量効果(random effect)** を混合したモデル。<br>
<small style="color: #999999;">「混合分布を使うモデル」という意味ではないらしい。</small>

<p>\[\begin{split}
y_i &\sim \text{Binomial}(n,~p_i) \\
\text{logit}(p_i) &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots
  + z_{1i} + \ldots \\
z_{1i} &\sim \mathcal{N}(\mu_1,~\sigma_1)
\end{split}\]</p>

e.g.,<br>
個体$i$の種子生存率$p_i$は、<br>
(固定効果) 体サイズ$x_{1i}$と日当たり$x_{2i}$に依存し、<br>
(変量効果) よくわからん個体差$z_{1i}$と植木鉢差$z_{2i}$もある。

---
## 固定効果にするか、変量効果にするか

推定したパラメータを予測に使うなら固定効果

予測に使えそうなので固定効果向き
: - 観測・操作した連続値変数: 長さ、重さ、温度、etc.
: - 観測・操作したカテゴリカル変数: 性別、投薬、etc.

予測に使えないので変量効果向き
: - 観測・操作できなかった個体差:<br>
    たまたま集まってくれた学生15人 {A, B, C, ...}。<br>
    Aさんの固定効果を推定できても、Zさんの予測には使えない。
: - 観測・操作できなかったグループ差:<br>
    ↑の学生をランダム5人ずつに分けたグループ {い、ろ、は}。<br>
    いグループの固定効果を推定できても、また集まることはない。

---
## どういうときに変量効果を考える必要があるか

データに**擬似反復**が含まれるとき。<br>
ぜんぶ独立のつもりで解析すると推定が偏ったり誤ったり。

| 植木鉢 | 個体/植木鉢 | 種子/個体 | 疑似反復 | 推定不可 |
| -----  | ----------- | ----------| ---- | ------ |
| 100個  | 1個体ずつ   | 1個ずつ   | – | 個体差・鉢差 |
| 25個   | 1個体ずつ   | 4個ずつ   | 個体 | 鉢差 |
| 20個   | 5個体ずつ   | 1個ずつ   | 植木鉢 | 個体差 |
| 5個    | 5個体ずつ   | 4個ずつ   | 植木鉢・個体 | – |

疑似反復あり<br>
→ 観測できなかった個体差・場所差(変量効果)を推定可能<br>
→ そのぶんを差し引いて固定効果を推定したい


---
## GLMMの問題点・展望

- 最尤推定の計算が難しくなるので、あまり複雑にはできない
    - ベイズ推定を使えばクリアできる
- GLMの拡張として理解はできても、実際に書くのは難しめ
    - 階層ベイズモデルの一種として見るほうが便利

→ ここでGLMMの練習はせず、階層ベイズモデルに進む。

<figure>
<a href="https://kuboweb.github.io/-kubo/ce/LinksGlm.html">
<img src="image/kubo-p2.png" width="60%">
<figcaption class="url">久保さん https://kuboweb.github.io/-kubo/ce/LinksGlm.html</figcaption>
</a>
</figure>


---
## 一般化線形(混合)モデルまとめ

- 何はともあれ作図して俯瞰
- GLMは統計モデリングの考え方の根幹
    - 確率分布・リンク関数・説明変数
    - 尤度・最尤法によるパラメータ推定
    - 情報量基準などによるモデル選択
- GLMMは現実のデータ解析に向けた強化
    - 疑似反復による変量効果を考慮
    - 階層ベイズモデルとして扱うほうが楽

---
## 参考文献

- [データ解析のための統計モデリング入門](https://amzn.to/33suMIZ) 久保拓弥 2012
- [StanとRでベイズ統計モデリング](https://amzn.to/3uwx7Pb) 松浦健太郎 2016
- [RとStanではじめる ベイズ統計モデリングによるデータ分析入門](https://amzn.to/3o1eCzP) 馬場真哉 2019
- [データ分析のための数理モデル入門](https://amzn.to/3uCxTKo) 江崎貴裕 2020
- [分析者のためのデータ解釈学入門](https://amzn.to/3uznzCK) 江崎貴裕 2020
- [統計学を哲学する](https://amzn.to/3ty80Kv) 大塚淳 2020

<a href="4-bayesian.html" rel="next" class="readmore">
4. ベイズ推定、階層ベイズモデル
</a>
